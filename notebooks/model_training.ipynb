{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7afddec3",
   "metadata": {},
   "source": [
    "# Span Classification Model Training\n",
    "\n",
    "This notebook trains a transformer model to classify the persuasive technique used in a marked text span within Russian articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dab74567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorWithPadding,\n",
    "    PreTrainedModel,\n",
    "    XLMRobertaPreTrainedModel\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# --- Constants ---\n",
    "BASE_DATA_PATH = \"../data/processed/ru/\"\n",
    "LABEL_FILES_PATTERN = os.path.join(BASE_DATA_PATH, \"train-labels-subtask-3-spans-*.txt\")\n",
    "WRAPPED_ARTICLES_DIR = os.path.join(BASE_DATA_PATH, \"wrapped-articles\")\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "MAX_LENGTH = 512\n",
    "TEST_SIZE = 0.1\n",
    "RANDOM_STATE = 42\n",
    "OUTPUT_DIR = \"./span_classification_model\"\n",
    "LOGGING_DIR = \"./span_classification_logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81326730",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "Load labels from all language files, assign a unique span index within each article, and create label mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d00bf1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found label files: ['../data/processed/ru/train-labels-subtask-3-spans-po.txt', '../data/processed/ru/train-labels-subtask-3-spans-it.txt', '../data/processed/ru/train-labels-subtask-3-spans-en.txt', '../data/processed/ru/train-labels-subtask-3-spans-ru.txt', '../data/processed/ru/train-labels-subtask-3-spans-ge.txt', '../data/processed/ru/train-labels-subtask-3-spans-fr.txt']\n",
      "Loaded 36511 labels from 6 files.\n",
      "Unique articles: 1550\n",
      "Unique labels: 23\n",
      "\n",
      "Number of unique labels: 23\n",
      "\n",
      "Sample of processed label data:\n",
      "      article_id                     label  span_idx  label_id\n",
      "17956  111111111                     Doubt         1         9\n",
      "17957  111111111       Appeal_to_Authority         2         0\n",
      "17958  111111111                Repetition         3        19\n",
      "17959  111111111  Appeal_to_Fear-Prejudice         4         1\n",
      "17960  111111111  Appeal_to_Fear-Prejudice         5         1\n"
     ]
    }
   ],
   "source": [
    "def load_all_labels(pattern: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads labels from all files matching the pattern and assigns span indices.\"\"\"\n",
    "    all_files = glob.glob(pattern)\n",
    "    df_list = []\n",
    "    print(f\"Found label files: {all_files}\")\n",
    "    for f in all_files:\n",
    "        try:\n",
    "            df_lang = pd.read_csv(f, sep=\"\\t\", header=None, names=[\"article_id\", \"label\", \"start\", \"end\"], dtype={'article_id': str})\n",
    "            df_list.append(df_lang)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {f}: {e}\")\n",
    "    \n",
    "    if not df_list:\n",
    "        raise ValueError(\"No label data loaded. Check LABEL_FILES_PATTERN.\")\n",
    "        \n",
    "    full_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # Sort by article_id and original position (start offset) to ensure consistent indexing\n",
    "    full_df = full_df.sort_values(by=['article_id', 'start'], ascending=[True, True])\n",
    "    \n",
    "    # Assign 1-based span index within each article group\n",
    "    full_df['span_idx'] = full_df.groupby('article_id').cumcount() + 1\n",
    "    \n",
    "    print(f\"Loaded {len(full_df)} labels from {len(all_files)} files.\")\n",
    "    print(f\"Unique articles: {full_df['article_id'].nunique()}\")\n",
    "    print(f\"Unique labels: {full_df['label'].nunique()}\")\n",
    "    return full_df[['article_id', 'label', 'span_idx']]\n",
    "\n",
    "# Load the data\n",
    "label_df = load_all_labels(LABEL_FILES_PATTERN)\n",
    "\n",
    "# Create label mappings\n",
    "unique_labels = sorted(label_df['label'].unique())\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "num_labels = len(unique_labels)\n",
    "\n",
    "print(f\"\\nNumber of unique labels: {num_labels}\")\n",
    "\n",
    "# Add label_id column to DataFrame\n",
    "label_df['label_id'] = label_df['label'].map(label2id)\n",
    "\n",
    "print(\"\\nSample of processed label data:\")\n",
    "print(label_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cea38e",
   "metadata": {},
   "source": [
    "## 2. Create PyTorch Dataset\n",
    "\n",
    "Define a dataset class to handle loading wrapped articles, replacing markers, tokenizing, and identifying the start marker position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdf259a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpanClassificationDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer, wrapped_articles_dir: str, max_length: int):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.wrapped_articles_dir = wrapped_articles_dir\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        article_id = item['article_id']\n",
    "        label_id = item['label_id']\n",
    "        span_idx = item['span_idx']\n",
    "\n",
    "        # --- Find the article file using glob ---\n",
    "        search_pattern = os.path.join(self.wrapped_articles_dir, f\"*article{article_id}.txt\")\n",
    "        possible_files = glob.glob(search_pattern)\n",
    "        \n",
    "        article_path = \"\"\n",
    "        if len(possible_files) == 1:\n",
    "            article_path = possible_files[0]\n",
    "        elif len(possible_files) == 0:\n",
    "             raise FileNotFoundError(f\"Missing article file for {article_id}. Searched pattern: {search_pattern}\")\n",
    "        else:\n",
    "            non_prefixed_path = os.path.join(self.wrapped_articles_dir, f\"article{article_id}.txt\")\n",
    "            if non_prefixed_path in possible_files:\n",
    "                 article_path = non_prefixed_path\n",
    "                 print(f\"Warning: Found multiple files for article {article_id}, using non-prefixed: {article_path}\")\n",
    "            else:\n",
    "                 article_path = possible_files[0]\n",
    "                 print(f\"Warning: Found multiple files for article {article_id}: {possible_files}. Using the first one: {article_path}\")\n",
    "\n",
    "        try:\n",
    "            with open(article_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading article {article_id} from {article_path}: {e}\")\n",
    "            raise e\n",
    "\n",
    "        start_marker = f\"<<S_{span_idx}>>\"\n",
    "        end_marker = f\"<</S_{span_idx}>>\"\n",
    "        processed_text = text.replace(start_marker, \"\", 1)\n",
    "        processed_text = text.replace(end_marker, \"\", 1)\n",
    "\n",
    "        processed_text = re.sub(r'<<S_\\d+>>', '', processed_text)\n",
    "        processed_text = re.sub(r'<</S_\\d+>>', '', processed_text)\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            processed_text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label_id, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee36d887",
   "metadata": {},
   "source": [
    "## 3. Initialize Tokenizer\n",
    "\n",
    "Load the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fa0c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888697c",
   "metadata": {},
   "source": [
    "## 4. Define Custom Model\n",
    "\n",
    "Create a model that uses the hidden state of the `[CLS]` token for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5bbe893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpanClassifierModel(XLMRobertaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = AutoModel.from_config(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.class_weights = None\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        cls_embedding = outputs[0][:, 0, :]\n",
    "        pooled_output = self.dropout(cls_embedding)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_weights = self.class_weights.to(logits.device) if self.class_weights is not None else None\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=loss_weights)\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1).to(logits.device))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27528f98",
   "metadata": {},
   "source": [
    "## 5. Prepare Data for Training\n",
    "\n",
    "Split the data, create Dataset instances, and define a data collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "137d229e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 32859\n",
      "Validation set size: 3652\n",
      "\n",
      "Calculated class weights:\n",
      "  Class 0 (Appeal_to_Authority): 2.1646\n",
      "  Class 1 (Appeal_to_Fear-Prejudice): 0.9205\n",
      "  Class 2 (Appeal_to_Hypocrisy): 1.6143\n",
      "  Class 3 (Appeal_to_Popularity): 4.3824\n",
      "  Class 4 (Appeal_to_Time): 9.0421\n",
      "  Class 5 (Appeal_to_Values): 2.2288\n",
      "  Class 6 (Causal_Oversimplification): 2.6704\n",
      "  Class 7 (Consequential_Oversimplification): 4.2268\n",
      "  Class 8 (Conversation_Killer): 1.5927\n",
      "  Class 9 (Doubt): 0.3335\n",
      "  Class 10 (Exaggeration-Minimisation): 0.8907\n",
      "  Class 11 (False_Dilemma-No_Choice): 3.2396\n",
      "  Class 12 (Flag_Waving): 2.0037\n",
      "  Class 13 (Guilt_by_Association): 2.3692\n",
      "  Class 14 (Loaded_Language): 0.1725\n",
      "  Class 15 (Name_Calling-Labeling): 0.2359\n",
      "  Class 16 (Obfuscation-Vagueness-Confusion): 4.2902\n",
      "  Class 17 (Questioning_the_Reputation): 0.6803\n",
      "  Class 18 (Red_Herring): 7.5590\n",
      "  Class 19 (Repetition): 1.2621\n",
      "  Class 20 (Slogans): 2.0439\n",
      "  Class 21 (Straw_Man): 4.9095\n",
      "  Class 22 (Whataboutism): 10.2781\n",
      "\n",
      "Sample dataset item:\n",
      "{'input_ids': torch.Size([512]), 'attention_mask': torch.Size([512]), 'labels': torch.Size([])}\n",
      "\n",
      "Sample collated batch (first 4 items):\n",
      "{'input_ids': torch.Size([4, 512]), 'attention_mask': torch.Size([4, 512]), 'labels': torch.Size([4])}\n",
      "\n",
      "Sample dataset item:\n",
      "{'input_ids': torch.Size([512]), 'attention_mask': torch.Size([512]), 'labels': torch.Size([])}\n",
      "\n",
      "Sample collated batch (first 4 items):\n",
      "{'input_ids': torch.Size([4, 512]), 'attention_mask': torch.Size([4, 512]), 'labels': torch.Size([4])}\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and validation sets\n",
    "train_df, val_df = train_test_split(\n",
    "    label_df, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=label_df['label_id']\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "\n",
    "# --- Calculate Class Weights from Training Data ---\n",
    "class_counts = train_df['label_id'].value_counts().sort_index()\n",
    "total_samples = len(train_df)\n",
    "weights = total_samples / (len(class_counts) * class_counts)\n",
    "class_weights_tensor = torch.tensor(weights.values, dtype=torch.float)\n",
    "print(\"\\nCalculated class weights:\")\n",
    "for i, w in enumerate(weights):\n",
    "    print(f\"  Class {i} ({id2label[i]}): {w:.4f}\")\n",
    "# --- End Class Weight Calculation ---\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = SpanClassificationDataset(train_df, tokenizer, WRAPPED_ARTICLES_DIR, MAX_LENGTH)\n",
    "val_dataset = SpanClassificationDataset(val_df, tokenizer, WRAPPED_ARTICLES_DIR, MAX_LENGTH)\n",
    "\n",
    "# Data Collator - pads sequences dynamically per batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Test the dataset and collator with one item\n",
    "try:\n",
    "    sample_item = train_dataset[0]\n",
    "    print(\"\\nSample dataset item:\")\n",
    "    print({k: v.shape if hasattr(v, 'shape') else v for k, v in sample_item.items()})\n",
    "    batch = data_collator([train_dataset[i] for i in range(4)])\n",
    "    print(\"\\nSample collated batch (first 4 items):\")\n",
    "    print({k: v.shape if hasattr(v, 'shape') else v for k, v in batch.items()})\n",
    "except Exception as e:\n",
    "    print(f\"Error testing dataset/collator: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7579f5",
   "metadata": {},
   "source": [
    "## 6. Configure Training\n",
    "\n",
    "Define metrics function and training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1685aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1_macro = f1_score(labels, predictions, average='macro', zero_division=0)\n",
    "    f1_per_class = f1_score(labels, predictions, average=None, zero_division=0)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': acc,\n",
    "        'f1_macro': f1_macro,\n",
    "    }\n",
    "    for i, f1 in enumerate(f1_per_class):\n",
    "        if i in id2label:\n",
    "             metrics[f'f1_{id2label[i]}'] = f1\n",
    "        else:\n",
    "             metrics[f'f1_class_{i}'] = f1\n",
    "    return metrics\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=LOGGING_DIR,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    lr_scheduler_type='cosine_with_restarts',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b7ccff",
   "metadata": {},
   "source": [
    "## 7. Initialize Model and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "236ab124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SpanClassifierModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Assigned class weights to the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1090/1483679515.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Ensure model config has correct label mappings\n",
    "model = SpanClassifierModel.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=num_labels, \n",
    "    label2id=label2id, \n",
    "    id2label=id2label\n",
    ")\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Assign Class Weights to Model ---\n",
    "model.class_weights = class_weights_tensor.to(device)\n",
    "print(\"Assigned class weights to the model.\")\n",
    "# --- End Assign Class Weights ---\n",
    "\n",
    "# Move model to the correct device\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0082528",
   "metadata": {},
   "source": [
    "## 8. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca9b690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1918' max='10270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1918/10270 1:01:00 < 4:25:56, 0.52 it/s, Epoch 0.93/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Before starting training, clear cache if needed\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Save training metrics\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "# Save the final model and tokenizer\n",
    "trainer.save_model() \n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12ddace",
   "metadata": {},
   "source": [
    "## 9. Evaluate the Model\n",
    "\n",
    "Evaluate the best model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c7b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the best model on the validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='229' max='229' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [229/229 00:59]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation finished.\n",
      "***** eval metrics *****\n",
      "  epoch                                    =        5.0\n",
      "  eval_accuracy                            =     0.1336\n",
      "  eval_f1_Appeal_to_Authority              =     0.1618\n",
      "  eval_f1_Appeal_to_Fear-Prejudice         =     0.1086\n",
      "  eval_f1_Appeal_to_Hypocrisy              =     0.1322\n",
      "  eval_f1_Appeal_to_Popularity             =     0.1029\n",
      "  eval_f1_Appeal_to_Time                   =        0.0\n",
      "  eval_f1_Appeal_to_Values                 =     0.0991\n",
      "  eval_f1_Causal_Oversimplification        =      0.098\n",
      "  eval_f1_Consequential_Oversimplification =     0.0606\n",
      "  eval_f1_Conversation_Killer              =        0.0\n",
      "  eval_f1_Doubt                            =       0.25\n",
      "  eval_f1_Exaggeration-Minimisation        =     0.0996\n",
      "  eval_f1_False_Dilemma-No_Choice          =     0.0388\n",
      "  eval_f1_Flag_Waving                      =     0.0976\n",
      "  eval_f1_Guilt_by_Association             =     0.1226\n",
      "  eval_f1_Loaded_Language                  =      0.054\n",
      "  eval_f1_Name_Calling-Labeling            =     0.1956\n",
      "  eval_f1_Obfuscation-Vagueness-Confusion  =     0.0263\n",
      "  eval_f1_Questioning_the_Reputation       =     0.1045\n",
      "  eval_f1_Red_Herring                      =        0.0\n",
      "  eval_f1_Repetition                       =     0.2679\n",
      "  eval_f1_Slogans                          =     0.1299\n",
      "  eval_f1_Straw_Man                        =     0.0719\n",
      "  eval_f1_Whataboutism                     =     0.0417\n",
      "  eval_f1_macro                            =     0.0984\n",
      "  eval_loss                                =     2.9336\n",
      "  eval_runtime                             = 0:01:00.21\n",
      "  eval_samples_per_second                  =     60.645\n",
      "  eval_steps_per_second                    =      3.803\n",
      "\n",
      "Evaluation Metrics:\n",
      "{'eval_loss': 2.93359112739563, 'eval_accuracy': 0.13362541073384446, 'eval_f1_macro': 0.09841326489891256, 'eval_f1_Appeal_to_Authority': 0.16176470588235295, 'eval_f1_Appeal_to_Fear-Prejudice': 0.10857142857142857, 'eval_f1_Appeal_to_Hypocrisy': 0.13215859030837004, 'eval_f1_Appeal_to_Popularity': 0.10294117647058823, 'eval_f1_Appeal_to_Time': 0.0, 'eval_f1_Appeal_to_Values': 0.09905660377358491, 'eval_f1_Causal_Oversimplification': 0.09803921568627451, 'eval_f1_Consequential_Oversimplification': 0.06060606060606061, 'eval_f1_Conversation_Killer': 0.0, 'eval_f1_Doubt': 0.25, 'eval_f1_Exaggeration-Minimisation': 0.09961685823754789, 'eval_f1_False_Dilemma-No_Choice': 0.038834951456310676, 'eval_f1_Flag_Waving': 0.0975609756097561, 'eval_f1_Guilt_by_Association': 0.12258064516129032, 'eval_f1_Loaded_Language': 0.054, 'eval_f1_Name_Calling-Labeling': 0.19557625145518046, 'eval_f1_Obfuscation-Vagueness-Confusion': 0.02631578947368421, 'eval_f1_Questioning_the_Reputation': 0.10454545454545454, 'eval_f1_Red_Herring': 0.0, 'eval_f1_Repetition': 0.26785714285714285, 'eval_f1_Slogans': 0.12987012987012986, 'eval_f1_Straw_Man': 0.07194244604316546, 'eval_f1_Whataboutism': 0.041666666666666664, 'eval_runtime': 60.2193, 'eval_samples_per_second': 60.645, 'eval_steps_per_second': 3.803, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating the best model on the validation set...\")\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation finished.\")\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc0edea",
   "metadata": {},
   "source": [
    "## Clear Model from Memory\n",
    "\n",
    "Delete the model and trainer objects and clear the GPU cache to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9fb6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing model and trainer from memory...\n",
      "  Deleted 'model' variable.\n",
      "  Deleted 'trainer' variable.\n",
      "  Deleted 'train_dataset'.\n",
      "  Deleted 'val_dataset'.\n",
      "  Deleted 'label_df'.\n",
      "  Ran garbage collector.\n",
      "  Cleared PyTorch CUDA cache.\n",
      "Memory clearing process finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "print(\"Clearing model and trainer from memory...\")\n",
    "\n",
    "# Check if variables exist before deleting\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    del model\n",
    "    print(\"  Deleted 'model' variable.\")\n",
    "if 'trainer' in locals() or 'trainer' in globals():\n",
    "    del trainer \n",
    "    print(\"  Deleted 'trainer' variable.\")\n",
    "if 'train_dataset' in locals() or 'train_dataset' in globals():\n",
    "    del train_dataset\n",
    "    print(\"  Deleted 'train_dataset'.\")\n",
    "if 'val_dataset' in locals() or 'val_dataset' in globals():\n",
    "    del val_dataset\n",
    "    print(\"  Deleted 'val_dataset'.\")\n",
    "if 'label_df' in locals() or 'label_df' in globals():\n",
    "    del label_df\n",
    "    print(\"  Deleted 'label_df'.\")\n",
    "\n",
    "# Run Python's garbage collector\n",
    "gc.collect()\n",
    "print(\"  Ran garbage collector.\")\n",
    "\n",
    "# Clear PyTorch's CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"  Cleared PyTorch CUDA cache.\")\n",
    "else:\n",
    "    print(\"  CUDA not available, skipping cache clearing.\")\n",
    "\n",
    "print(\"Memory clearing process finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bb96e",
   "metadata": {},
   "source": [
    "---\n",
    "## Obsolete Code (Commented Out)\n",
    "\n",
    "The following cells contained previous data loading/analysis code which is no longer needed for the current span classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4b3c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OBSOLETE CODE ---\n",
    "# Original data loading utilities - replaced by new loading logic above\n",
    "# import os, re\n",
    "# from collections import defaultdict\n",
    "# from typing import List, Tuple, Dict, Set\n",
    "# from pathlib import Path # Added import\n",
    "# \n",
    "# def read_article(path: str) -> str:\n",
    "#     with open(path, \"rb\") as f:\n",
    "#         raw = f.read()\n",
    "#     return raw.decode(\"utf-8\", \"ignore\")\n",
    "# \n",
    "# def load_span_labels(label_file: str\n",
    "#     ) -> Dict[str, List[Tuple[str, int, int]]]:\n",
    "#     \n",
    "#     spans = defaultdict(list)\n",
    "#     with open(label_file, encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             # Handle potential empty lines or lines with incorrect format\n",
    "#             parts = line.rstrip().split(\"\\t\")\n",
    "#             if len(parts) == 4:\n",
    "#                 art_id, lab, s, e = parts\n",
    "#                 try:\n",
    "#                     spans[art_id].append((lab, int(s), int(e)))\n",
    "#                 except ValueError:\n",
    "#                     print(f\"Warning: Skipping malformed line in {label_file}: {line.rstrip()}\")\n",
    "#             elif line.strip(): # Print warning for non-empty, but malformed lines\n",
    "#                  print(f\"Warning: Skipping malformed line in {label_file}: {line.rstrip()}\")\n",
    "#     return spans\n",
    "# \n",
    "# # Added function to extract base classes from the span file\n",
    "# def get_base_classes_from_spans(label_file: str) -> Set[str]:\n",
    "#     base_classes = set()\n",
    "#     with open(label_file, encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             parts = line.rstrip().split(\"\\t\")\n",
    "#             if len(parts) == 4:\n",
    "#                 _, lab, _, _ = parts\n",
    "#                 base_classes.add(lab)\n",
    "#             elif line.strip():\n",
    "#                  # Warnings handled in load_span_labels, no need to repeat here\n",
    "#                  pass\n",
    "#     return base_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c69b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OBSOLETE CODE ---\n",
    "# Original span printing utility\n",
    "# def print_span(article_number, start_offset, end_offset, lang=\"en\", base_path=\"data/raw\"):\n",
    "#     article_path = f\"{base_path}/{lang}/train-articles-subtask-3/{lang}_article{article_number}.txt\"\n",
    "#     with open(article_path, \"rb\") as f:\n",
    "#         raw = f.read()\n",
    "#     text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
    "#     span = text[start_offset:end_offset]\n",
    "#     print(span)\n",
    "#     \n",
    "# import pandas as pd\n",
    "# \n",
    "# df = pd.read_csv(\"../data/processed/ru/train-labels-subtask-3-spans-en.txt\", sep=\"\\t\", header=None, names=[\"article_id\", \"label\", \"start\", \"end\"])\n",
    "# \n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89483cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OBSOLETE CODE ---\n",
    "# Original span display functions\n",
    "# import re\n",
    "# # Function to display spans for a specific article\n",
    "# def show_article_spans(article_id, lang=\"en\", base_path=\"../data/processed/ru\"):\n",
    "#     # Get all spans for this specific article\n",
    "#     article_spans = df[df['article_id'] == article_id].sort_values(by='start')\n",
    "#     \n",
    "#     # Check if we found any spans\n",
    "#     if len(article_spans) == 0:\n",
    "#         print(f\"No spans found for article {article_id}\")\n",
    "#         return\n",
    "#     \n",
    "#     r = []\n",
    "#     # Try to get the original article text\n",
    "#     article_path = f\"{base_path}/unwrapped-articles/{lang}_article{article_id}.txt\"\n",
    "#     try:\n",
    "#         with open(article_path, \"rb\") as f:\n",
    "#             raw = f.read()\n",
    "#         article_text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
    "#         \n",
    "#         print(f\"\\n{'='*60}\")\n",
    "#         print(f\"Article ID: {article_id}\")\n",
    "#         print(f\"Total spans: {len(article_spans)}\")\n",
    "#         print(f\"{'='*60}\")\n",
    "#         \n",
    "#         # Display each span\n",
    "#         for index, row in article_spans.iterrows():\n",
    "#             label = row['label']\n",
    "#             start_offset = row['start']\n",
    "#             end_offset = row['end']\n",
    "#             span_text = article_text[start_offset:end_offset]\n",
    "#             r.append((label, start_offset, end_offset, span_text))\n",
    "#             \n",
    "#             print(f\"\\nSpan {index}\")\n",
    "#             print(f\"Label: {label}\")\n",
    "#             print(f\"Position: {start_offset} to {end_offset}\")\n",
    "#             print(f\"Text: '{span_text}'\")\n",
    "# \n",
    "#         return r\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error accessing article {article_id}: {e}\")\n",
    "# \n",
    "# # Function to show text between span markers in wrapped articles\n",
    "# def show_wrapped_spans(article_id, lang=\"en\", base_path=\"../data/processed/ru\"):\n",
    "#     # Try to get the wrapped article text\n",
    "#     article_path = f\"{base_path}/wrapped-articles/{lang}_article{article_id}.txt\"\n",
    "#     r = []\n",
    "#     try:\n",
    "#         with open(article_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#             wrapped_text = f.read()\n",
    "#         \n",
    "#         print(f\"\\n{'='*60}\")\n",
    "#         print(f\"Wrapped Article ID: {article_id}\")\n",
    "#         print(f\"{'='*60}\")\n",
    "#         \n",
    "#         # Find all span markers using regex\n",
    "#         spans = re.finditer(r'<<S_(\\d+)>>(.*?)<</S_\\1>>', wrapped_text, re.DOTALL)\n",
    "#         \n",
    "#         found_spans = False\n",
    "#         for i, span in enumerate(spans):\n",
    "#             found_spans = True\n",
    "#             span_number = span.group(1)\n",
    "#             span_text = span.group(2)\n",
    "#             print(f\"\\nMarked span #{span_number}\")\n",
    "#             print(f\"Text: '{span_text}'\")\n",
    "#             r.append((span_number, span_text))\n",
    "#         \n",
    "#         if not found_spans:\n",
    "#             print(f\"No marked spans found in wrapped article {article_id}\")\n",
    "#         return r\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error accessing wrapped article {article_id}: {e}\")\n",
    "# \n",
    "# # Example usage - show spans for a sample article\n",
    "# sample_article_id = df['article_id'].sample(1).iloc[0]\n",
    "# article_spans = show_article_spans(sample_article_id)\n",
    "# \n",
    "# # Show the wrapped version of the same article\n",
    "# wrapped_spans = show_wrapped_spans(sample_article_id)\n",
    "# \n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e01ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OBSOLETE CODE ---\n",
    "# Original span comparison function\n",
    "# import difflib\n",
    "# \n",
    "# def compare_article_and_wrapped_span_texts(article_id, lang=\"en\", base_path=\"../data/processed/ru\", min_ratio=0.8):\n",
    "#     \"\"\"\n",
    "#     Compare the span texts from the article (offset-based) and wrapped (marker-based) versions,\n",
    "#     reporting the closest matches and their differences. Order is not assumed to be the same.\n",
    "#     \"\"\"\n",
    "#     # Get span texts from both sources\n",
    "#     article_spans = show_article_spans(article_id, lang=lang, base_path=base_path)\n",
    "#     wrapped_spans = show_wrapped_spans(article_id, lang=lang, base_path=base_path)\n",
    "# \n",
    "#     if not article_spans or not wrapped_spans:\n",
    "#         print(\"No spans found in one or both sources.\")\n",
    "#         return\n",
    "# \n",
    "#     # Extract just the span texts\n",
    "#     article_texts = [span[3].strip() for span in article_spans]\n",
    "#     wrapped_texts = [span[1].strip() for span in wrapped_spans]\n",
    "# \n",
    "#     # For each article span, find the best matching wrapped span (by similarity ratio)\n",
    "#     print(f\"\\n{'Article Span Index':<18} {'Best Match Index':<16} {'Similarity':<10} {'Article Span Text':<40} {'Wrapped Span Text'}\")\n",
    "#     print(\"-\" * 120)\n",
    "#     used_wrapped = set()\n",
    "#     for i, art_text in enumerate(article_texts):\n",
    "#         # Find the best match in wrapped_texts\n",
    "#         best_ratio = 0\n",
    "#         best_j = -1\n",
    "#         for j, wrap_text in enumerate(wrapped_texts):\n",
    "#             if j in used_wrapped:\n",
    "#                 continue\n",
    "#             ratio = difflib.SequenceMatcher(None, art_text, wrap_text).ratio()\n",
    "#             if ratio > best_ratio:\n",
    "#                 best_ratio = ratio\n",
    "#                 best_j = j\n",
    "#         # Optionally, only consider matches above a threshold\n",
    "#         if best_ratio >= min_ratio and best_j != -1:\n",
    "#             used_wrapped.add(best_j)\n",
    "#             print(f\"{i:<18} {best_j:<16} {best_ratio:.2f} {art_text[:40]:<40} {wrapped_texts[best_j][:40]}\")\n",
    "#             # Show diff if not identical\n",
    "#             if best_ratio < 1.0:\n",
    "#                 diff = difflib.unified_diff(\n",
    "#                     art_text.splitlines(), wrapped_texts[best_j].splitlines(),\n",
    "#                     fromfile='article_span', tofile='wrapped_span', lineterm=''\n",
    "#                 )\n",
    "#                 print('\\n'.join(diff))\n",
    "#         else:\n",
    "#             print(f\"{i:<18} {'-':<16} {'0.00':<10} {art_text[:40]:<40} {'NO MATCH'}\")\n",
    "# \n",
    "#     # Optionally, report wrapped spans that were not matched\n",
    "#     unmatched = [j for j in range(len(wrapped_texts)) if j not in used_wrapped]\n",
    "#     if unmatched:\n",
    "#         print(\"\\nWrapped spans not matched to any article span:\")\n",
    "#         for j in unmatched:\n",
    "#             print(f\"Wrapped index {j}: {wrapped_texts[j][:60]}\")\n",
    "# \n",
    "# # Example usage:\n",
    "# compare_article_and_wrapped_span_texts(sample_article_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
