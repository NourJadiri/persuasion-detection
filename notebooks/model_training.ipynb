{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7afddec3",
   "metadata": {},
   "source": [
    "# Span Classification Model Training\n",
    "\n",
    "This notebook trains a transformer model to classify the persuasive technique used in a marked text span within Russian articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dab74567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/twoface/persuasion-detection/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorWithPadding,\n",
    "    PreTrainedModel,\n",
    "    XLMRobertaPreTrainedModel\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# --- Constants ---\n",
    "BASE_DATA_PATH = \"../data/processed/ru/\"\n",
    "LABEL_FILES_PATTERN = os.path.join(BASE_DATA_PATH, \"train-labels-subtask-3-spans-*.txt\")\n",
    "WRAPPED_ARTICLES_DIR = os.path.join(BASE_DATA_PATH, \"wrapped-articles\")\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "SPAN_START_TOKEN = \"[SPAN_START]\"\n",
    "SPAN_END_TOKEN = \"[SPAN_END]\"\n",
    "SPECIAL_TOKENS = { \"additional_special_tokens\": [SPAN_START_TOKEN, SPAN_END_TOKEN] }\n",
    "MAX_LENGTH = 512\n",
    "TEST_SIZE = 0.1\n",
    "RANDOM_STATE = 42\n",
    "OUTPUT_DIR = \"./span_classification_model\"\n",
    "LOGGING_DIR = \"./span_classification_logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81326730",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "Load labels from all language files, assign a unique span index within each article, and create label mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d00bf1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found label files: ['../data/processed/ru/train-labels-subtask-3-spans-po.txt', '../data/processed/ru/train-labels-subtask-3-spans-it.txt', '../data/processed/ru/train-labels-subtask-3-spans-en.txt', '../data/processed/ru/train-labels-subtask-3-spans-ru.txt', '../data/processed/ru/train-labels-subtask-3-spans-ge.txt', '../data/processed/ru/train-labels-subtask-3-spans-fr.txt']\n",
      "Loaded 36511 labels from 6 files.\n",
      "Unique articles: 1550\n",
      "Unique labels: 23\n",
      "\n",
      "Number of unique labels: 23\n",
      "\n",
      "Sample of processed label data:\n",
      "      article_id                     label  span_idx  label_id\n",
      "17956  111111111                     Doubt         1         9\n",
      "17957  111111111       Appeal_to_Authority         2         0\n",
      "17958  111111111                Repetition         3        19\n",
      "17959  111111111  Appeal_to_Fear-Prejudice         4         1\n",
      "17960  111111111  Appeal_to_Fear-Prejudice         5         1\n"
     ]
    }
   ],
   "source": [
    "def load_all_labels(pattern: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads labels from all files matching the pattern and assigns span indices.\"\"\"\n",
    "    all_files = glob.glob(pattern)\n",
    "    df_list = []\n",
    "    print(f\"Found label files: {all_files}\")\n",
    "    for f in all_files:\n",
    "        try:\n",
    "            df_lang = pd.read_csv(f, sep=\"\\t\", header=None, names=[\"article_id\", \"label\", \"start\", \"end\"], dtype={'article_id': str})\n",
    "            df_list.append(df_lang)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {f}: {e}\")\n",
    "    \n",
    "    if not df_list:\n",
    "        raise ValueError(\"No label data loaded. Check LABEL_FILES_PATTERN.\")\n",
    "        \n",
    "    full_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # Sort by article_id and original position (start offset) to ensure consistent indexing\n",
    "    full_df = full_df.sort_values(by=['article_id', 'start'], ascending=[True, True])\n",
    "    \n",
    "    # Assign 1-based span index within each article group\n",
    "    full_df['span_idx'] = full_df.groupby('article_id').cumcount() + 1\n",
    "    \n",
    "    print(f\"Loaded {len(full_df)} labels from {len(all_files)} files.\")\n",
    "    print(f\"Unique articles: {full_df['article_id'].nunique()}\")\n",
    "    print(f\"Unique labels: {full_df['label'].nunique()}\")\n",
    "    return full_df[['article_id', 'label', 'span_idx']]\n",
    "\n",
    "# Load the data\n",
    "label_df = load_all_labels(LABEL_FILES_PATTERN)\n",
    "\n",
    "# Create label mappings\n",
    "unique_labels = sorted(label_df['label'].unique())\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "num_labels = len(unique_labels)\n",
    "\n",
    "print(f\"\\nNumber of unique labels: {num_labels}\")\n",
    "\n",
    "# Add label_id column to DataFrame\n",
    "label_df['label_id'] = label_df['label'].map(label2id)\n",
    "\n",
    "print(\"\\nSample of processed label data:\")\n",
    "print(label_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cea38e",
   "metadata": {},
   "source": [
    "## 2. Create PyTorch Dataset\n",
    "\n",
    "Define a dataset class to handle loading wrapped articles, replacing markers, tokenizing, and identifying the start marker position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf259a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpanClassificationDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer, wrapped_articles_dir: str, max_length: int):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.wrapped_articles_dir = wrapped_articles_dir\n",
    "        self.max_length = max_length\n",
    "        self.start_token_id = self.tokenizer.convert_tokens_to_ids(SPAN_START_TOKEN)\n",
    "        if self.start_token_id == self.tokenizer.unk_token_id:\n",
    "             print(f\"Warning: {SPAN_START_TOKEN} not found in tokenizer vocab after adding!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        article_id = item['article_id']\n",
    "        label_id = item['label_id']\n",
    "        span_idx = item['span_idx']\n",
    "\n",
    "        # --- Find the article file using glob ---\n",
    "        search_pattern = os.path.join(self.wrapped_articles_dir, f\"*article{article_id}.txt\")\n",
    "        possible_files = glob.glob(search_pattern)\n",
    "        \n",
    "        article_path = \"\"\n",
    "        if len(possible_files) == 1:\n",
    "            article_path = possible_files[0]\n",
    "        elif len(possible_files) == 0:\n",
    "             raise FileNotFoundError(f\"Missing article file for {article_id}. Searched pattern: {search_pattern}\")\n",
    "        else:\n",
    "            # Handle case where multiple files match (e.g., if cleanup wasn't perfect)\n",
    "            # Prioritize non-prefixed if available, otherwise raise error or take first.\n",
    "            non_prefixed_path = os.path.join(self.wrapped_articles_dir, f\"article{article_id}.txt\")\n",
    "            if non_prefixed_path in possible_files:\n",
    "                 article_path = non_prefixed_path\n",
    "                 print(f\"Warning: Found multiple files for article {article_id}, using non-prefixed: {article_path}\")\n",
    "            else:\n",
    "                 # If non-prefixed isn't among them, just take the first one found and warn.\n",
    "                 article_path = possible_files[0]\n",
    "                 print(f\"Warning: Found multiple files for article {article_id}: {possible_files}. Using the first one: {article_path}\")\n",
    "        # --- End file finding logic ---\n",
    "\n",
    "        try:\n",
    "            with open(article_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading article {article_id} from {article_path}: {e}\")\n",
    "            raise e\n",
    "\n",
    "        start_marker = f\"<<S_{span_idx}>>\"\n",
    "        end_marker = f\"<</S_{span_idx}>>\"\n",
    "        processed_text = text.replace(start_marker, SPAN_START_TOKEN, 1)\n",
    "        processed_text = processed_text.replace(end_marker, SPAN_END_TOKEN, 1)\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            processed_text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        start_token_indices = torch.where(input_ids == self.start_token_id)[0]\n",
    "        \n",
    "        if len(start_token_indices) == 0:\n",
    "            # If the start token is truncated out, use index 0 (like CLS)\n",
    "            # This might happen if the span is very late in a long document.\n",
    "            start_token_idx = 0 \n",
    "            # print(f\"Warning: SPAN_START_TOKEN truncated for article {article_id}, span {span_idx}. Using index 0.\") # Optional warning\n",
    "        else:\n",
    "            start_token_idx = start_token_indices[0].item()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'start_token_idx': torch.tensor(start_token_idx, dtype=torch.long),\n",
    "            'labels': torch.tensor(label_id, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee36d887",
   "metadata": {},
   "source": [
    "## 3. Initialize Tokenizer\n",
    "\n",
    "Load the tokenizer and add the special span marker tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fa0c355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2 special tokens: ['[SPAN_START]', '[SPAN_END]']\n",
      "ID for [SPAN_START]: 250002\n",
      "ID for [SPAN_END]: 250003\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add special tokens\n",
    "num_added_toks = tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "print(f\"Added {num_added_toks} special tokens: {SPECIAL_TOKENS['additional_special_tokens']}\")\n",
    "\n",
    "# Verify tokens are added\n",
    "print(f\"ID for {SPAN_START_TOKEN}: {tokenizer.convert_tokens_to_ids(SPAN_START_TOKEN)}\")\n",
    "print(f\"ID for {SPAN_END_TOKEN}: {tokenizer.convert_tokens_to_ids(SPAN_END_TOKEN)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888697c",
   "metadata": {},
   "source": [
    "## 4. Define Custom Model\n",
    "\n",
    "Create a model that uses the hidden state of the `[SPAN_START]` token for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5bbe893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpanClassifierModel(XLMRobertaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = AutoModel.from_config(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # Add a placeholder for class weights\n",
    "        self.class_weights = None\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        start_token_idx=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = outputs[0]\n",
    "\n",
    "        if start_token_idx is None:\n",
    "            raise ValueError(\"start_token_idx must be provided to SpanClassifierModel\")\n",
    "        \n",
    "        # Ensure start_token_idx is on the same device as last_hidden_state\n",
    "        idx = start_token_idx.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, last_hidden_state.shape[-1]).to(last_hidden_state.device)\n",
    "        start_token_hidden_state = last_hidden_state.gather(1, idx).squeeze(1)\n",
    "\n",
    "        pooled_output = self.dropout(start_token_hidden_state)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Use class weights if they are set and move labels to the correct device\n",
    "            loss_weights = self.class_weights.to(logits.device) if self.class_weights is not None else None\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=loss_weights)\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1).to(logits.device))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27528f98",
   "metadata": {},
   "source": [
    "## 5. Prepare Data for Training\n",
    "\n",
    "Split the data, create Dataset instances, and define a data collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "137d229e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 32859\n",
      "Validation set size: 3652\n",
      "\n",
      "Calculated class weights:\n",
      "  Class 0 (Appeal_to_Authority): 2.1646\n",
      "  Class 1 (Appeal_to_Fear-Prejudice): 0.9205\n",
      "  Class 2 (Appeal_to_Hypocrisy): 1.6143\n",
      "  Class 3 (Appeal_to_Popularity): 4.3824\n",
      "  Class 4 (Appeal_to_Time): 9.0421\n",
      "  Class 5 (Appeal_to_Values): 2.2288\n",
      "  Class 6 (Causal_Oversimplification): 2.6704\n",
      "  Class 7 (Consequential_Oversimplification): 4.2268\n",
      "  Class 8 (Conversation_Killer): 1.5927\n",
      "  Class 9 (Doubt): 0.3335\n",
      "  Class 10 (Exaggeration-Minimisation): 0.8907\n",
      "  Class 11 (False_Dilemma-No_Choice): 3.2396\n",
      "  Class 12 (Flag_Waving): 2.0037\n",
      "  Class 13 (Guilt_by_Association): 2.3692\n",
      "  Class 14 (Loaded_Language): 0.1725\n",
      "  Class 15 (Name_Calling-Labeling): 0.2359\n",
      "  Class 16 (Obfuscation-Vagueness-Confusion): 4.2902\n",
      "  Class 17 (Questioning_the_Reputation): 0.6803\n",
      "  Class 18 (Red_Herring): 7.5590\n",
      "  Class 19 (Repetition): 1.2621\n",
      "  Class 20 (Slogans): 2.0439\n",
      "  Class 21 (Straw_Man): 4.9095\n",
      "  Class 22 (Whataboutism): 10.2781\n",
      "\n",
      "Sample dataset item:\n",
      "{'input_ids': torch.Size([512]), 'attention_mask': torch.Size([512]), 'start_token_idx': torch.Size([]), 'labels': torch.Size([])}\n",
      "\n",
      "Sample collated batch:\n",
      "{'input_ids': torch.Size([2, 512]), 'attention_mask': torch.Size([2, 512]), 'start_token_idx': torch.Size([2]), 'labels': torch.Size([2])}\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and validation sets\n",
    "train_df, val_df = train_test_split(\n",
    "    label_df, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=label_df['label_id']\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "\n",
    "# --- Calculate Class Weights from Training Data ---\n",
    "class_counts = train_df['label_id'].value_counts().sort_index()\n",
    "total_samples = len(train_df)\n",
    "# Calculate weights as inverse frequency, smoothed or normalized\n",
    "# Simple inverse frequency:\n",
    "# weights = total_samples / (len(class_counts) * class_counts)\n",
    "# Another common approach (sklearn's compute_class_weight 'balanced'):\n",
    "weights = total_samples / (len(class_counts) * class_counts)\n",
    "class_weights_tensor = torch.tensor(weights.values, dtype=torch.float)\n",
    "print(\"\\nCalculated class weights:\")\n",
    "for i, w in enumerate(weights):\n",
    "    print(f\"  Class {i} ({id2label[i]}): {w:.4f}\")\n",
    "# --- End Class Weight Calculation ---\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = SpanClassificationDataset(train_df, tokenizer, WRAPPED_ARTICLES_DIR, MAX_LENGTH)\n",
    "val_dataset = SpanClassificationDataset(val_df, tokenizer, WRAPPED_ARTICLES_DIR, MAX_LENGTH)\n",
    "\n",
    "# Data Collator - pads sequences dynamically per batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Test the dataset and collator with one item\n",
    "try:\n",
    "    sample_item = train_dataset[0]\n",
    "    print(\"\\nSample dataset item:\")\n",
    "    print({k: v.shape if hasattr(v, 'shape') else v for k, v in sample_item.items()})\n",
    "    batch = data_collator([train_dataset[i] for i in range(2)])\n",
    "    print(\"\\nSample collated batch:\")\n",
    "    print({k: v.shape if hasattr(v, 'shape') else v for k, v in batch.items()})\n",
    "    assert 'start_token_idx' in batch, \"start_token_idx missing from collated batch!\"\n",
    "except Exception as e:\n",
    "    print(f\"Error testing dataset/collator: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7579f5",
   "metadata": {},
   "source": [
    "## 6. Configure Training\n",
    "\n",
    "Define metrics function and training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1685aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_macro': f1,\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=LOGGING_DIR,\n",
    "    logging_steps=100, # Optional: Log less frequently\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"tensorboard\",   \n",
    "    remove_unused_columns=False, \n",
    "    fp16=torch.cuda.is_available(), # Enable mixed precision if CUDA is available\n",
    "    # dataloader_num_workers=4, # Optional: Increase num_workers if data loading is slow\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b7ccff",
   "metadata": {},
   "source": [
    "## 7. Initialize Model and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "236ab124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SpanClassifierModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Assigned class weights to the model.\n",
      "Assigned class weights to the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1122/2273628289.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Ensure model config has correct label mappings\n",
    "model = SpanClassifierModel.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=num_labels, \n",
    "    label2id=label2id, \n",
    "    id2label=id2label\n",
    ")\n",
    "\n",
    "# Resize embeddings to match tokenizer vocab size (including special tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Assign Class Weights to Model ---\n",
    "# Move weights tensor to the correct device\n",
    "model.class_weights = class_weights_tensor.to(device)\n",
    "print(\"Assigned class weights to the model.\")\n",
    "# --- End Assign Class Weights ---\n",
    "\n",
    "# Move model to the correct device\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0082528",
   "metadata": {},
   "source": [
    "## 8. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca9b690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4113' max='6162' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4113/6162 1:04:57 < 32:22, 1.05 it/s, Epoch 2.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.147000</td>\n",
       "      <td>3.150713</td>\n",
       "      <td>0.034502</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.163800</td>\n",
       "      <td>3.143060</td>\n",
       "      <td>0.048740</td>\n",
       "      <td>0.004041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m     torch.cuda.empty_cache()\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining finished.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Save training metrics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/trainer.py:2514\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2512\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2513\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2514\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2515\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[32m   2516\u001b[39m     step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/trainer.py:5243\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5241\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5243\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5244\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5245\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/accelerate/data_loader.py:577\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    575\u001b[39m     current_batch = send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m.device, non_blocking=\u001b[38;5;28mself\u001b[39m._non_blocking)\n\u001b[32m    576\u001b[39m \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m next_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_index >= \u001b[38;5;28mself\u001b[39m.skip_batches:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mSpanClassificationDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     51\u001b[39m processed_text = text.replace(start_marker, SPAN_START_TOKEN, \u001b[32m1\u001b[39m)\n\u001b[32m     52\u001b[39m processed_text = processed_text.replace(end_marker, SPAN_END_TOKEN, \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m encoding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessed_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax_length\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     60\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m input_ids = encoding[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].squeeze(\u001b[32m0\u001b[39m)\n\u001b[32m     63\u001b[39m attention_mask = encoding[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m].squeeze(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2887\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2885\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2886\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2887\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2888\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2889\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2997\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   2975\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_encode_plus(\n\u001b[32m   2976\u001b[39m         batch_text_or_text_pairs=batch_text_or_text_pairs,\n\u001b[32m   2977\u001b[39m         add_special_tokens=add_special_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2994\u001b[39m         **kwargs,\n\u001b[32m   2995\u001b[39m     )\n\u001b[32m   2996\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2997\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2998\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3000\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3005\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3007\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3016\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3017\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3018\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3073\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   3063\u001b[39m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[32m   3064\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3065\u001b[39m     padding=padding,\n\u001b[32m   3066\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3070\u001b[39m     **kwargs,\n\u001b[32m   3071\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3086\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3090\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3091\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3092\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msplit_special_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3093\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3094\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:613\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m    589\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_encode_plus\u001b[39m(\n\u001b[32m    590\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    591\u001b[39m     text: Union[TextInput, PreTokenizedInput],\n\u001b[32m   (...)\u001b[39m\u001b[32m    610\u001b[39m     **kwargs,\n\u001b[32m    611\u001b[39m ) -> BatchEncoding:\n\u001b[32m    612\u001b[39m     batched_input = [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m     batched_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    635\u001b[39m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[32m    636\u001b[39m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:539\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens != split_special_tokens:\n\u001b[32m    537\u001b[39m     \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens = split_special_tokens\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[32m    546\u001b[39m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[32m    547\u001b[39m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[32m    548\u001b[39m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[32m    549\u001b[39m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[32m    551\u001b[39m tokens_and_encodings = [\n\u001b[32m    552\u001b[39m     \u001b[38;5;28mself\u001b[39m._convert_encoding(\n\u001b[32m    553\u001b[39m         encoding=encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    562\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[32m    563\u001b[39m ]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Before starting training, clear cache if needed\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Save training metrics\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "# Save the final model and tokenizer\n",
    "trainer.save_model() \n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12ddace",
   "metadata": {},
   "source": [
    "## 9. Evaluate the Model\n",
    "\n",
    "Evaluate the best model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c7b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating the best model on the validation set...\")\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation finished.\")\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc0edea",
   "metadata": {},
   "source": [
    "## Clear Model from Memory\n",
    "\n",
    "Delete the model and trainer objects and clear the GPU cache to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9fb6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "print(\"Clearing model and trainer from memory...\")\n",
    "\n",
    "# Check if variables exist before deleting\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    del model\n",
    "    print(\"  Deleted 'model' variable.\")\n",
    "if 'trainer' in locals() or 'trainer' in globals():\n",
    "    del trainer \n",
    "    print(\"  Deleted 'trainer' variable.\")\n",
    "if 'train_dataset' in locals() or 'train_dataset' in globals():\n",
    "    del train_dataset\n",
    "    print(\"  Deleted 'train_dataset'.\")\n",
    "if 'val_dataset' in locals() or 'val_dataset' in globals():\n",
    "    del val_dataset\n",
    "    print(\"  Deleted 'val_dataset'.\")\n",
    "if 'label_df' in locals() or 'label_df' in globals():\n",
    "    del label_df\n",
    "    print(\"  Deleted 'label_df'.\")\n",
    "\n",
    "# Run Python's garbage collector\n",
    "gc.collect()\n",
    "print(\"  Ran garbage collector.\")\n",
    "\n",
    "# Clear PyTorch's CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"  Cleared PyTorch CUDA cache.\")\n",
    "else:\n",
    "    print(\"  CUDA not available, skipping cache clearing.\")\n",
    "\n",
    "print(\"Memory clearing process finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bb96e",
   "metadata": {},
   "source": [
    "---\n",
    "## Obsolete Code (Commented Out)\n",
    "\n",
    "The following cells contained previous data loading/analysis code which is no longer needed for the current span classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4b3c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OBSOLETE CODE ---\n",
    "# Original data loading utilities - replaced by new loading logic above\n",
    "# import os, re\n",
    "# from collections import defaultdict\n",
    "# from typing import List, Tuple, Dict, Set\n",
    "# from pathlib import Path # Added import\n",
    "# \n",
    "# def read_article(path: str) -> str:\n",
    "#     with open(path, \"rb\") as f:\n",
    "#         raw = f.read()\n",
    "#     return raw.decode(\"utf-8\", \"ignore\")\n",
    "# \n",
    "# def load_span_labels(label_file: str\n",
    "#     ) -> Dict[str, List[Tuple[str, int, int]]]:\n",
    "#     \n",
    "#     spans = defaultdict(list)\n",
    "#     with open(label_file, encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             # Handle potential empty lines or lines with incorrect format\n",
    "#             parts = line.rstrip().split(\"\\t\")\n",
    "#             if len(parts) == 4:\n",
    "#                 art_id, lab, s, e = parts\n",
    "#                 try:\n",
    "#                     spans[art_id].append((lab, int(s), int(e)))\n",
    "#                 except ValueError:\n",
    "#                     print(f\"Warning: Skipping malformed line in {label_file}: {line.rstrip()}\")\n",
    "#             elif line.strip(): # Print warning for non-empty, but malformed lines\n",
    "#                  print(f\"Warning: Skipping malformed line in {label_file}: {line.rstrip()}\")\n",
    "#     return spans\n",
    "# \n",
    "# # Added function to extract base classes from the span file\n",
    "# def get_base_classes_from_spans(label_file: str) -> Set[str]:\n",
    "#     base_classes = set()\n",
    "#     with open(label_file, encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             parts = line.rstrip().split(\"\\t\")\n",
    "#             if len(parts) == 4:\n",
    "#                 _, lab, _, _ = parts\n",
    "#                 base_classes.add(lab)\n",
    "#             elif line.strip():\n",
    "#                  # Warnings handled in load_span_labels, no need to repeat here\n",
    "#                  pass\n",
    "#     return base_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c69b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OBSOLETE CODE ---\n",
    "# Original span printing utility\n",
    "# def print_span(article_number, start_offset, end_offset, lang=\"en\", base_path=\"data/raw\"):\n",
    "#     article_path = f\"{base_path}/{lang}/train-articles-subtask-3/{lang}_article{article_number}.txt\"\n",
    "#     with open(article_path, \"rb\") as f:\n",
    "#         raw = f.read()\n",
    "#     text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
    "#     span = text[start_offset:end_offset]\n",
    "#     print(span)\n",
    "#     \n",
    "# import pandas as pd\n",
    "# \n",
    "# df = pd.read_csv(\"../data/processed/ru/train-labels-subtask-3-spans-en.txt\", sep=\"\\t\", header=None, names=[\"article_id\", \"label\", \"start\", \"end\"])\n",
    "# \n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89483cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Article ID: 773520636\n",
      "Total spans: 74\n",
      "============================================================\n",
      "\n",
      "Span 7452\n",
      "Label: Loaded_Language\n",
      "Position: 52 to 70\n",
      "Text: ' '\n",
      "\n",
      "Span 7453\n",
      "Label: Loaded_Language\n",
      "Position: 148 to 165\n",
      "Text: ' '\n",
      "\n",
      "Span 7454\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 301 to 343\n",
      "Text: ',   '\n",
      "\n",
      "Span 7455\n",
      "Label: Doubt\n",
      "Position: 606 to 719\n",
      "Text: '         ,       '\n",
      "\n",
      "Span 7456\n",
      "Label: Doubt\n",
      "Position: 808 to 988\n",
      "Text: ',            !   ,   ,  ,   ,    '\n",
      "\n",
      "Span 7457\n",
      "Label: Flag_Waving\n",
      "Position: 1535 to 1669\n",
      "Text: ',      -    ,       '\n",
      "\n",
      "Span 7458\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 1738 to 1763\n",
      "Text: '  '\n",
      "\n",
      "Span 7459\n",
      "Label: Repetition\n",
      "Position: 1885 to 1891\n",
      "Text: ''\n",
      "\n",
      "Span 7460\n",
      "Label: Slogans\n",
      "Position: 1957 to 2000\n",
      "Text: '     '\n",
      "\n",
      "Span 7461\n",
      "Label: Slogans\n",
      "Position: 2059 to 2088\n",
      "Text: ' ,   '\n",
      "\n",
      "Span 7462\n",
      "Label: Slogans\n",
      "Position: 2142 to 2179\n",
      "Text: '     '\n",
      "\n",
      "Span 7463\n",
      "Label: Appeal_to_Authority\n",
      "Position: 2312 to 2376\n",
      "Text: ' ,    , '\n",
      "\n",
      "Span 7464\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 2703 to 2721\n",
      "Text: '  '\n",
      "\n",
      "Span 7465\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 2859 to 2883\n",
      "Text: ' '\n",
      "\n",
      "Span 7466\n",
      "Label: Doubt\n",
      "Position: 3015 to 3073\n",
      "Text: '   ,     '\n",
      "\n",
      "Span 7467\n",
      "Label: Slogans\n",
      "Position: 3271 to 3318\n",
      "Text: '   ,      '\n",
      "\n",
      "Span 7468\n",
      "Label: Appeal_to_Fear-Prejudice\n",
      "Position: 3621 to 3734\n",
      "Text: '  ,     ,      '\n",
      "\n",
      "Span 7469\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 3992 to 4068\n",
      "Text: ' , ,        '\n",
      "\n",
      "Span 7470\n",
      "Label: Loaded_Language\n",
      "Position: 4629 to 4642\n",
      "Text: ' '\n",
      "\n",
      "Span 7471\n",
      "Label: Loaded_Language\n",
      "Position: 4765 to 4777\n",
      "Text: ', '\n",
      "\n",
      "Span 7472\n",
      "Label: Loaded_Language\n",
      "Position: 5062 to 5074\n",
      "Text: ' '\n",
      "\n",
      "Span 7473\n",
      "Label: Repetition\n",
      "Position: 5243 to 5253\n",
      "Text: ', '\n",
      "\n",
      "Span 7474\n",
      "Label: False_Dilemma-No_Choice\n",
      "Position: 5532 to 5625\n",
      "Text: '       ,      '\n",
      "\n",
      "Span 7475\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 5758 to 5779\n",
      "Text: ' '\n",
      "\n",
      "Span 7476\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 5794 to 5802\n",
      "Text: ''\n",
      "\n",
      "Span 7478\n",
      "Label: Doubt\n",
      "Position: 5860 to 5960\n",
      "Text: ':     ,       '\n",
      "\n",
      "Span 7477\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 5903 to 5960\n",
      "Text: ',       '\n",
      "\n",
      "Span 7479\n",
      "Label: Loaded_Language\n",
      "Position: 5989 to 6005\n",
      "Text: ', '\n",
      "\n",
      "Span 7481\n",
      "Label: Doubt\n",
      "Position: 6009 to 6131\n",
      "Text: '    ,   ,          '\n",
      "\n",
      "Span 7480\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6041 to 6131\n",
      "Text: ',   ,          '\n",
      "\n",
      "Span 7482\n",
      "Label: Loaded_Language\n",
      "Position: 6168 to 6184\n",
      "Text: ', '\n",
      "\n",
      "Span 7485\n",
      "Label: Doubt\n",
      "Position: 6188 to 6254\n",
      "Text: '    , ,     '\n",
      "\n",
      "Span 7483\n",
      "Label: Repetition\n",
      "Position: 6200 to 6203\n",
      "Text: ''\n",
      "\n",
      "Span 7484\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6200 to 6254\n",
      "Text: '   , ,     '\n",
      "\n",
      "Span 7486\n",
      "Label: Repetition\n",
      "Position: 6256 to 6272\n",
      "Text: ', '\n",
      "\n",
      "Span 7487\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6344 to 6399\n",
      "Text: ', ,   '\n",
      "\n",
      "Span 7488\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6400 to 6404\n",
      "Text: ''\n",
      "\n",
      "Span 7489\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6579 to 6593\n",
      "Text: ' '\n",
      "\n",
      "Span 7490\n",
      "Label: Loaded_Language\n",
      "Position: 6742 to 6751\n",
      "Text: ''\n",
      "\n",
      "Span 7491\n",
      "Label: Loaded_Language\n",
      "Position: 6824 to 6842\n",
      "Text: '  '\n",
      "\n",
      "Span 7492\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6900 to 6951\n",
      "Text: '   '\n",
      "\n",
      "Span 7493\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 7082 to 7132\n",
      "Text: '   '\n",
      "\n",
      "Span 7494\n",
      "Label: Loaded_Language\n",
      "Position: 7261 to 7273\n",
      "Text: ', '\n",
      "\n",
      "Span 7495\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 7416 to 7436\n",
      "Text: '   '\n",
      "\n",
      "Span 7496\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 7482 to 7511\n",
      "Text: ',  '\n",
      "\n",
      "Span 7497\n",
      "Label: Loaded_Language\n",
      "Position: 7815 to 7859\n",
      "Text: '      '\n",
      "\n",
      "Span 7499\n",
      "Label: Doubt\n",
      "Position: 7862 to 7991\n",
      "Text: ' ,            ,       '\n",
      "\n",
      "Span 7498\n",
      "Label: Loaded_Language\n",
      "Position: 7894 to 7940\n",
      "Text: '      '\n",
      "\n",
      "Span 7500\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8042 to 8065\n",
      "Text: '  '\n",
      "\n",
      "Span 7501\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8260 to 8278\n",
      "Text: '  '\n",
      "\n",
      "Span 7502\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8435 to 8465\n",
      "Text: '    '\n",
      "\n",
      "Span 7503\n",
      "Label: Appeal_to_Fear-Prejudice\n",
      "Position: 8488 to 8520\n",
      "Text: '  , '\n",
      "\n",
      "Span 7504\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8870 to 8875\n",
      "Text: ''\n",
      "\n",
      "Span 7505\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8893 to 8911\n",
      "Text: ' '\n",
      "\n",
      "Span 7506\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 9019 to 9026\n",
      "Text: ''\n",
      "\n",
      "Span 7507\n",
      "Label: Repetition\n",
      "Position: 9076 to 9106\n",
      "Text: '  '\n",
      "\n",
      "Span 7508\n",
      "Label: Loaded_Language\n",
      "Position: 9842 to 9859\n",
      "Text: ' '\n",
      "\n",
      "Span 7509\n",
      "Label: Appeal_to_Fear-Prejudice\n",
      "Position: 10107 to 10193\n",
      "Text: ',      ,        '\n",
      "\n",
      "Span 7510\n",
      "Label: Repetition\n",
      "Position: 10237 to 10243\n",
      "Text: ''\n",
      "\n",
      "Span 7511\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 10447 to 10473\n",
      "Text: '  '\n",
      "\n",
      "Span 7512\n",
      "Label: Repetition\n",
      "Position: 11536 to 11552\n",
      "Text: ''\n",
      "\n",
      "Span 7513\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 11536 to 11589\n",
      "Text: '    '\n",
      "\n",
      "Span 7514\n",
      "Label: Loaded_Language\n",
      "Position: 11741 to 11754\n",
      "Text: ' '\n",
      "\n",
      "Span 7515\n",
      "Label: Loaded_Language\n",
      "Position: 12105 to 12120\n",
      "Text: ', '\n",
      "\n",
      "Span 7516\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 12122 to 12146\n",
      "Text: ' '\n",
      "\n",
      "Span 7517\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 12149 to 12179\n",
      "Text: ' '\n",
      "\n",
      "Span 7518\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 13300 to 13323\n",
      "Text: '   '\n",
      "\n",
      "Span 7519\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 14092 to 14099\n",
      "Text: ''\n",
      "\n",
      "Span 7520\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 15381 to 15404\n",
      "Text: ',   '\n",
      "\n",
      "Span 7522\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 15449 to 15487\n",
      "Text: '   '\n",
      "\n",
      "Span 7521\n",
      "Label: Repetition\n",
      "Position: 15469 to 15474\n",
      "Text: ''\n",
      "\n",
      "Span 7523\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 15500 to 15541\n",
      "Text: '    '\n",
      "\n",
      "Span 7524\n",
      "Label: Repetition\n",
      "Position: 15769 to 15774\n",
      "Text: ''\n",
      "\n",
      "Span 7525\n",
      "Label: Doubt\n",
      "Position: 15778 to 16020\n",
      "Text: ' ,  ,    ,       12:21,       13,          ,    '\n",
      "\n",
      "============================================================\n",
      "Wrapped Article ID: 773520636\n",
      "============================================================\n",
      "\n",
      "Marked span #1\n",
      "Text: ' '\n",
      "\n",
      "Marked span #2\n",
      "Text: ' '\n",
      "\n",
      "Marked span #3\n",
      "Text: ',   '\n",
      "\n",
      "Marked span #4\n",
      "Text: '         ,       '\n",
      "\n",
      "Marked span #5\n",
      "Text: ',            !   ,   ,  ,   ,    '\n",
      "\n",
      "Marked span #6\n",
      "Text: ',      -    ,       '\n",
      "\n",
      "Marked span #7\n",
      "Text: '  '\n",
      "\n",
      "Marked span #8\n",
      "Text: ''\n",
      "\n",
      "Marked span #9\n",
      "Text: '     '\n",
      "\n",
      "Marked span #10\n",
      "Text: ' ,   '\n",
      "\n",
      "Marked span #11\n",
      "Text: '     '\n",
      "\n",
      "Marked span #12\n",
      "Text: ' ,    , '\n",
      "\n",
      "Marked span #13\n",
      "Text: '  '\n",
      "\n",
      "Marked span #14\n",
      "Text: ' '\n",
      "\n",
      "Marked span #15\n",
      "Text: '   ,     '\n",
      "\n",
      "Marked span #16\n",
      "Text: '   ,      '\n",
      "\n",
      "Marked span #17\n",
      "Text: '  ,     ,      '\n",
      "\n",
      "Marked span #18\n",
      "Text: ' , ,        '\n",
      "\n",
      "Marked span #19\n",
      "Text: ' '\n",
      "\n",
      "Marked span #20\n",
      "Text: ', '\n",
      "\n",
      "Marked span #21\n",
      "Text: ' '\n",
      "\n",
      "Marked span #22\n",
      "Text: ', '\n",
      "\n",
      "Marked span #23\n",
      "Text: '       ,      '\n",
      "\n",
      "Marked span #24\n",
      "Text: ' '\n",
      "\n",
      "Marked span #25\n",
      "Text: ''\n",
      "\n",
      "Marked span #26\n",
      "Text: ':     <<S_27>>,       <</S_27>>'\n",
      "\n",
      "Marked span #28\n",
      "Text: ', '\n",
      "\n",
      "Marked span #29\n",
      "Text: '    <<S_30>>,   ,          <</S_30>>'\n",
      "\n",
      "Marked span #31\n",
      "Text: ', '\n",
      "\n",
      "Marked span #32\n",
      "Text: ' <<S_33>><<S_34>><</S_34>>   , ,     <</S_33>>'\n",
      "\n",
      "Marked span #35\n",
      "Text: ', '\n",
      "\n",
      "Marked span #36\n",
      "Text: ', ,   '\n",
      "\n",
      "Marked span #37\n",
      "Text: ''\n",
      "\n",
      "Marked span #38\n",
      "Text: ' '\n",
      "\n",
      "Marked span #39\n",
      "Text: ''\n",
      "\n",
      "Marked span #40\n",
      "Text: '  '\n",
      "\n",
      "Marked span #41\n",
      "Text: '   '\n",
      "\n",
      "Marked span #42\n",
      "Text: '   '\n",
      "\n",
      "Marked span #43\n",
      "Text: ', '\n",
      "\n",
      "Marked span #44\n",
      "Text: '   '\n",
      "\n",
      "Marked span #45\n",
      "Text: ',  '\n",
      "\n",
      "Marked span #46\n",
      "Text: '      '\n",
      "\n",
      "Marked span #47\n",
      "Text: ' ,      <<S_48>>      <</S_48>>,       '\n",
      "\n",
      "Marked span #49\n",
      "Text: '  '\n",
      "\n",
      "Marked span #50\n",
      "Text: '  '\n",
      "\n",
      "Marked span #51\n",
      "Text: '    '\n",
      "\n",
      "Marked span #52\n",
      "Text: '  , '\n",
      "\n",
      "Marked span #53\n",
      "Text: ''\n",
      "\n",
      "Marked span #54\n",
      "Text: ' '\n",
      "\n",
      "Marked span #55\n",
      "Text: ''\n",
      "\n",
      "Marked span #56\n",
      "Text: '  '\n",
      "\n",
      "Marked span #57\n",
      "Text: ' '\n",
      "\n",
      "Marked span #58\n",
      "Text: ',      ,        '\n",
      "\n",
      "Marked span #59\n",
      "Text: ''\n",
      "\n",
      "Marked span #60\n",
      "Text: '  '\n",
      "\n",
      "Marked span #61\n",
      "Text: '<<S_62>><</S_62>>    '\n",
      "\n",
      "Marked span #63\n",
      "Text: ' '\n",
      "\n",
      "Marked span #64\n",
      "Text: ', '\n",
      "\n",
      "Marked span #65\n",
      "Text: ' '\n",
      "\n",
      "Marked span #66\n",
      "Text: ' '\n",
      "\n",
      "Marked span #68\n",
      "Text: '   '\n",
      "\n",
      "Marked span #69\n",
      "Text: ''\n",
      "\n",
      "Marked span #70\n",
      "Text: ',   '\n",
      "\n",
      "Marked span #71\n",
      "Text: '  <<S_72>><</S_72>> '\n",
      "\n",
      "Marked span #73\n",
      "Text: '    '\n",
      "\n",
      "Marked span #74\n",
      "Text: ''\n",
      "\n",
      "Marked span #75\n",
      "Text: ' ,  ,    ,       12:21,       13,          ,    '\n"
     ]
    }
   ],
   "source": [
    "# --- OBSOLETE CODE ---\n",
    "# Original span display functions\n",
    "# import re\n",
    "# # Function to display spans for a specific article\n",
    "# def show_article_spans(article_id, lang=\"en\", base_path=\"../data/processed/ru\"):\n",
    "#     # Get all spans for this specific article\n",
    "#     article_spans = df[df['article_id'] == article_id].sort_values(by='start')\n",
    "#     \n",
    "#     # Check if we found any spans\n",
    "#     if len(article_spans) == 0:\n",
    "#         print(f\"No spans found for article {article_id}\")\n",
    "#         return\n",
    "#     \n",
    "#     r = []\n",
    "#     # Try to get the original article text\n",
    "#     article_path = f\"{base_path}/unwrapped-articles/{lang}_article{article_id}.txt\"\n",
    "#     try:\n",
    "#         with open(article_path, \"rb\") as f:\n",
    "#             raw = f.read()\n",
    "#         article_text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
    "#         \n",
    "#         print(f\"\\n{'='*60}\")\n",
    "#         print(f\"Article ID: {article_id}\")\n",
    "#         print(f\"Total spans: {len(article_spans)}\")\n",
    "#         print(f\"{'='*60}\")\n",
    "#         \n",
    "#         # Display each span\n",
    "#         for index, row in article_spans.iterrows():\n",
    "#             label = row['label']\n",
    "#             start_offset = row['start']\n",
    "#             end_offset = row['end']\n",
    "#             span_text = article_text[start_offset:end_offset]\n",
    "#             r.append((label, start_offset, end_offset, span_text))\n",
    "#             \n",
    "#             print(f\"\\nSpan {index}\")\n",
    "#             print(f\"Label: {label}\")\n",
    "#             print(f\"Position: {start_offset} to {end_offset}\")\n",
    "#             print(f\"Text: '{span_text}'\")\n",
    "# \n",
    "#         return r\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error accessing article {article_id}: {e}\")\n",
    "# \n",
    "# # Function to show text between span markers in wrapped articles\n",
    "# def show_wrapped_spans(article_id, lang=\"en\", base_path=\"../data/processed/ru\"):\n",
    "#     # Try to get the wrapped article text\n",
    "#     article_path = f\"{base_path}/wrapped-articles/{lang}_article{article_id}.txt\"\n",
    "#     r = []\n",
    "#     try:\n",
    "#         with open(article_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#             wrapped_text = f.read()\n",
    "#         \n",
    "#         print(f\"\\n{'='*60}\")\n",
    "#         print(f\"Wrapped Article ID: {article_id}\")\n",
    "#         print(f\"{'='*60}\")\n",
    "#         \n",
    "#         # Find all span markers using regex\n",
    "#         spans = re.finditer(r'<<S_(\\d+)>>(.*?)<</S_\\1>>', wrapped_text, re.DOTALL)\n",
    "#         \n",
    "#         found_spans = False\n",
    "#         for i, span in enumerate(spans):\n",
    "#             found_spans = True\n",
    "#             span_number = span.group(1)\n",
    "#             span_text = span.group(2)\n",
    "#             print(f\"\\nMarked span #{span_number}\")\n",
    "#             print(f\"Text: '{span_text}'\")\n",
    "#             r.append((span_number, span_text))\n",
    "#         \n",
    "#         if not found_spans:\n",
    "#             print(f\"No marked spans found in wrapped article {article_id}\")\n",
    "#         return r\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error accessing wrapped article {article_id}: {e}\")\n",
    "# \n",
    "# # Example usage - show spans for a sample article\n",
    "# sample_article_id = df['article_id'].sample(1).iloc[0]\n",
    "# article_spans = show_article_spans(sample_article_id)\n",
    "# \n",
    "# # Show the wrapped version of the same article\n",
    "# wrapped_spans = show_wrapped_spans(sample_article_id)\n",
    "# \n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e01ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Article ID: 773520636\n",
      "Total spans: 74\n",
      "============================================================\n",
      "\n",
      "Span 7452\n",
      "Label: Loaded_Language\n",
      "Position: 52 to 70\n",
      "Text: ' '\n",
      "\n",
      "Span 7453\n",
      "Label: Loaded_Language\n",
      "Position: 148 to 165\n",
      "Text: ' '\n",
      "\n",
      "Span 7454\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 301 to 343\n",
      "Text: ',   '\n",
      "\n",
      "Span 7455\n",
      "Label: Doubt\n",
      "Position: 606 to 719\n",
      "Text: '         ,       '\n",
      "\n",
      "Span 7456\n",
      "Label: Doubt\n",
      "Position: 808 to 988\n",
      "Text: ',            !   ,   ,  ,   ,    '\n",
      "\n",
      "Span 7457\n",
      "Label: Flag_Waving\n",
      "Position: 1535 to 1669\n",
      "Text: ',      -    ,       '\n",
      "\n",
      "Span 7458\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 1738 to 1763\n",
      "Text: '  '\n",
      "\n",
      "Span 7459\n",
      "Label: Repetition\n",
      "Position: 1885 to 1891\n",
      "Text: ''\n",
      "\n",
      "Span 7460\n",
      "Label: Slogans\n",
      "Position: 1957 to 2000\n",
      "Text: '     '\n",
      "\n",
      "Span 7461\n",
      "Label: Slogans\n",
      "Position: 2059 to 2088\n",
      "Text: ' ,   '\n",
      "\n",
      "Span 7462\n",
      "Label: Slogans\n",
      "Position: 2142 to 2179\n",
      "Text: '     '\n",
      "\n",
      "Span 7463\n",
      "Label: Appeal_to_Authority\n",
      "Position: 2312 to 2376\n",
      "Text: ' ,    , '\n",
      "\n",
      "Span 7464\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 2703 to 2721\n",
      "Text: '  '\n",
      "\n",
      "Span 7465\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 2859 to 2883\n",
      "Text: ' '\n",
      "\n",
      "Span 7466\n",
      "Label: Doubt\n",
      "Position: 3015 to 3073\n",
      "Text: '   ,     '\n",
      "\n",
      "Span 7467\n",
      "Label: Slogans\n",
      "Position: 3271 to 3318\n",
      "Text: '   ,      '\n",
      "\n",
      "Span 7468\n",
      "Label: Appeal_to_Fear-Prejudice\n",
      "Position: 3621 to 3734\n",
      "Text: '  ,     ,      '\n",
      "\n",
      "Span 7469\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 3992 to 4068\n",
      "Text: ' , ,        '\n",
      "\n",
      "Span 7470\n",
      "Label: Loaded_Language\n",
      "Position: 4629 to 4642\n",
      "Text: ' '\n",
      "\n",
      "Span 7471\n",
      "Label: Loaded_Language\n",
      "Position: 4765 to 4777\n",
      "Text: ', '\n",
      "\n",
      "Span 7472\n",
      "Label: Loaded_Language\n",
      "Position: 5062 to 5074\n",
      "Text: ' '\n",
      "\n",
      "Span 7473\n",
      "Label: Repetition\n",
      "Position: 5243 to 5253\n",
      "Text: ', '\n",
      "\n",
      "Span 7474\n",
      "Label: False_Dilemma-No_Choice\n",
      "Position: 5532 to 5625\n",
      "Text: '       ,      '\n",
      "\n",
      "Span 7475\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 5758 to 5779\n",
      "Text: ' '\n",
      "\n",
      "Span 7476\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 5794 to 5802\n",
      "Text: ''\n",
      "\n",
      "Span 7478\n",
      "Label: Doubt\n",
      "Position: 5860 to 5960\n",
      "Text: ':     ,       '\n",
      "\n",
      "Span 7477\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 5903 to 5960\n",
      "Text: ',       '\n",
      "\n",
      "Span 7479\n",
      "Label: Loaded_Language\n",
      "Position: 5989 to 6005\n",
      "Text: ', '\n",
      "\n",
      "Span 7481\n",
      "Label: Doubt\n",
      "Position: 6009 to 6131\n",
      "Text: '    ,   ,          '\n",
      "\n",
      "Span 7480\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6041 to 6131\n",
      "Text: ',   ,          '\n",
      "\n",
      "Span 7482\n",
      "Label: Loaded_Language\n",
      "Position: 6168 to 6184\n",
      "Text: ', '\n",
      "\n",
      "Span 7485\n",
      "Label: Doubt\n",
      "Position: 6188 to 6254\n",
      "Text: '    , ,     '\n",
      "\n",
      "Span 7483\n",
      "Label: Repetition\n",
      "Position: 6200 to 6203\n",
      "Text: ''\n",
      "\n",
      "Span 7484\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6200 to 6254\n",
      "Text: '   , ,     '\n",
      "\n",
      "Span 7486\n",
      "Label: Repetition\n",
      "Position: 6256 to 6272\n",
      "Text: ', '\n",
      "\n",
      "Span 7487\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6344 to 6399\n",
      "Text: ', ,   '\n",
      "\n",
      "Span 7488\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6400 to 6404\n",
      "Text: ''\n",
      "\n",
      "Span 7489\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6579 to 6593\n",
      "Text: ' '\n",
      "\n",
      "Span 7490\n",
      "Label: Loaded_Language\n",
      "Position: 6742 to 6751\n",
      "Text: ''\n",
      "\n",
      "Span 7491\n",
      "Label: Loaded_Language\n",
      "Position: 6824 to 6842\n",
      "Text: '  '\n",
      "\n",
      "Span 7492\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6900 to 6951\n",
      "Text: '   '\n",
      "\n",
      "Span 7493\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 7082 to 7132\n",
      "Text: '   '\n",
      "\n",
      "Span 7494\n",
      "Label: Loaded_Language\n",
      "Position: 7261 to 7273\n",
      "Text: ', '\n",
      "\n",
      "Span 7495\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 7416 to 7436\n",
      "Text: '   '\n",
      "\n",
      "Span 7496\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 7482 to 7511\n",
      "Text: ',  '\n",
      "\n",
      "Span 7497\n",
      "Label: Loaded_Language\n",
      "Position: 7815 to 7859\n",
      "Text: '      '\n",
      "\n",
      "Span 7499\n",
      "Label: Doubt\n",
      "Position: 7862 to 7991\n",
      "Text: ' ,            ,       '\n",
      "\n",
      "Span 7498\n",
      "Label: Loaded_Language\n",
      "Position: 7894 to 7940\n",
      "Text: '      '\n",
      "\n",
      "Span 7500\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8042 to 8065\n",
      "Text: '  '\n",
      "\n",
      "Span 7501\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8260 to 8278\n",
      "Text: '  '\n",
      "\n",
      "Span 7502\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8435 to 8465\n",
      "Text: '    '\n",
      "\n",
      "Span 7503\n",
      "Label: Appeal_to_Fear-Prejudice\n",
      "Position: 8488 to 8520\n",
      "Text: '  , '\n",
      "\n",
      "Span 7504\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8870 to 8875\n",
      "Text: ''\n",
      "\n",
      "Span 7505\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8893 to 8911\n",
      "Text: ' '\n",
      "\n",
      "Span 7506\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 9019 to 9026\n",
      "Text: ''\n",
      "\n",
      "Span 7507\n",
      "Label: Repetition\n",
      "Position: 9076 to 9106\n",
      "Text: '  '\n",
      "\n",
      "Span 7508\n",
      "Label: Loaded_Language\n",
      "Position: 9842 to 9859\n",
      "Text: ' '\n",
      "\n",
      "Span 7509\n",
      "Label: Appeal_to_Fear-Prejudice\n",
      "Position: 10107 to 10193\n",
      "Text: ',      ,        '\n",
      "\n",
      "Span 7510\n",
      "Label: Repetition\n",
      "Position: 10237 to 10243\n",
      "Text: ''\n",
      "\n",
      "Span 7511\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 10447 to 10473\n",
      "Text: '  '\n",
      "\n",
      "Span 7512\n",
      "Label: Repetition\n",
      "Position: 11536 to 11552\n",
      "Text: ''\n",
      "\n",
      "Span 7513\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 11536 to 11589\n",
      "Text: '    '\n",
      "\n",
      "Span 7514\n",
      "Label: Loaded_Language\n",
      "Position: 11741 to 11754\n",
      "Text: ' '\n",
      "\n",
      "Span 7515\n",
      "Label: Loaded_Language\n",
      "Position: 12105 to 12120\n",
      "Text: ', '\n",
      "\n",
      "Span 7516\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 12122 to 12146\n",
      "Text: ' '\n",
      "\n",
      "Span 7517\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 12149 to 12179\n",
      "Text: ' '\n",
      "\n",
      "Span 7518\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 13300 to 13323\n",
      "Text: '   '\n",
      "\n",
      "Span 7519\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 14092 to 14099\n",
      "Text: ''\n",
      "\n",
      "Span 7520\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 15381 to 15404\n",
      "Text: ',   '\n",
      "\n",
      "Span 7522\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 15449 to 15487\n",
      "Text: '   '\n",
      "\n",
      "Span 7521\n",
      "Label: Repetition\n",
      "Position: 15469 to 15474\n",
      "Text: ''\n",
      "\n",
      "Span 7523\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 15500 to 15541\n",
      "Text: '    '\n",
      "\n",
      "Span 7524\n",
      "Label: Repetition\n",
      "Position: 15769 to 15774\n",
      "Text: ''\n",
      "\n",
      "Span 7525\n",
      "Label: Doubt\n",
      "Position: 15778 to 16020\n",
      "Text: ' ,  ,    ,       12:21,       13,          ,    '\n",
      "\n",
      "============================================================\n",
      "Wrapped Article ID: 773520636\n",
      "============================================================\n",
      "\n",
      "Marked span #1\n",
      "Text: ' '\n",
      "\n",
      "Marked span #2\n",
      "Text: ' '\n",
      "\n",
      "Marked span #3\n",
      "Text: ',   '\n",
      "\n",
      "Marked span #4\n",
      "Text: '         ,       '\n",
      "\n",
      "Marked span #5\n",
      "Text: ',            !   ,   ,  ,   ,    '\n",
      "\n",
      "Marked span #6\n",
      "Text: ',      -    ,       '\n",
      "\n",
      "Marked span #7\n",
      "Text: '  '\n",
      "\n",
      "Marked span #8\n",
      "Text: ''\n",
      "\n",
      "Marked span #9\n",
      "Text: '     '\n",
      "\n",
      "Marked span #10\n",
      "Text: ' ,   '\n",
      "\n",
      "Marked span #11\n",
      "Text: '     '\n",
      "\n",
      "Marked span #12\n",
      "Text: ' ,    , '\n",
      "\n",
      "Marked span #13\n",
      "Text: '  '\n",
      "\n",
      "Marked span #14\n",
      "Text: ' '\n",
      "\n",
      "Marked span #15\n",
      "Text: '   ,     '\n",
      "\n",
      "Marked span #16\n",
      "Text: '   ,      '\n",
      "\n",
      "Marked span #17\n",
      "Text: '  ,     ,      '\n",
      "\n",
      "Marked span #18\n",
      "Text: ' , ,        '\n",
      "\n",
      "Marked span #19\n",
      "Text: ' '\n",
      "\n",
      "Marked span #20\n",
      "Text: ', '\n",
      "\n",
      "Marked span #21\n",
      "Text: ' '\n",
      "\n",
      "Marked span #22\n",
      "Text: ', '\n",
      "\n",
      "Marked span #23\n",
      "Text: '       ,      '\n",
      "\n",
      "Marked span #24\n",
      "Text: ' '\n",
      "\n",
      "Marked span #25\n",
      "Text: ''\n",
      "\n",
      "Marked span #26\n",
      "Text: ':     <<S_27>>,       <</S_27>>'\n",
      "\n",
      "Marked span #28\n",
      "Text: ', '\n",
      "\n",
      "Marked span #29\n",
      "Text: '    <<S_30>>,   ,          <</S_30>>'\n",
      "\n",
      "Marked span #31\n",
      "Text: ', '\n",
      "\n",
      "Marked span #32\n",
      "Text: ' <<S_33>><<S_34>><</S_34>>   , ,     <</S_33>>'\n",
      "\n",
      "Marked span #35\n",
      "Text: ', '\n",
      "\n",
      "Marked span #36\n",
      "Text: ', ,   '\n",
      "\n",
      "Marked span #37\n",
      "Text: ''\n",
      "\n",
      "Marked span #38\n",
      "Text: ' '\n",
      "\n",
      "Marked span #39\n",
      "Text: ''\n",
      "\n",
      "Marked span #40\n",
      "Text: '  '\n",
      "\n",
      "Marked span #41\n",
      "Text: '   '\n",
      "\n",
      "Marked span #42\n",
      "Text: '   '\n",
      "\n",
      "Marked span #43\n",
      "Text: ', '\n",
      "\n",
      "Marked span #44\n",
      "Text: '   '\n",
      "\n",
      "Marked span #45\n",
      "Text: ',  '\n",
      "\n",
      "Marked span #46\n",
      "Text: '      '\n",
      "\n",
      "Marked span #47\n",
      "Text: ' ,      <<S_48>>      <</S_48>>,       '\n",
      "\n",
      "Marked span #49\n",
      "Text: '  '\n",
      "\n",
      "Marked span #50\n",
      "Text: '  '\n",
      "\n",
      "Marked span #51\n",
      "Text: '    '\n",
      "\n",
      "Marked span #52\n",
      "Text: '  , '\n",
      "\n",
      "Marked span #53\n",
      "Text: ''\n",
      "\n",
      "Marked span #54\n",
      "Text: ' '\n",
      "\n",
      "Marked span #55\n",
      "Text: ''\n",
      "\n",
      "Marked span #56\n",
      "Text: '  '\n",
      "\n",
      "Marked span #57\n",
      "Text: ' '\n",
      "\n",
      "Marked span #58\n",
      "Text: ',      ,        '\n",
      "\n",
      "Marked span #59\n",
      "Text: ''\n",
      "\n",
      "Marked span #60\n",
      "Text: '  '\n",
      "\n",
      "Marked span #61\n",
      "Text: '<<S_62>><</S_62>>    '\n",
      "\n",
      "Marked span #63\n",
      "Text: ' '\n",
      "\n",
      "Marked span #64\n",
      "Text: ', '\n",
      "\n",
      "Marked span #65\n",
      "Text: ' '\n",
      "\n",
      "Marked span #66\n",
      "Text: ' '\n",
      "\n",
      "Marked span #68\n",
      "Text: '   '\n",
      "\n",
      "Marked span #69\n",
      "Text: ''\n",
      "\n",
      "Marked span #70\n",
      "Text: ',   '\n",
      "\n",
      "Marked span #71\n",
      "Text: '  <<S_72>><</S_72>> '\n",
      "\n",
      "Marked span #73\n",
      "Text: '    '\n",
      "\n",
      "Marked span #74\n",
      "Text: ''\n",
      "\n",
      "Marked span #75\n",
      "Text: ' ,  ,    ,       12:21,       13,          ,    '\n",
      "\n",
      "Article Span Index Best Match Index Similarity Article Span Text                        Wrapped Span Text\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "0                  0                1.00                          \n",
      "1                  1                1.00                           \n",
      "2                  2                1.00 ,    ,   \n",
      "3                  3                1.00            \n",
      "4                  4                1.00 ,      ,     \n",
      "5                  5                1.00 ,      - ,      -\n",
      "6                  6                1.00                     \n",
      "7                  7                1.00                                    \n",
      "8                  8                1.00            \n",
      "9                  9                1.00  ,                ,   \n",
      "10                 10               1.00               \n",
      "11                 11               1.00  ,     ,   \n",
      "12                 12               1.00                            \n",
      "13                 13               1.00                    \n",
      "14                 14               1.00    ,      ,  \n",
      "15                 15               1.00    ,        ,    \n",
      "16                 16               1.00   ,      ,   \n",
      "17                 17               1.00  , ,    , ,  \n",
      "18                 18               1.00                               \n",
      "19                 19               1.00 ,                              , \n",
      "20                 20               1.00                                \n",
      "21                 21               1.00 ,                                , \n",
      "22                 22               1.00              \n",
      "23                 23               1.00                       \n",
      "24                 24               1.00                                  \n",
      "25                 25               0.92 :     :    \n",
      "--- article_span\n",
      "+++ wrapped_span\n",
      "@@ -1 +1 @@\n",
      "-:     ,       \n",
      "+:     <<S_27>>,       <</S_27>>\n",
      "26                 -                0.00       ,     NO MATCH\n",
      "27                 26               1.00 ,                          , \n",
      "28                 27               0.93     ,      <<S_30>>\n",
      "--- article_span\n",
      "+++ wrapped_span\n",
      "@@ -1 +1 @@\n",
      "-    ,   ,          \n",
      "+    <<S_30>>,   ,          <</S_30>>\n",
      "29                 -                0.00       ,   ,   NO MATCH\n",
      "30                 28               1.00 ,                          , \n",
      "31                 -                0.00           , , NO MATCH\n",
      "32                 -                0.00                                             NO MATCH\n",
      "33                 -                0.00          , ,    NO MATCH\n",
      "34                 30               1.00 ,                          , \n",
      "35                 31               1.00 , ,   , ,  \n",
      "36                 32               1.00                                      \n",
      "37                 33               1.00                              \n",
      "38                 34               1.00                                 \n",
      "39                 35               1.00                            \n",
      "40                 36               1.00      \n",
      "41                 37               1.00      \n",
      "42                 38               1.00 ,                              , \n",
      "43                 39               1.00                            \n",
      "44                 40               1.00 ,              ,  \n",
      "45                 41               1.00            \n",
      "46                 42               0.94  ,        ,      <<S_48>>\n",
      "--- article_span\n",
      "+++ wrapped_span\n",
      "@@ -1 +1 @@\n",
      "- ,            ,       \n",
      "+ ,      <<S_48>>      <</S_48>>,       \n",
      "47                 -                0.00             NO MATCH\n",
      "48                 43               1.00                       \n",
      "49                 44               1.00                            \n",
      "50                 45               1.00                    \n",
      "51                 46               1.00   ,            , \n",
      "52                 47               1.00                                     \n",
      "53                 48               1.00                          \n",
      "54                 49               1.00                                   \n",
      "55                 50               1.00                \n",
      "56                 51               1.00                           \n",
      "57                 52               1.00 ,      ,  ,      , \n",
      "58                 53               1.00                                    \n",
      "59                 54               1.00                    \n",
      "60                 -                0.00                                NO MATCH\n",
      "61                 55               0.86     <<S_62>><</S_62>> \n",
      "--- article_span\n",
      "+++ wrapped_span\n",
      "@@ -1 +1 @@\n",
      "-    \n",
      "+<<S_62>><</S_62>>    \n",
      "62                 56               1.00                               \n",
      "63                 57               1.00 ,                           , \n",
      "64                 58               1.00                    \n",
      "65                 59               1.00              \n",
      "66                 60               1.00                         \n",
      "67                 61               1.00                                   \n",
      "68                 62               1.00 ,                     ,   \n",
      "69                 63               0.82         <<S_72>><</S_72\n",
      "--- article_span\n",
      "+++ wrapped_span\n",
      "@@ -1 +1 @@\n",
      "-   \n",
      "+  <<S_72>><</S_72>> \n",
      "70                 -                0.00                                           NO MATCH\n",
      "71                 64               1.00          \n",
      "72                 65               1.00                                     \n",
      "73                 66               1.00  ,  ,    ,  ,  \n",
      "\n",
      "Wrapped spans not matched to any article span:\n",
      "Wrapped index 29:  <<S_33>><<S_34>><</S_34>>   ,\n"
     ]
    }
   ],
   "source": [
    "# --- OBSOLETE CODE ---\n",
    "# Original span comparison function\n",
    "# import difflib\n",
    "# \n",
    "# def compare_article_and_wrapped_span_texts(article_id, lang=\"en\", base_path=\"../data/processed/ru\", min_ratio=0.8):\n",
    "#     \"\"\"\n",
    "#     Compare the span texts from the article (offset-based) and wrapped (marker-based) versions,\n",
    "#     reporting the closest matches and their differences. Order is not assumed to be the same.\n",
    "#     \"\"\"\n",
    "#     # Get span texts from both sources\n",
    "#     article_spans = show_article_spans(article_id, lang=lang, base_path=base_path)\n",
    "#     wrapped_spans = show_wrapped_spans(article_id, lang=lang, base_path=base_path)\n",
    "# \n",
    "#     if not article_spans or not wrapped_spans:\n",
    "#         print(\"No spans found in one or both sources.\")\n",
    "#         return\n",
    "# \n",
    "#     # Extract just the span texts\n",
    "#     article_texts = [span[3].strip() for span in article_spans]\n",
    "#     wrapped_texts = [span[1].strip() for span in wrapped_spans]\n",
    "# \n",
    "#     # For each article span, find the best matching wrapped span (by similarity ratio)\n",
    "#     print(f\"\\n{'Article Span Index':<18} {'Best Match Index':<16} {'Similarity':<10} {'Article Span Text':<40} {'Wrapped Span Text'}\")\n",
    "#     print(\"-\" * 120)\n",
    "#     used_wrapped = set()\n",
    "#     for i, art_text in enumerate(article_texts):\n",
    "#         # Find the best match in wrapped_texts\n",
    "#         best_ratio = 0\n",
    "#         best_j = -1\n",
    "#         for j, wrap_text in enumerate(wrapped_texts):\n",
    "#             if j in used_wrapped:\n",
    "#                 continue\n",
    "#             ratio = difflib.SequenceMatcher(None, art_text, wrap_text).ratio()\n",
    "#             if ratio > best_ratio:\n",
    "#                 best_ratio = ratio\n",
    "#                 best_j = j\n",
    "#         # Optionally, only consider matches above a threshold\n",
    "#         if best_ratio >= min_ratio and best_j != -1:\n",
    "#             used_wrapped.add(best_j)\n",
    "#             print(f\"{i:<18} {best_j:<16} {best_ratio:.2f} {art_text[:40]:<40} {wrapped_texts[best_j][:40]}\")\n",
    "#             # Show diff if not identical\n",
    "#             if best_ratio < 1.0:\n",
    "#                 diff = difflib.unified_diff(\n",
    "#                     art_text.splitlines(), wrapped_texts[best_j].splitlines(),\n",
    "#                     fromfile='article_span', tofile='wrapped_span', lineterm=''\n",
    "#                 )\n",
    "#                 print('\\n'.join(diff))\n",
    "#         else:\n",
    "#             print(f\"{i:<18} {'-':<16} {'0.00':<10} {art_text[:40]:<40} {'NO MATCH'}\")\n",
    "# \n",
    "#     # Optionally, report wrapped spans that were not matched\n",
    "#     unmatched = [j for j in range(len(wrapped_texts)) if j not in used_wrapped]\n",
    "#     if unmatched:\n",
    "#         print(\"\\nWrapped spans not matched to any article span:\")\n",
    "#         for j in unmatched:\n",
    "#             print(f\"Wrapped index {j}: {wrapped_texts[j][:60]}\")\n",
    "# \n",
    "# # Example usage:\n",
    "# compare_article_and_wrapped_span_texts(sample_article_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
