{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7afddec3",
   "metadata": {},
   "source": [
    "# Span Classification Model Training\n",
    "\n",
    "This notebook trains a transformer model to classify the persuasive technique used in a marked text span within Russian articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dab74567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/twoface/persuasion-detection/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorWithPadding,\n",
    "    PreTrainedModel,\n",
    "    XLMRobertaPreTrainedModel\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# --- Constants ---\n",
    "BASE_DATA_PATH = \"../data/processed/ru/\"\n",
    "LABEL_FILES_PATTERN = os.path.join(BASE_DATA_PATH, \"train-labels-subtask-3-spans-*.txt\")\n",
    "WRAPPED_ARTICLES_DIR = os.path.join(BASE_DATA_PATH, \"wrapped-articles\")\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "SPAN_START_TOKEN = \"[SPAN_START]\"\n",
    "SPAN_END_TOKEN = \"[SPAN_END]\"\n",
    "SPECIAL_TOKENS = { \"additional_special_tokens\": [SPAN_START_TOKEN, SPAN_END_TOKEN] }\n",
    "MAX_LENGTH = 512\n",
    "TEST_SIZE = 0.1\n",
    "RANDOM_STATE = 42\n",
    "OUTPUT_DIR = \"./span_classification_model\"\n",
    "LOGGING_DIR = \"./span_classification_logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81326730",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "Load labels from all language files, assign a unique span index within each article, and create label mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d00bf1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found label files: ['../data/processed/ru/train-labels-subtask-3-spans-po.txt', '../data/processed/ru/train-labels-subtask-3-spans-it.txt', '../data/processed/ru/train-labels-subtask-3-spans-en.txt', '../data/processed/ru/train-labels-subtask-3-spans-ru.txt', '../data/processed/ru/train-labels-subtask-3-spans-ge.txt', '../data/processed/ru/train-labels-subtask-3-spans-fr.txt']\n",
      "Loaded 36511 labels from 6 files.\n",
      "Unique articles: 1550\n",
      "Unique labels: 23\n",
      "\n",
      "Number of unique labels: 23\n",
      "\n",
      "Sample of processed label data:\n",
      "      article_id                     label  span_idx  label_id\n",
      "17956  111111111                     Doubt         1         9\n",
      "17957  111111111       Appeal_to_Authority         2         0\n",
      "17958  111111111                Repetition         3        19\n",
      "17959  111111111  Appeal_to_Fear-Prejudice         4         1\n",
      "17960  111111111  Appeal_to_Fear-Prejudice         5         1\n"
     ]
    }
   ],
   "source": [
    "def load_all_labels(pattern: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads labels from all files matching the pattern and assigns span indices.\"\"\"\n",
    "    all_files = glob.glob(pattern)\n",
    "    df_list = []\n",
    "    print(f\"Found label files: {all_files}\")\n",
    "    for f in all_files:\n",
    "        try:\n",
    "            df_lang = pd.read_csv(f, sep=\"\\t\", header=None, names=[\"article_id\", \"label\", \"start\", \"end\"], dtype={'article_id': str})\n",
    "            df_list.append(df_lang)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {f}: {e}\")\n",
    "    \n",
    "    if not df_list:\n",
    "        raise ValueError(\"No label data loaded. Check LABEL_FILES_PATTERN.\")\n",
    "        \n",
    "    full_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # Sort by article_id and original position (start offset) to ensure consistent indexing\n",
    "    full_df = full_df.sort_values(by=['article_id', 'start'], ascending=[True, True])\n",
    "    \n",
    "    # Assign 1-based span index within each article group\n",
    "    full_df['span_idx'] = full_df.groupby('article_id').cumcount() + 1\n",
    "    \n",
    "    print(f\"Loaded {len(full_df)} labels from {len(all_files)} files.\")\n",
    "    print(f\"Unique articles: {full_df['article_id'].nunique()}\")\n",
    "    print(f\"Unique labels: {full_df['label'].nunique()}\")\n",
    "    return full_df[['article_id', 'label', 'span_idx']]\n",
    "\n",
    "# Load the data\n",
    "label_df = load_all_labels(LABEL_FILES_PATTERN)\n",
    "\n",
    "# Create label mappings\n",
    "unique_labels = sorted(label_df['label'].unique())\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "num_labels = len(unique_labels)\n",
    "\n",
    "print(f\"\\nNumber of unique labels: {num_labels}\")\n",
    "\n",
    "# Add label_id column to DataFrame\n",
    "label_df['label_id'] = label_df['label'].map(label2id)\n",
    "\n",
    "print(\"\\nSample of processed label data:\")\n",
    "print(label_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cea38e",
   "metadata": {},
   "source": [
    "## 2. Create PyTorch Dataset\n",
    "\n",
    "Define a dataset class to handle loading wrapped articles, replacing markers, tokenizing, and identifying the start marker position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf259a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpanClassificationDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer, wrapped_articles_dir: str, max_length: int):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.wrapped_articles_dir = wrapped_articles_dir\n",
    "        self.max_length = max_length\n",
    "        self.start_token_id = self.tokenizer.convert_tokens_to_ids(SPAN_START_TOKEN)\n",
    "        if self.start_token_id == self.tokenizer.unk_token_id:\n",
    "             print(f\"Warning: {SPAN_START_TOKEN} not found in tokenizer vocab after adding!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        article_id = item['article_id']\n",
    "        label_id = item['label_id']\n",
    "        span_idx = item['span_idx']\n",
    "\n",
    "        # --- Find the article file using glob ---\n",
    "        search_pattern = os.path.join(self.wrapped_articles_dir, f\"*article{article_id}.txt\")\n",
    "        possible_files = glob.glob(search_pattern)\n",
    "        \n",
    "        article_path = \"\"\n",
    "        if len(possible_files) == 1:\n",
    "            article_path = possible_files[0]\n",
    "        elif len(possible_files) == 0:\n",
    "             raise FileNotFoundError(f\"Missing article file for {article_id}. Searched pattern: {search_pattern}\")\n",
    "        else:\n",
    "            # Handle case where multiple files match (e.g., if cleanup wasn't perfect)\n",
    "            # Prioritize non-prefixed if available, otherwise raise error or take first.\n",
    "            non_prefixed_path = os.path.join(self.wrapped_articles_dir, f\"article{article_id}.txt\")\n",
    "            if non_prefixed_path in possible_files:\n",
    "                 article_path = non_prefixed_path\n",
    "                 print(f\"Warning: Found multiple files for article {article_id}, using non-prefixed: {article_path}\")\n",
    "            else:\n",
    "                 # If non-prefixed isn't among them, just take the first one found and warn.\n",
    "                 article_path = possible_files[0]\n",
    "                 print(f\"Warning: Found multiple files for article {article_id}: {possible_files}. Using the first one: {article_path}\")\n",
    "        # --- End file finding logic ---\n",
    "\n",
    "        try:\n",
    "            with open(article_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading article {article_id} from {article_path}: {e}\")\n",
    "            raise e\n",
    "\n",
    "        start_marker = f\"<<S_{span_idx}>>\"\n",
    "        end_marker = f\"<</S_{span_idx}>>\"\n",
    "        processed_text = text.replace(start_marker, SPAN_START_TOKEN, 1)\n",
    "        processed_text = processed_text.replace(end_marker, SPAN_END_TOKEN, 1)\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            processed_text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        start_token_indices = torch.where(input_ids == self.start_token_id)[0]\n",
    "        \n",
    "        if len(start_token_indices) == 0:\n",
    "            # If the start token is truncated out, use index 0 (like CLS)\n",
    "            # This might happen if the span is very late in a long document.\n",
    "            start_token_idx = 0 \n",
    "            # print(f\"Warning: SPAN_START_TOKEN truncated for article {article_id}, span {span_idx}. Using index 0.\") # Optional warning\n",
    "        else:\n",
    "            start_token_idx = start_token_indices[0].item()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'start_token_idx': torch.tensor(start_token_idx, dtype=torch.long),\n",
    "            'labels': torch.tensor(label_id, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee36d887",
   "metadata": {},
   "source": [
    "## 3. Initialize Tokenizer\n",
    "\n",
    "Load the tokenizer and add the special span marker tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fa0c355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2 special tokens: ['[SPAN_START]', '[SPAN_END]']\n",
      "ID for [SPAN_START]: 250002\n",
      "ID for [SPAN_END]: 250003\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add special tokens\n",
    "num_added_toks = tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "print(f\"Added {num_added_toks} special tokens: {SPECIAL_TOKENS['additional_special_tokens']}\")\n",
    "\n",
    "# Verify tokens are added\n",
    "print(f\"ID for {SPAN_START_TOKEN}: {tokenizer.convert_tokens_to_ids(SPAN_START_TOKEN)}\")\n",
    "print(f\"ID for {SPAN_END_TOKEN}: {tokenizer.convert_tokens_to_ids(SPAN_END_TOKEN)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888697c",
   "metadata": {},
   "source": [
    "## 4. Define Custom Model\n",
    "\n",
    "Create a model that uses the hidden state of the `[SPAN_START]` token for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5bbe893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpanClassifierModel(XLMRobertaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = AutoModel.from_config(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # Add a placeholder for class weights\n",
    "        self.class_weights = None\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        start_token_idx=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = outputs[0]\n",
    "\n",
    "        if start_token_idx is None:\n",
    "            raise ValueError(\"start_token_idx must be provided to SpanClassifierModel\")\n",
    "        \n",
    "        # Ensure start_token_idx is on the same device as last_hidden_state\n",
    "        idx = start_token_idx.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, last_hidden_state.shape[-1]).to(last_hidden_state.device)\n",
    "        start_token_hidden_state = last_hidden_state.gather(1, idx).squeeze(1)\n",
    "\n",
    "        pooled_output = self.dropout(start_token_hidden_state)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Use class weights if they are set and move labels to the correct device\n",
    "            loss_weights = self.class_weights.to(logits.device) if self.class_weights is not None else None\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=loss_weights)\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1).to(logits.device))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27528f98",
   "metadata": {},
   "source": [
    "## 5. Prepare Data for Training\n",
    "\n",
    "Split the data, create Dataset instances, and define a data collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "137d229e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 32859\n",
      "Validation set size: 3652\n",
      "\n",
      "Calculated class weights:\n",
      "  Class 0 (Appeal_to_Authority): 2.1646\n",
      "  Class 1 (Appeal_to_Fear-Prejudice): 0.9205\n",
      "  Class 2 (Appeal_to_Hypocrisy): 1.6143\n",
      "  Class 3 (Appeal_to_Popularity): 4.3824\n",
      "  Class 4 (Appeal_to_Time): 9.0421\n",
      "  Class 5 (Appeal_to_Values): 2.2288\n",
      "  Class 6 (Causal_Oversimplification): 2.6704\n",
      "  Class 7 (Consequential_Oversimplification): 4.2268\n",
      "  Class 8 (Conversation_Killer): 1.5927\n",
      "  Class 9 (Doubt): 0.3335\n",
      "  Class 10 (Exaggeration-Minimisation): 0.8907\n",
      "  Class 11 (False_Dilemma-No_Choice): 3.2396\n",
      "  Class 12 (Flag_Waving): 2.0037\n",
      "  Class 13 (Guilt_by_Association): 2.3692\n",
      "  Class 14 (Loaded_Language): 0.1725\n",
      "  Class 15 (Name_Calling-Labeling): 0.2359\n",
      "  Class 16 (Obfuscation-Vagueness-Confusion): 4.2902\n",
      "  Class 17 (Questioning_the_Reputation): 0.6803\n",
      "  Class 18 (Red_Herring): 7.5590\n",
      "  Class 19 (Repetition): 1.2621\n",
      "  Class 20 (Slogans): 2.0439\n",
      "  Class 21 (Straw_Man): 4.9095\n",
      "  Class 22 (Whataboutism): 10.2781\n",
      "\n",
      "Sample dataset item:\n",
      "{'input_ids': torch.Size([512]), 'attention_mask': torch.Size([512]), 'start_token_idx': torch.Size([]), 'labels': torch.Size([])}\n",
      "\n",
      "Sample collated batch:\n",
      "{'input_ids': torch.Size([2, 512]), 'attention_mask': torch.Size([2, 512]), 'start_token_idx': torch.Size([2]), 'labels': torch.Size([2])}\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and validation sets\n",
    "train_df, val_df = train_test_split(\n",
    "    label_df, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=label_df['label_id']\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "\n",
    "# --- Calculate Class Weights from Training Data ---\n",
    "class_counts = train_df['label_id'].value_counts().sort_index()\n",
    "total_samples = len(train_df)\n",
    "# Calculate weights as inverse frequency, smoothed or normalized\n",
    "# Simple inverse frequency:\n",
    "# weights = total_samples / (len(class_counts) * class_counts)\n",
    "# Another common approach (sklearn's compute_class_weight 'balanced'):\n",
    "weights = total_samples / (len(class_counts) * class_counts)\n",
    "class_weights_tensor = torch.tensor(weights.values, dtype=torch.float)\n",
    "print(\"\\nCalculated class weights:\")\n",
    "for i, w in enumerate(weights):\n",
    "    print(f\"  Class {i} ({id2label[i]}): {w:.4f}\")\n",
    "# --- End Class Weight Calculation ---\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = SpanClassificationDataset(train_df, tokenizer, WRAPPED_ARTICLES_DIR, MAX_LENGTH)\n",
    "val_dataset = SpanClassificationDataset(val_df, tokenizer, WRAPPED_ARTICLES_DIR, MAX_LENGTH)\n",
    "\n",
    "# Data Collator - pads sequences dynamically per batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Test the dataset and collator with one item\n",
    "try:\n",
    "    sample_item = train_dataset[0]\n",
    "    print(\"\\nSample dataset item:\")\n",
    "    print({k: v.shape if hasattr(v, 'shape') else v for k, v in sample_item.items()})\n",
    "    batch = data_collator([train_dataset[i] for i in range(2)])\n",
    "    print(\"\\nSample collated batch:\")\n",
    "    print({k: v.shape if hasattr(v, 'shape') else v for k, v in batch.items()})\n",
    "    assert 'start_token_idx' in batch, \"start_token_idx missing from collated batch!\"\n",
    "except Exception as e:\n",
    "    print(f\"Error testing dataset/collator: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7579f5",
   "metadata": {},
   "source": [
    "## 6. Configure Training\n",
    "\n",
    "Define metrics function and training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1685aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_macro': f1,\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=LOGGING_DIR,\n",
    "    logging_steps=100, # Optional: Log less frequently\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"tensorboard\",   \n",
    "    remove_unused_columns=False, \n",
    "    fp16=torch.cuda.is_available(), # Enable mixed precision if CUDA is available\n",
    "    # dataloader_num_workers=4, # Optional: Increase num_workers if data loading is slow\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b7ccff",
   "metadata": {},
   "source": [
    "## 7. Initialize Model and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "236ab124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SpanClassifierModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Assigned class weights to the model.\n",
      "Assigned class weights to the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1122/2273628289.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Ensure model config has correct label mappings\n",
    "model = SpanClassifierModel.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=num_labels, \n",
    "    label2id=label2id, \n",
    "    id2label=id2label\n",
    ")\n",
    "\n",
    "# Resize embeddings to match tokenizer vocab size (including special tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Assign Class Weights to Model ---\n",
    "# Move weights tensor to the correct device\n",
    "model.class_weights = class_weights_tensor.to(device)\n",
    "print(\"Assigned class weights to the model.\")\n",
    "# --- End Assign Class Weights ---\n",
    "\n",
    "# Move model to the correct device\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0082528",
   "metadata": {},
   "source": [
    "## 8. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca9b690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4113' max='6162' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4113/6162 1:04:57 < 32:22, 1.05 it/s, Epoch 2.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.147000</td>\n",
       "      <td>3.150713</td>\n",
       "      <td>0.034502</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.163800</td>\n",
       "      <td>3.143060</td>\n",
       "      <td>0.048740</td>\n",
       "      <td>0.004041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m     torch.cuda.empty_cache()\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining finished.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Save training metrics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/trainer.py:2514\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2512\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2513\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2514\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2515\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[32m   2516\u001b[39m     step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/trainer.py:5243\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5241\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5243\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5244\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5245\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/accelerate/data_loader.py:577\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    575\u001b[39m     current_batch = send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m.device, non_blocking=\u001b[38;5;28mself\u001b[39m._non_blocking)\n\u001b[32m    576\u001b[39m \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m next_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_index >= \u001b[38;5;28mself\u001b[39m.skip_batches:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mSpanClassificationDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     51\u001b[39m processed_text = text.replace(start_marker, SPAN_START_TOKEN, \u001b[32m1\u001b[39m)\n\u001b[32m     52\u001b[39m processed_text = processed_text.replace(end_marker, SPAN_END_TOKEN, \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m encoding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessed_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax_length\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     60\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m input_ids = encoding[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].squeeze(\u001b[32m0\u001b[39m)\n\u001b[32m     63\u001b[39m attention_mask = encoding[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m].squeeze(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2887\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2885\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2886\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2887\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2888\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2889\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2997\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   2975\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_encode_plus(\n\u001b[32m   2976\u001b[39m         batch_text_or_text_pairs=batch_text_or_text_pairs,\n\u001b[32m   2977\u001b[39m         add_special_tokens=add_special_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2994\u001b[39m         **kwargs,\n\u001b[32m   2995\u001b[39m     )\n\u001b[32m   2996\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2997\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2998\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3000\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3005\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3007\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3016\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3017\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3018\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3073\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   3063\u001b[39m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[32m   3064\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3065\u001b[39m     padding=padding,\n\u001b[32m   3066\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3070\u001b[39m     **kwargs,\n\u001b[32m   3071\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3086\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3090\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3091\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3092\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msplit_special_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3093\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3094\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:613\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m    589\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_encode_plus\u001b[39m(\n\u001b[32m    590\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    591\u001b[39m     text: Union[TextInput, PreTokenizedInput],\n\u001b[32m   (...)\u001b[39m\u001b[32m    610\u001b[39m     **kwargs,\n\u001b[32m    611\u001b[39m ) -> BatchEncoding:\n\u001b[32m    612\u001b[39m     batched_input = [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m     batched_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    635\u001b[39m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[32m    636\u001b[39m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:539\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens != split_special_tokens:\n\u001b[32m    537\u001b[39m     \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens = split_special_tokens\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[32m    546\u001b[39m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[32m    547\u001b[39m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[32m    548\u001b[39m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[32m    549\u001b[39m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[32m    551\u001b[39m tokens_and_encodings = [\n\u001b[32m    552\u001b[39m     \u001b[38;5;28mself\u001b[39m._convert_encoding(\n\u001b[32m    553\u001b[39m         encoding=encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    562\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[32m    563\u001b[39m ]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Before starting training, clear cache if needed\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Save training metrics\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "# Save the final model and tokenizer\n",
    "trainer.save_model() \n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12ddace",
   "metadata": {},
   "source": [
    "## 9. Evaluate the Model\n",
    "\n",
    "Evaluate the best model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c7b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating the best model on the validation set...\")\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation finished.\")\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc0edea",
   "metadata": {},
   "source": [
    "## Clear Model from Memory\n",
    "\n",
    "Delete the model and trainer objects and clear the GPU cache to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9fb6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "print(\"Clearing model and trainer from memory...\")\n",
    "\n",
    "# Check if variables exist before deleting\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    del model\n",
    "    print(\"  Deleted 'model' variable.\")\n",
    "if 'trainer' in locals() or 'trainer' in globals():\n",
    "    del trainer \n",
    "    print(\"  Deleted 'trainer' variable.\")\n",
    "if 'train_dataset' in locals() or 'train_dataset' in globals():\n",
    "    del train_dataset\n",
    "    print(\"  Deleted 'train_dataset'.\")\n",
    "if 'val_dataset' in locals() or 'val_dataset' in globals():\n",
    "    del val_dataset\n",
    "    print(\"  Deleted 'val_dataset'.\")\n",
    "if 'label_df' in locals() or 'label_df' in globals():\n",
    "    del label_df\n",
    "    print(\"  Deleted 'label_df'.\")\n",
    "\n",
    "# Run Python's garbage collector\n",
    "gc.collect()\n",
    "print(\"  Ran garbage collector.\")\n",
    "\n",
    "# Clear PyTorch's CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"  Cleared PyTorch CUDA cache.\")\n",
    "else:\n",
    "    print(\"  CUDA not available, skipping cache clearing.\")\n",
    "\n",
    "print(\"Memory clearing process finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bb96e",
   "metadata": {},
   "source": [
    "---\n",
    "## Obsolete Code (Commented Out)\n",
    "\n",
    "The following cells contained previous data loading/analysis code which is no longer needed for the current span classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4b3c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OBSOLETE CODE ---\n",
    "# Original data loading utilities - replaced by new loading logic above\n",
    "# import os, re\n",
    "# from collections import defaultdict\n",
    "# from typing import List, Tuple, Dict, Set\n",
    "# from pathlib import Path # Added import\n",
    "# \n",
    "# def read_article(path: str) -> str:\n",
    "#     with open(path, \"rb\") as f:\n",
    "#         raw = f.read()\n",
    "#     return raw.decode(\"utf-8\", \"ignore\")\n",
    "# \n",
    "# def load_span_labels(label_file: str\n",
    "#     ) -> Dict[str, List[Tuple[str, int, int]]]:\n",
    "#     \n",
    "#     spans = defaultdict(list)\n",
    "#     with open(label_file, encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             # Handle potential empty lines or lines with incorrect format\n",
    "#             parts = line.rstrip().split(\"\\t\")\n",
    "#             if len(parts) == 4:\n",
    "#                 art_id, lab, s, e = parts\n",
    "#                 try:\n",
    "#                     spans[art_id].append((lab, int(s), int(e)))\n",
    "#                 except ValueError:\n",
    "#                     print(f\"Warning: Skipping malformed line in {label_file}: {line.rstrip()}\")\n",
    "#             elif line.strip(): # Print warning for non-empty, but malformed lines\n",
    "#                  print(f\"Warning: Skipping malformed line in {label_file}: {line.rstrip()}\")\n",
    "#     return spans\n",
    "# \n",
    "# # Added function to extract base classes from the span file\n",
    "# def get_base_classes_from_spans(label_file: str) -> Set[str]:\n",
    "#     base_classes = set()\n",
    "#     with open(label_file, encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             parts = line.rstrip().split(\"\\t\")\n",
    "#             if len(parts) == 4:\n",
    "#                 _, lab, _, _ = parts\n",
    "#                 base_classes.add(lab)\n",
    "#             elif line.strip():\n",
    "#                  # Warnings handled in load_span_labels, no need to repeat here\n",
    "#                  pass\n",
    "#     return base_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c69b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OBSOLETE CODE ---\n",
    "# Original span printing utility\n",
    "# def print_span(article_number, start_offset, end_offset, lang=\"en\", base_path=\"data/raw\"):\n",
    "#     article_path = f\"{base_path}/{lang}/train-articles-subtask-3/{lang}_article{article_number}.txt\"\n",
    "#     with open(article_path, \"rb\") as f:\n",
    "#         raw = f.read()\n",
    "#     text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
    "#     span = text[start_offset:end_offset]\n",
    "#     print(span)\n",
    "#     \n",
    "# import pandas as pd\n",
    "# \n",
    "# df = pd.read_csv(\"../data/processed/ru/train-labels-subtask-3-spans-en.txt\", sep=\"\\t\", header=None, names=[\"article_id\", \"label\", \"start\", \"end\"])\n",
    "# \n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89483cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Article ID: 773520636\n",
      "Total spans: 74\n",
      "============================================================\n",
      "\n",
      "Span 7452\n",
      "Label: Loaded_Language\n",
      "Position: 52 to 70\n",
      "Text: 'покоряться тирании'\n",
      "\n",
      "Span 7453\n",
      "Label: Loaded_Language\n",
      "Position: 148 to 165\n",
      "Text: 'Воскликнул оратор'\n",
      "\n",
      "Span 7454\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 301 to 343\n",
      "Text: 'растерянного, библейски неграмотного друга'\n",
      "\n",
      "Span 7455\n",
      "Label: Doubt\n",
      "Position: 606 to 719\n",
      "Text: 'этот протестант должен был остаться дома и читать свою Библию, а не протестовать против коррупции в правительстве'\n",
      "\n",
      "Span 7456\n",
      "Label: Doubt\n",
      "Position: 808 to 988\n",
      "Text: 'может, этому человеку стоило бы потратить время и узнать немного своей собственной истории! Нет ничего хуже, чем когда человек, не знающий, о чём говорит, пытается учить вас истине'\n",
      "\n",
      "Span 7457\n",
      "Label: Flag_Waving\n",
      "Position: 1535 to 1669\n",
      "Text: 'Может, ему стоило бы обратиться к отцам-основателям и к тому документу, который они составили под названием «Декларация независимости»'\n",
      "\n",
      "Span 7458\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 1738 to 1763\n",
      "Text: 'тиранический король Георг'\n",
      "\n",
      "Span 7459\n",
      "Label: Repetition\n",
      "Position: 1885 to 1891\n",
      "Text: 'предки'\n",
      "\n",
      "Span 7460\n",
      "Label: Slogans\n",
      "Position: 1957 to 2000\n",
      "Text: 'Сопротивление тиранам — это послушание Богу'\n",
      "\n",
      "Span 7461\n",
      "Label: Slogans\n",
      "Position: 2059 to 2088\n",
      "Text: 'Нет короля, кроме Царя Иисуса'\n",
      "\n",
      "Span 7462\n",
      "Label: Slogans\n",
      "Position: 2142 to 2179\n",
      "Text: 'Бунт против тиранов — послушание Богу'\n",
      "\n",
      "Span 7463\n",
      "Label: Appeal_to_Authority\n",
      "Position: 2312 to 2376\n",
      "Text: 'Джон Хэнкок, первый подписавший Декларацию независимости, сказал'\n",
      "\n",
      "Span 7464\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 2703 to 2721\n",
      "Text: 'тиран король Георг'\n",
      "\n",
      "Span 7465\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 2859 to 2883\n",
      "Text: 'коррумпированному королю'\n",
      "\n",
      "Span 7466\n",
      "Label: Doubt\n",
      "Position: 3015 to 3073\n",
      "Text: 'этот протестант должен знать, что говорит Библия о властях'\n",
      "\n",
      "Span 7467\n",
      "Label: Slogans\n",
      "Position: 3271 to 3318\n",
      "Text: 'Представители работают на нас, а не мы — на них'\n",
      "\n",
      "Span 7468\n",
      "Label: Appeal_to_Fear-Prejudice\n",
      "Position: 3621 to 3734\n",
      "Text: 'Кто противится власти, тот противится установленному Богом порядку, и сопротивляющиеся навлекут на себя осуждение'\n",
      "\n",
      "Span 7469\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 3992 to 4068\n",
      "Text: 'служитель Божий, мститель, чтобы привести в исполнения гнев на делающего зло'\n",
      "\n",
      "Span 7470\n",
      "Label: Loaded_Language\n",
      "Position: 4629 to 4642\n",
      "Text: 'Абсолютно нет'\n",
      "\n",
      "Span 7471\n",
      "Label: Loaded_Language\n",
      "Position: 4765 to 4777\n",
      "Text: 'Конечно, нет'\n",
      "\n",
      "Span 7472\n",
      "Label: Loaded_Language\n",
      "Position: 5062 to 5074\n",
      "Text: 'плодами тьмы'\n",
      "\n",
      "Span 7473\n",
      "Label: Repetition\n",
      "Position: 5243 to 5253\n",
      "Text: 'Мы, народу'\n",
      "\n",
      "Span 7474\n",
      "Label: False_Dilemma-No_Choice\n",
      "Position: 5532 to 5625\n",
      "Text: 'Ни один человеческий указ не может считаться законом, если он не соответствует Закону Божьему'\n",
      "\n",
      "Span 7475\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 5758 to 5779\n",
      "Text: 'библейски неграмотные'\n",
      "\n",
      "Span 7476\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 5794 to 5802\n",
      "Text: 'тиранами'\n",
      "\n",
      "Span 7478\n",
      "Label: Doubt\n",
      "Position: 5860 to 5960\n",
      "Text: 'Спрашиваю: должны ли христиане подчиняться тем, кто санкционировал убийство невинных в утробе матери'\n",
      "\n",
      "Span 7477\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 5903 to 5960\n",
      "Text: 'тем, кто санкционировал убийство невинных в утробе матери'\n",
      "\n",
      "Span 7479\n",
      "Label: Loaded_Language\n",
      "Position: 5989 to 6005\n",
      "Text: 'Определенно, нет'\n",
      "\n",
      "Span 7481\n",
      "Label: Doubt\n",
      "Position: 6009 to 6131\n",
      "Text: 'Должны ли христиане подчиняться тем, кто переопределил брак, позволив мужчине «жениться» на мужчине и женщине — на женщине'\n",
      "\n",
      "Span 7480\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6041 to 6131\n",
      "Text: 'тем, кто переопределил брак, позволив мужчине «жениться» на мужчине и женщине — на женщине'\n",
      "\n",
      "Span 7482\n",
      "Label: Loaded_Language\n",
      "Position: 6168 to 6184\n",
      "Text: 'Определенно, нет'\n",
      "\n",
      "Span 7485\n",
      "Label: Doubt\n",
      "Position: 6188 to 6254\n",
      "Text: 'Подчиняться тем кто постоянно лжёт, тем, кто крадёт и даже убивает'\n",
      "\n",
      "Span 7483\n",
      "Label: Repetition\n",
      "Position: 6200 to 6203\n",
      "Text: 'тем'\n",
      "\n",
      "Span 7484\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6200 to 6254\n",
      "Text: 'тем кто постоянно лжёт, тем, кто крадёт и даже убивает'\n",
      "\n",
      "Span 7486\n",
      "Label: Repetition\n",
      "Position: 6256 to 6272\n",
      "Text: 'Определённо, нет'\n",
      "\n",
      "Span 7487\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6344 to 6399\n",
      "Text: 'извращённые, современные, исповедующие себя христианами'\n",
      "\n",
      "Span 7488\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6400 to 6404\n",
      "Text: 'овец'\n",
      "\n",
      "Span 7489\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6579 to 6593\n",
      "Text: 'детьми Божьими'\n",
      "\n",
      "Span 7490\n",
      "Label: Loaded_Language\n",
      "Position: 6742 to 6751\n",
      "Text: 'восклицая'\n",
      "\n",
      "Span 7491\n",
      "Label: Loaded_Language\n",
      "Position: 6824 to 6842\n",
      "Text: 'учат такому ересям'\n",
      "\n",
      "Span 7492\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6900 to 6951\n",
      "Text: 'враждебными коррумпированными гражданскими властями'\n",
      "\n",
      "Span 7493\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 7082 to 7132\n",
      "Text: 'различными коррумпированными гражданскими властями'\n",
      "\n",
      "Span 7494\n",
      "Label: Loaded_Language\n",
      "Position: 7261 to 7273\n",
      "Text: 'Конечно, нет'\n",
      "\n",
      "Span 7495\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 7416 to 7436\n",
      "Text: '«ужасом для дел злых'\n",
      "\n",
      "Span 7496\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 7482 to 7511\n",
      "Text: 'добрых, законопослушных людей'\n",
      "\n",
      "Span 7497\n",
      "Label: Loaded_Language\n",
      "Position: 7815 to 7859\n",
      "Text: 'приводить в исполнение гнев на делающего зло'\n",
      "\n",
      "Span 7499\n",
      "Label: Doubt\n",
      "Position: 7862 to 7991\n",
      "Text: 'Кто учит, что у них есть власть приводить в исполнение гнев на делающего добро, тот невежда и невежественный в своих заблуждениях'\n",
      "\n",
      "Span 7498\n",
      "Label: Loaded_Language\n",
      "Position: 7894 to 7940\n",
      "Text: 'приводить в исполнение гнев на делающего добро'\n",
      "\n",
      "Span 7500\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8042 to 8065\n",
      "Text: 'злой гражданской власти'\n",
      "\n",
      "Span 7501\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8260 to 8278\n",
      "Text: 'Господь Бог евреев'\n",
      "\n",
      "Span 7502\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8435 to 8465\n",
      "Text: 'самый кроткий человек на земле'\n",
      "\n",
      "Span 7503\n",
      "Label: Appeal_to_Fear-Prejudice\n",
      "Position: 8488 to 8520\n",
      "Text: 'с требованием подчиниться, иначе'\n",
      "\n",
      "Span 7504\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8870 to 8875\n",
      "Text: 'Тиран'\n",
      "\n",
      "Span 7505\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8893 to 8911\n",
      "Text: 'мятежником Израиля'\n",
      "\n",
      "Span 7506\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 9019 to 9026\n",
      "Text: 'Господа'\n",
      "\n",
      "Span 7507\n",
      "Label: Repetition\n",
      "Position: 9076 to 9106\n",
      "Text: 'нарушение принципов подчинения'\n",
      "\n",
      "Span 7508\n",
      "Label: Loaded_Language\n",
      "Position: 9842 to 9859\n",
      "Text: 'воскликнул громко'\n",
      "\n",
      "Span 7509\n",
      "Label: Appeal_to_Fear-Prejudice\n",
      "Position: 10107 to 10193\n",
      "Text: 'тот, кто не поклонится и не упадёт, будет тотчас же брошен в раскалённую огненную печь'\n",
      "\n",
      "Span 7510\n",
      "Label: Repetition\n",
      "Position: 10237 to 10243\n",
      "Text: 'тирану'\n",
      "\n",
      "Span 7511\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 10447 to 10473\n",
      "Text: 'лишь коррумпированный царь'\n",
      "\n",
      "Span 7512\n",
      "Label: Repetition\n",
      "Position: 11536 to 11552\n",
      "Text: 'коррумпированных'\n",
      "\n",
      "Span 7513\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 11536 to 11589\n",
      "Text: 'коррумпированных священников и коррумпированных царей'\n",
      "\n",
      "Span 7514\n",
      "Label: Loaded_Language\n",
      "Position: 11741 to 11754\n",
      "Text: 'все злодеяния'\n",
      "\n",
      "Span 7515\n",
      "Label: Loaded_Language\n",
      "Position: 12105 to 12120\n",
      "Text: 'Несомненно, нет'\n",
      "\n",
      "Span 7516\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 12122 to 12146\n",
      "Text: 'Коррумпированные фарисеи'\n",
      "\n",
      "Span 7517\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 12149 to 12179\n",
      "Text: 'коррумпированное правительство'\n",
      "\n",
      "Span 7518\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 13300 to 13323\n",
      "Text: 'неучёные и простые люди'\n",
      "\n",
      "Span 7519\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 14092 to 14099\n",
      "Text: 'тиранов'\n",
      "\n",
      "Span 7520\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 15381 to 15404\n",
      "Text: 'злых, воюющих с Христом'\n",
      "\n",
      "Span 7522\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 15449 to 15487\n",
      "Text: 'коррумпированными и злыми священниками'\n",
      "\n",
      "Span 7521\n",
      "Label: Repetition\n",
      "Position: 15469 to 15474\n",
      "Text: 'злыми'\n",
      "\n",
      "Span 7523\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 15500 to 15541\n",
      "Text: 'коррумпированными и злыми в правительстве'\n",
      "\n",
      "Span 7524\n",
      "Label: Repetition\n",
      "Position: 15769 to 15774\n",
      "Text: 'Нигде'\n",
      "\n",
      "Span 7525\n",
      "Label: Doubt\n",
      "Position: 15778 to 16020\n",
      "Text: 'Кроме того, автор статьи, призывающей людей подчиняться властям, мог бы уделить время прочтению Римлянам 12:21, прежде чем искажать и перекручивать Римлянам 13, чтобы в следующий раз не быть виновным в искажении Писания, как это делает сатана'\n",
      "\n",
      "============================================================\n",
      "Wrapped Article ID: 773520636\n",
      "============================================================\n",
      "\n",
      "Marked span #1\n",
      "Text: 'покоряться тирании'\n",
      "\n",
      "Marked span #2\n",
      "Text: 'Воскликнул оратор'\n",
      "\n",
      "Marked span #3\n",
      "Text: 'растерянного, библейски неграмотного друга'\n",
      "\n",
      "Marked span #4\n",
      "Text: 'этот протестант должен был остаться дома и читать свою Библию, а не протестовать против коррупции в правительстве'\n",
      "\n",
      "Marked span #5\n",
      "Text: 'может, этому человеку стоило бы потратить время и узнать немного своей собственной истории! Нет ничего хуже, чем когда человек, не знающий, о чём говорит, пытается учить вас истине'\n",
      "\n",
      "Marked span #6\n",
      "Text: 'Может, ему стоило бы обратиться к отцам-основателям и к тому документу, который они составили под названием «Декларация независимости»'\n",
      "\n",
      "Marked span #7\n",
      "Text: 'тиранический король Георг'\n",
      "\n",
      "Marked span #8\n",
      "Text: 'предки'\n",
      "\n",
      "Marked span #9\n",
      "Text: 'Сопротивление тиранам — это послушание Богу'\n",
      "\n",
      "Marked span #10\n",
      "Text: 'Нет короля, кроме Царя Иисуса'\n",
      "\n",
      "Marked span #11\n",
      "Text: 'Бунт против тиранов — послушание Богу'\n",
      "\n",
      "Marked span #12\n",
      "Text: 'Джон Хэнкок, первый подписавший Декларацию независимости, сказал'\n",
      "\n",
      "Marked span #13\n",
      "Text: 'тиран король Георг'\n",
      "\n",
      "Marked span #14\n",
      "Text: 'коррумпированному королю'\n",
      "\n",
      "Marked span #15\n",
      "Text: 'этот протестант должен знать, что говорит Библия о властях'\n",
      "\n",
      "Marked span #16\n",
      "Text: 'Представители работают на нас, а не мы — на них'\n",
      "\n",
      "Marked span #17\n",
      "Text: 'Кто противится власти, тот противится установленному Богом порядку, и сопротивляющиеся навлекут на себя осуждение'\n",
      "\n",
      "Marked span #18\n",
      "Text: 'служитель Божий, мститель, чтобы привести в исполнения гнев на делающего зло'\n",
      "\n",
      "Marked span #19\n",
      "Text: 'Абсолютно нет'\n",
      "\n",
      "Marked span #20\n",
      "Text: 'Конечно, нет'\n",
      "\n",
      "Marked span #21\n",
      "Text: 'плодами тьмы'\n",
      "\n",
      "Marked span #22\n",
      "Text: 'Мы, народу'\n",
      "\n",
      "Marked span #23\n",
      "Text: 'Ни один человеческий указ не может считаться законом, если он не соответствует Закону Божьему'\n",
      "\n",
      "Marked span #24\n",
      "Text: 'библейски неграмотные'\n",
      "\n",
      "Marked span #25\n",
      "Text: 'тиранами'\n",
      "\n",
      "Marked span #26\n",
      "Text: 'Спрашиваю: должны ли христиане подчиняться <<S_27>>тем, кто санкционировал убийство невинных в утробе матери<</S_27>>'\n",
      "\n",
      "Marked span #28\n",
      "Text: 'Определенно, нет'\n",
      "\n",
      "Marked span #29\n",
      "Text: 'Должны ли христиане подчиняться <<S_30>>тем, кто переопределил брак, позволив мужчине «жениться» на мужчине и женщине — на женщине<</S_30>>'\n",
      "\n",
      "Marked span #31\n",
      "Text: 'Определенно, нет'\n",
      "\n",
      "Marked span #32\n",
      "Text: 'Подчиняться <<S_33>><<S_34>>тем<</S_34>> кто постоянно лжёт, тем, кто крадёт и даже убивает<</S_33>>'\n",
      "\n",
      "Marked span #35\n",
      "Text: 'Определённо, нет'\n",
      "\n",
      "Marked span #36\n",
      "Text: 'извращённые, современные, исповедующие себя христианами'\n",
      "\n",
      "Marked span #37\n",
      "Text: 'овец'\n",
      "\n",
      "Marked span #38\n",
      "Text: 'детьми Божьими'\n",
      "\n",
      "Marked span #39\n",
      "Text: 'восклицая'\n",
      "\n",
      "Marked span #40\n",
      "Text: 'учат такому ересям'\n",
      "\n",
      "Marked span #41\n",
      "Text: 'враждебными коррумпированными гражданскими властями'\n",
      "\n",
      "Marked span #42\n",
      "Text: 'различными коррумпированными гражданскими властями'\n",
      "\n",
      "Marked span #43\n",
      "Text: 'Конечно, нет'\n",
      "\n",
      "Marked span #44\n",
      "Text: '«ужасом для дел злых'\n",
      "\n",
      "Marked span #45\n",
      "Text: 'добрых, законопослушных людей'\n",
      "\n",
      "Marked span #46\n",
      "Text: 'приводить в исполнение гнев на делающего зло'\n",
      "\n",
      "Marked span #47\n",
      "Text: 'Кто учит, что у них есть власть <<S_48>>приводить в исполнение гнев на делающего добро<</S_48>>, тот невежда и невежественный в своих заблуждениях'\n",
      "\n",
      "Marked span #49\n",
      "Text: 'злой гражданской власти'\n",
      "\n",
      "Marked span #50\n",
      "Text: 'Господь Бог евреев'\n",
      "\n",
      "Marked span #51\n",
      "Text: 'самый кроткий человек на земле'\n",
      "\n",
      "Marked span #52\n",
      "Text: 'с требованием подчиниться, иначе'\n",
      "\n",
      "Marked span #53\n",
      "Text: 'Тиран'\n",
      "\n",
      "Marked span #54\n",
      "Text: 'мятежником Израиля'\n",
      "\n",
      "Marked span #55\n",
      "Text: 'Господа'\n",
      "\n",
      "Marked span #56\n",
      "Text: 'нарушение принципов подчинения'\n",
      "\n",
      "Marked span #57\n",
      "Text: 'воскликнул громко'\n",
      "\n",
      "Marked span #58\n",
      "Text: 'тот, кто не поклонится и не упадёт, будет тотчас же брошен в раскалённую огненную печь'\n",
      "\n",
      "Marked span #59\n",
      "Text: 'тирану'\n",
      "\n",
      "Marked span #60\n",
      "Text: 'лишь коррумпированный царь'\n",
      "\n",
      "Marked span #61\n",
      "Text: '<<S_62>>коррумпированных<</S_62>> священников и коррумпированных царей'\n",
      "\n",
      "Marked span #63\n",
      "Text: 'все злодеяния'\n",
      "\n",
      "Marked span #64\n",
      "Text: 'Несомненно, нет'\n",
      "\n",
      "Marked span #65\n",
      "Text: 'Коррумпированные фарисеи'\n",
      "\n",
      "Marked span #66\n",
      "Text: 'коррумпированное правительство'\n",
      "\n",
      "Marked span #68\n",
      "Text: 'неучёные и простые люди'\n",
      "\n",
      "Marked span #69\n",
      "Text: 'тиранов'\n",
      "\n",
      "Marked span #70\n",
      "Text: 'злых, воюющих с Христом'\n",
      "\n",
      "Marked span #71\n",
      "Text: 'коррумпированными и <<S_72>>злыми<</S_72>> священниками'\n",
      "\n",
      "Marked span #73\n",
      "Text: 'коррумпированными и злыми в правительстве'\n",
      "\n",
      "Marked span #74\n",
      "Text: 'Нигде'\n",
      "\n",
      "Marked span #75\n",
      "Text: 'Кроме того, автор статьи, призывающей людей подчиняться властям, мог бы уделить время прочтению Римлянам 12:21, прежде чем искажать и перекручивать Римлянам 13, чтобы в следующий раз не быть виновным в искажении Писания, как это делает сатана'\n"
     ]
    }
   ],
   "source": [
    "# --- OBSOLETE CODE ---\n",
    "# Original span display functions\n",
    "# import re\n",
    "# # Function to display spans for a specific article\n",
    "# def show_article_spans(article_id, lang=\"en\", base_path=\"../data/processed/ru\"):\n",
    "#     # Get all spans for this specific article\n",
    "#     article_spans = df[df['article_id'] == article_id].sort_values(by='start')\n",
    "#     \n",
    "#     # Check if we found any spans\n",
    "#     if len(article_spans) == 0:\n",
    "#         print(f\"No spans found for article {article_id}\")\n",
    "#         return\n",
    "#     \n",
    "#     r = []\n",
    "#     # Try to get the original article text\n",
    "#     article_path = f\"{base_path}/unwrapped-articles/{lang}_article{article_id}.txt\"\n",
    "#     try:\n",
    "#         with open(article_path, \"rb\") as f:\n",
    "#             raw = f.read()\n",
    "#         article_text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
    "#         \n",
    "#         print(f\"\\n{'='*60}\")\n",
    "#         print(f\"Article ID: {article_id}\")\n",
    "#         print(f\"Total spans: {len(article_spans)}\")\n",
    "#         print(f\"{'='*60}\")\n",
    "#         \n",
    "#         # Display each span\n",
    "#         for index, row in article_spans.iterrows():\n",
    "#             label = row['label']\n",
    "#             start_offset = row['start']\n",
    "#             end_offset = row['end']\n",
    "#             span_text = article_text[start_offset:end_offset]\n",
    "#             r.append((label, start_offset, end_offset, span_text))\n",
    "#             \n",
    "#             print(f\"\\nSpan {index}\")\n",
    "#             print(f\"Label: {label}\")\n",
    "#             print(f\"Position: {start_offset} to {end_offset}\")\n",
    "#             print(f\"Text: '{span_text}'\")\n",
    "# \n",
    "#         return r\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error accessing article {article_id}: {e}\")\n",
    "# \n",
    "# # Function to show text between span markers in wrapped articles\n",
    "# def show_wrapped_spans(article_id, lang=\"en\", base_path=\"../data/processed/ru\"):\n",
    "#     # Try to get the wrapped article text\n",
    "#     article_path = f\"{base_path}/wrapped-articles/{lang}_article{article_id}.txt\"\n",
    "#     r = []\n",
    "#     try:\n",
    "#         with open(article_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#             wrapped_text = f.read()\n",
    "#         \n",
    "#         print(f\"\\n{'='*60}\")\n",
    "#         print(f\"Wrapped Article ID: {article_id}\")\n",
    "#         print(f\"{'='*60}\")\n",
    "#         \n",
    "#         # Find all span markers using regex\n",
    "#         spans = re.finditer(r'<<S_(\\d+)>>(.*?)<</S_\\1>>', wrapped_text, re.DOTALL)\n",
    "#         \n",
    "#         found_spans = False\n",
    "#         for i, span in enumerate(spans):\n",
    "#             found_spans = True\n",
    "#             span_number = span.group(1)\n",
    "#             span_text = span.group(2)\n",
    "#             print(f\"\\nMarked span #{span_number}\")\n",
    "#             print(f\"Text: '{span_text}'\")\n",
    "#             r.append((span_number, span_text))\n",
    "#         \n",
    "#         if not found_spans:\n",
    "#             print(f\"No marked spans found in wrapped article {article_id}\")\n",
    "#         return r\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error accessing wrapped article {article_id}: {e}\")\n",
    "# \n",
    "# # Example usage - show spans for a sample article\n",
    "# sample_article_id = df['article_id'].sample(1).iloc[0]\n",
    "# article_spans = show_article_spans(sample_article_id)\n",
    "# \n",
    "# # Show the wrapped version of the same article\n",
    "# wrapped_spans = show_wrapped_spans(sample_article_id)\n",
    "# \n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e01ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Article ID: 773520636\n",
      "Total spans: 74\n",
      "============================================================\n",
      "\n",
      "Span 7452\n",
      "Label: Loaded_Language\n",
      "Position: 52 to 70\n",
      "Text: 'покоряться тирании'\n",
      "\n",
      "Span 7453\n",
      "Label: Loaded_Language\n",
      "Position: 148 to 165\n",
      "Text: 'Воскликнул оратор'\n",
      "\n",
      "Span 7454\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 301 to 343\n",
      "Text: 'растерянного, библейски неграмотного друга'\n",
      "\n",
      "Span 7455\n",
      "Label: Doubt\n",
      "Position: 606 to 719\n",
      "Text: 'этот протестант должен был остаться дома и читать свою Библию, а не протестовать против коррупции в правительстве'\n",
      "\n",
      "Span 7456\n",
      "Label: Doubt\n",
      "Position: 808 to 988\n",
      "Text: 'может, этому человеку стоило бы потратить время и узнать немного своей собственной истории! Нет ничего хуже, чем когда человек, не знающий, о чём говорит, пытается учить вас истине'\n",
      "\n",
      "Span 7457\n",
      "Label: Flag_Waving\n",
      "Position: 1535 to 1669\n",
      "Text: 'Может, ему стоило бы обратиться к отцам-основателям и к тому документу, который они составили под названием «Декларация независимости»'\n",
      "\n",
      "Span 7458\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 1738 to 1763\n",
      "Text: 'тиранический король Георг'\n",
      "\n",
      "Span 7459\n",
      "Label: Repetition\n",
      "Position: 1885 to 1891\n",
      "Text: 'предки'\n",
      "\n",
      "Span 7460\n",
      "Label: Slogans\n",
      "Position: 1957 to 2000\n",
      "Text: 'Сопротивление тиранам — это послушание Богу'\n",
      "\n",
      "Span 7461\n",
      "Label: Slogans\n",
      "Position: 2059 to 2088\n",
      "Text: 'Нет короля, кроме Царя Иисуса'\n",
      "\n",
      "Span 7462\n",
      "Label: Slogans\n",
      "Position: 2142 to 2179\n",
      "Text: 'Бунт против тиранов — послушание Богу'\n",
      "\n",
      "Span 7463\n",
      "Label: Appeal_to_Authority\n",
      "Position: 2312 to 2376\n",
      "Text: 'Джон Хэнкок, первый подписавший Декларацию независимости, сказал'\n",
      "\n",
      "Span 7464\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 2703 to 2721\n",
      "Text: 'тиран король Георг'\n",
      "\n",
      "Span 7465\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 2859 to 2883\n",
      "Text: 'коррумпированному королю'\n",
      "\n",
      "Span 7466\n",
      "Label: Doubt\n",
      "Position: 3015 to 3073\n",
      "Text: 'этот протестант должен знать, что говорит Библия о властях'\n",
      "\n",
      "Span 7467\n",
      "Label: Slogans\n",
      "Position: 3271 to 3318\n",
      "Text: 'Представители работают на нас, а не мы — на них'\n",
      "\n",
      "Span 7468\n",
      "Label: Appeal_to_Fear-Prejudice\n",
      "Position: 3621 to 3734\n",
      "Text: 'Кто противится власти, тот противится установленному Богом порядку, и сопротивляющиеся навлекут на себя осуждение'\n",
      "\n",
      "Span 7469\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 3992 to 4068\n",
      "Text: 'служитель Божий, мститель, чтобы привести в исполнения гнев на делающего зло'\n",
      "\n",
      "Span 7470\n",
      "Label: Loaded_Language\n",
      "Position: 4629 to 4642\n",
      "Text: 'Абсолютно нет'\n",
      "\n",
      "Span 7471\n",
      "Label: Loaded_Language\n",
      "Position: 4765 to 4777\n",
      "Text: 'Конечно, нет'\n",
      "\n",
      "Span 7472\n",
      "Label: Loaded_Language\n",
      "Position: 5062 to 5074\n",
      "Text: 'плодами тьмы'\n",
      "\n",
      "Span 7473\n",
      "Label: Repetition\n",
      "Position: 5243 to 5253\n",
      "Text: 'Мы, народу'\n",
      "\n",
      "Span 7474\n",
      "Label: False_Dilemma-No_Choice\n",
      "Position: 5532 to 5625\n",
      "Text: 'Ни один человеческий указ не может считаться законом, если он не соответствует Закону Божьему'\n",
      "\n",
      "Span 7475\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 5758 to 5779\n",
      "Text: 'библейски неграмотные'\n",
      "\n",
      "Span 7476\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 5794 to 5802\n",
      "Text: 'тиранами'\n",
      "\n",
      "Span 7478\n",
      "Label: Doubt\n",
      "Position: 5860 to 5960\n",
      "Text: 'Спрашиваю: должны ли христиане подчиняться тем, кто санкционировал убийство невинных в утробе матери'\n",
      "\n",
      "Span 7477\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 5903 to 5960\n",
      "Text: 'тем, кто санкционировал убийство невинных в утробе матери'\n",
      "\n",
      "Span 7479\n",
      "Label: Loaded_Language\n",
      "Position: 5989 to 6005\n",
      "Text: 'Определенно, нет'\n",
      "\n",
      "Span 7481\n",
      "Label: Doubt\n",
      "Position: 6009 to 6131\n",
      "Text: 'Должны ли христиане подчиняться тем, кто переопределил брак, позволив мужчине «жениться» на мужчине и женщине — на женщине'\n",
      "\n",
      "Span 7480\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6041 to 6131\n",
      "Text: 'тем, кто переопределил брак, позволив мужчине «жениться» на мужчине и женщине — на женщине'\n",
      "\n",
      "Span 7482\n",
      "Label: Loaded_Language\n",
      "Position: 6168 to 6184\n",
      "Text: 'Определенно, нет'\n",
      "\n",
      "Span 7485\n",
      "Label: Doubt\n",
      "Position: 6188 to 6254\n",
      "Text: 'Подчиняться тем кто постоянно лжёт, тем, кто крадёт и даже убивает'\n",
      "\n",
      "Span 7483\n",
      "Label: Repetition\n",
      "Position: 6200 to 6203\n",
      "Text: 'тем'\n",
      "\n",
      "Span 7484\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6200 to 6254\n",
      "Text: 'тем кто постоянно лжёт, тем, кто крадёт и даже убивает'\n",
      "\n",
      "Span 7486\n",
      "Label: Repetition\n",
      "Position: 6256 to 6272\n",
      "Text: 'Определённо, нет'\n",
      "\n",
      "Span 7487\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6344 to 6399\n",
      "Text: 'извращённые, современные, исповедующие себя христианами'\n",
      "\n",
      "Span 7488\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6400 to 6404\n",
      "Text: 'овец'\n",
      "\n",
      "Span 7489\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6579 to 6593\n",
      "Text: 'детьми Божьими'\n",
      "\n",
      "Span 7490\n",
      "Label: Loaded_Language\n",
      "Position: 6742 to 6751\n",
      "Text: 'восклицая'\n",
      "\n",
      "Span 7491\n",
      "Label: Loaded_Language\n",
      "Position: 6824 to 6842\n",
      "Text: 'учат такому ересям'\n",
      "\n",
      "Span 7492\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 6900 to 6951\n",
      "Text: 'враждебными коррумпированными гражданскими властями'\n",
      "\n",
      "Span 7493\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 7082 to 7132\n",
      "Text: 'различными коррумпированными гражданскими властями'\n",
      "\n",
      "Span 7494\n",
      "Label: Loaded_Language\n",
      "Position: 7261 to 7273\n",
      "Text: 'Конечно, нет'\n",
      "\n",
      "Span 7495\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 7416 to 7436\n",
      "Text: '«ужасом для дел злых'\n",
      "\n",
      "Span 7496\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 7482 to 7511\n",
      "Text: 'добрых, законопослушных людей'\n",
      "\n",
      "Span 7497\n",
      "Label: Loaded_Language\n",
      "Position: 7815 to 7859\n",
      "Text: 'приводить в исполнение гнев на делающего зло'\n",
      "\n",
      "Span 7499\n",
      "Label: Doubt\n",
      "Position: 7862 to 7991\n",
      "Text: 'Кто учит, что у них есть власть приводить в исполнение гнев на делающего добро, тот невежда и невежественный в своих заблуждениях'\n",
      "\n",
      "Span 7498\n",
      "Label: Loaded_Language\n",
      "Position: 7894 to 7940\n",
      "Text: 'приводить в исполнение гнев на делающего добро'\n",
      "\n",
      "Span 7500\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8042 to 8065\n",
      "Text: 'злой гражданской власти'\n",
      "\n",
      "Span 7501\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8260 to 8278\n",
      "Text: 'Господь Бог евреев'\n",
      "\n",
      "Span 7502\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8435 to 8465\n",
      "Text: 'самый кроткий человек на земле'\n",
      "\n",
      "Span 7503\n",
      "Label: Appeal_to_Fear-Prejudice\n",
      "Position: 8488 to 8520\n",
      "Text: 'с требованием подчиниться, иначе'\n",
      "\n",
      "Span 7504\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8870 to 8875\n",
      "Text: 'Тиран'\n",
      "\n",
      "Span 7505\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 8893 to 8911\n",
      "Text: 'мятежником Израиля'\n",
      "\n",
      "Span 7506\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 9019 to 9026\n",
      "Text: 'Господа'\n",
      "\n",
      "Span 7507\n",
      "Label: Repetition\n",
      "Position: 9076 to 9106\n",
      "Text: 'нарушение принципов подчинения'\n",
      "\n",
      "Span 7508\n",
      "Label: Loaded_Language\n",
      "Position: 9842 to 9859\n",
      "Text: 'воскликнул громко'\n",
      "\n",
      "Span 7509\n",
      "Label: Appeal_to_Fear-Prejudice\n",
      "Position: 10107 to 10193\n",
      "Text: 'тот, кто не поклонится и не упадёт, будет тотчас же брошен в раскалённую огненную печь'\n",
      "\n",
      "Span 7510\n",
      "Label: Repetition\n",
      "Position: 10237 to 10243\n",
      "Text: 'тирану'\n",
      "\n",
      "Span 7511\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 10447 to 10473\n",
      "Text: 'лишь коррумпированный царь'\n",
      "\n",
      "Span 7512\n",
      "Label: Repetition\n",
      "Position: 11536 to 11552\n",
      "Text: 'коррумпированных'\n",
      "\n",
      "Span 7513\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 11536 to 11589\n",
      "Text: 'коррумпированных священников и коррумпированных царей'\n",
      "\n",
      "Span 7514\n",
      "Label: Loaded_Language\n",
      "Position: 11741 to 11754\n",
      "Text: 'все злодеяния'\n",
      "\n",
      "Span 7515\n",
      "Label: Loaded_Language\n",
      "Position: 12105 to 12120\n",
      "Text: 'Несомненно, нет'\n",
      "\n",
      "Span 7516\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 12122 to 12146\n",
      "Text: 'Коррумпированные фарисеи'\n",
      "\n",
      "Span 7517\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 12149 to 12179\n",
      "Text: 'коррумпированное правительство'\n",
      "\n",
      "Span 7518\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 13300 to 13323\n",
      "Text: 'неучёные и простые люди'\n",
      "\n",
      "Span 7519\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 14092 to 14099\n",
      "Text: 'тиранов'\n",
      "\n",
      "Span 7520\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 15381 to 15404\n",
      "Text: 'злых, воюющих с Христом'\n",
      "\n",
      "Span 7522\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 15449 to 15487\n",
      "Text: 'коррумпированными и злыми священниками'\n",
      "\n",
      "Span 7521\n",
      "Label: Repetition\n",
      "Position: 15469 to 15474\n",
      "Text: 'злыми'\n",
      "\n",
      "Span 7523\n",
      "Label: Name_Calling-Labeling\n",
      "Position: 15500 to 15541\n",
      "Text: 'коррумпированными и злыми в правительстве'\n",
      "\n",
      "Span 7524\n",
      "Label: Repetition\n",
      "Position: 15769 to 15774\n",
      "Text: 'Нигде'\n",
      "\n",
      "Span 7525\n",
      "Label: Doubt\n",
      "Position: 15778 to 16020\n",
      "Text: 'Кроме того, автор статьи, призывающей людей подчиняться властям, мог бы уделить время прочтению Римлянам 12:21, прежде чем искажать и перекручивать Римлянам 13, чтобы в следующий раз не быть виновным в искажении Писания, как это делает сатана'\n",
      "\n",
      "============================================================\n",
      "Wrapped Article ID: 773520636\n",
      "============================================================\n",
      "\n",
      "Marked span #1\n",
      "Text: 'покоряться тирании'\n",
      "\n",
      "Marked span #2\n",
      "Text: 'Воскликнул оратор'\n",
      "\n",
      "Marked span #3\n",
      "Text: 'растерянного, библейски неграмотного друга'\n",
      "\n",
      "Marked span #4\n",
      "Text: 'этот протестант должен был остаться дома и читать свою Библию, а не протестовать против коррупции в правительстве'\n",
      "\n",
      "Marked span #5\n",
      "Text: 'может, этому человеку стоило бы потратить время и узнать немного своей собственной истории! Нет ничего хуже, чем когда человек, не знающий, о чём говорит, пытается учить вас истине'\n",
      "\n",
      "Marked span #6\n",
      "Text: 'Может, ему стоило бы обратиться к отцам-основателям и к тому документу, который они составили под названием «Декларация независимости»'\n",
      "\n",
      "Marked span #7\n",
      "Text: 'тиранический король Георг'\n",
      "\n",
      "Marked span #8\n",
      "Text: 'предки'\n",
      "\n",
      "Marked span #9\n",
      "Text: 'Сопротивление тиранам — это послушание Богу'\n",
      "\n",
      "Marked span #10\n",
      "Text: 'Нет короля, кроме Царя Иисуса'\n",
      "\n",
      "Marked span #11\n",
      "Text: 'Бунт против тиранов — послушание Богу'\n",
      "\n",
      "Marked span #12\n",
      "Text: 'Джон Хэнкок, первый подписавший Декларацию независимости, сказал'\n",
      "\n",
      "Marked span #13\n",
      "Text: 'тиран король Георг'\n",
      "\n",
      "Marked span #14\n",
      "Text: 'коррумпированному королю'\n",
      "\n",
      "Marked span #15\n",
      "Text: 'этот протестант должен знать, что говорит Библия о властях'\n",
      "\n",
      "Marked span #16\n",
      "Text: 'Представители работают на нас, а не мы — на них'\n",
      "\n",
      "Marked span #17\n",
      "Text: 'Кто противится власти, тот противится установленному Богом порядку, и сопротивляющиеся навлекут на себя осуждение'\n",
      "\n",
      "Marked span #18\n",
      "Text: 'служитель Божий, мститель, чтобы привести в исполнения гнев на делающего зло'\n",
      "\n",
      "Marked span #19\n",
      "Text: 'Абсолютно нет'\n",
      "\n",
      "Marked span #20\n",
      "Text: 'Конечно, нет'\n",
      "\n",
      "Marked span #21\n",
      "Text: 'плодами тьмы'\n",
      "\n",
      "Marked span #22\n",
      "Text: 'Мы, народу'\n",
      "\n",
      "Marked span #23\n",
      "Text: 'Ни один человеческий указ не может считаться законом, если он не соответствует Закону Божьему'\n",
      "\n",
      "Marked span #24\n",
      "Text: 'библейски неграмотные'\n",
      "\n",
      "Marked span #25\n",
      "Text: 'тиранами'\n",
      "\n",
      "Marked span #26\n",
      "Text: 'Спрашиваю: должны ли христиане подчиняться <<S_27>>тем, кто санкционировал убийство невинных в утробе матери<</S_27>>'\n",
      "\n",
      "Marked span #28\n",
      "Text: 'Определенно, нет'\n",
      "\n",
      "Marked span #29\n",
      "Text: 'Должны ли христиане подчиняться <<S_30>>тем, кто переопределил брак, позволив мужчине «жениться» на мужчине и женщине — на женщине<</S_30>>'\n",
      "\n",
      "Marked span #31\n",
      "Text: 'Определенно, нет'\n",
      "\n",
      "Marked span #32\n",
      "Text: 'Подчиняться <<S_33>><<S_34>>тем<</S_34>> кто постоянно лжёт, тем, кто крадёт и даже убивает<</S_33>>'\n",
      "\n",
      "Marked span #35\n",
      "Text: 'Определённо, нет'\n",
      "\n",
      "Marked span #36\n",
      "Text: 'извращённые, современные, исповедующие себя христианами'\n",
      "\n",
      "Marked span #37\n",
      "Text: 'овец'\n",
      "\n",
      "Marked span #38\n",
      "Text: 'детьми Божьими'\n",
      "\n",
      "Marked span #39\n",
      "Text: 'восклицая'\n",
      "\n",
      "Marked span #40\n",
      "Text: 'учат такому ересям'\n",
      "\n",
      "Marked span #41\n",
      "Text: 'враждебными коррумпированными гражданскими властями'\n",
      "\n",
      "Marked span #42\n",
      "Text: 'различными коррумпированными гражданскими властями'\n",
      "\n",
      "Marked span #43\n",
      "Text: 'Конечно, нет'\n",
      "\n",
      "Marked span #44\n",
      "Text: '«ужасом для дел злых'\n",
      "\n",
      "Marked span #45\n",
      "Text: 'добрых, законопослушных людей'\n",
      "\n",
      "Marked span #46\n",
      "Text: 'приводить в исполнение гнев на делающего зло'\n",
      "\n",
      "Marked span #47\n",
      "Text: 'Кто учит, что у них есть власть <<S_48>>приводить в исполнение гнев на делающего добро<</S_48>>, тот невежда и невежественный в своих заблуждениях'\n",
      "\n",
      "Marked span #49\n",
      "Text: 'злой гражданской власти'\n",
      "\n",
      "Marked span #50\n",
      "Text: 'Господь Бог евреев'\n",
      "\n",
      "Marked span #51\n",
      "Text: 'самый кроткий человек на земле'\n",
      "\n",
      "Marked span #52\n",
      "Text: 'с требованием подчиниться, иначе'\n",
      "\n",
      "Marked span #53\n",
      "Text: 'Тиран'\n",
      "\n",
      "Marked span #54\n",
      "Text: 'мятежником Израиля'\n",
      "\n",
      "Marked span #55\n",
      "Text: 'Господа'\n",
      "\n",
      "Marked span #56\n",
      "Text: 'нарушение принципов подчинения'\n",
      "\n",
      "Marked span #57\n",
      "Text: 'воскликнул громко'\n",
      "\n",
      "Marked span #58\n",
      "Text: 'тот, кто не поклонится и не упадёт, будет тотчас же брошен в раскалённую огненную печь'\n",
      "\n",
      "Marked span #59\n",
      "Text: 'тирану'\n",
      "\n",
      "Marked span #60\n",
      "Text: 'лишь коррумпированный царь'\n",
      "\n",
      "Marked span #61\n",
      "Text: '<<S_62>>коррумпированных<</S_62>> священников и коррумпированных царей'\n",
      "\n",
      "Marked span #63\n",
      "Text: 'все злодеяния'\n",
      "\n",
      "Marked span #64\n",
      "Text: 'Несомненно, нет'\n",
      "\n",
      "Marked span #65\n",
      "Text: 'Коррумпированные фарисеи'\n",
      "\n",
      "Marked span #66\n",
      "Text: 'коррумпированное правительство'\n",
      "\n",
      "Marked span #68\n",
      "Text: 'неучёные и простые люди'\n",
      "\n",
      "Marked span #69\n",
      "Text: 'тиранов'\n",
      "\n",
      "Marked span #70\n",
      "Text: 'злых, воюющих с Христом'\n",
      "\n",
      "Marked span #71\n",
      "Text: 'коррумпированными и <<S_72>>злыми<</S_72>> священниками'\n",
      "\n",
      "Marked span #73\n",
      "Text: 'коррумпированными и злыми в правительстве'\n",
      "\n",
      "Marked span #74\n",
      "Text: 'Нигде'\n",
      "\n",
      "Marked span #75\n",
      "Text: 'Кроме того, автор статьи, призывающей людей подчиняться властям, мог бы уделить время прочтению Римлянам 12:21, прежде чем искажать и перекручивать Римлянам 13, чтобы в следующий раз не быть виновным в искажении Писания, как это делает сатана'\n",
      "\n",
      "Article Span Index Best Match Index Similarity Article Span Text                        Wrapped Span Text\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "0                  0                1.00 покоряться тирании                       покоряться тирании\n",
      "1                  1                1.00 Воскликнул оратор                        Воскликнул оратор\n",
      "2                  2                1.00 растерянного, библейски неграмотного дру растерянного, библейски неграмотного дру\n",
      "3                  3                1.00 этот протестант должен был остаться дома этот протестант должен был остаться дома\n",
      "4                  4                1.00 может, этому человеку стоило бы потратит может, этому человеку стоило бы потратит\n",
      "5                  5                1.00 Может, ему стоило бы обратиться к отцам- Может, ему стоило бы обратиться к отцам-\n",
      "6                  6                1.00 тиранический король Георг                тиранический король Георг\n",
      "7                  7                1.00 предки                                   предки\n",
      "8                  8                1.00 Сопротивление тиранам — это послушание Б Сопротивление тиранам — это послушание Б\n",
      "9                  9                1.00 Нет короля, кроме Царя Иисуса            Нет короля, кроме Царя Иисуса\n",
      "10                 10               1.00 Бунт против тиранов — послушание Богу    Бунт против тиранов — послушание Богу\n",
      "11                 11               1.00 Джон Хэнкок, первый подписавший Декларац Джон Хэнкок, первый подписавший Декларац\n",
      "12                 12               1.00 тиран король Георг                       тиран король Георг\n",
      "13                 13               1.00 коррумпированному королю                 коррумпированному королю\n",
      "14                 14               1.00 этот протестант должен знать, что говори этот протестант должен знать, что говори\n",
      "15                 15               1.00 Представители работают на нас, а не мы — Представители работают на нас, а не мы —\n",
      "16                 16               1.00 Кто противится власти, тот противится ус Кто противится власти, тот противится ус\n",
      "17                 17               1.00 служитель Божий, мститель, чтобы привест служитель Божий, мститель, чтобы привест\n",
      "18                 18               1.00 Абсолютно нет                            Абсолютно нет\n",
      "19                 19               1.00 Конечно, нет                             Конечно, нет\n",
      "20                 20               1.00 плодами тьмы                             плодами тьмы\n",
      "21                 21               1.00 Мы, народу                               Мы, народу\n",
      "22                 22               1.00 Ни один человеческий указ не может счита Ни один человеческий указ не может счита\n",
      "23                 23               1.00 библейски неграмотные                    библейски неграмотные\n",
      "24                 24               1.00 тиранами                                 тиранами\n",
      "25                 25               0.92 Спрашиваю: должны ли христиане подчинять Спрашиваю: должны ли христиане подчинять\n",
      "--- article_span\n",
      "+++ wrapped_span\n",
      "@@ -1 +1 @@\n",
      "-Спрашиваю: должны ли христиане подчиняться тем, кто санкционировал убийство невинных в утробе матери\n",
      "+Спрашиваю: должны ли христиане подчиняться <<S_27>>тем, кто санкционировал убийство невинных в утробе матери<</S_27>>\n",
      "26                 -                0.00       тем, кто санкционировал убийство невинны NO MATCH\n",
      "27                 26               1.00 Определенно, нет                         Определенно, нет\n",
      "28                 27               0.93 Должны ли христиане подчиняться тем, кто Должны ли христиане подчиняться <<S_30>>\n",
      "--- article_span\n",
      "+++ wrapped_span\n",
      "@@ -1 +1 @@\n",
      "-Должны ли христиане подчиняться тем, кто переопределил брак, позволив мужчине «жениться» на мужчине и женщине — на женщине\n",
      "+Должны ли христиане подчиняться <<S_30>>тем, кто переопределил брак, позволив мужчине «жениться» на мужчине и женщине — на женщине<</S_30>>\n",
      "29                 -                0.00       тем, кто переопределил брак, позволив му NO MATCH\n",
      "30                 28               1.00 Определенно, нет                         Определенно, нет\n",
      "31                 -                0.00       Подчиняться тем кто постоянно лжёт, тем, NO MATCH\n",
      "32                 -                0.00       тем                                      NO MATCH\n",
      "33                 -                0.00       тем кто постоянно лжёт, тем, кто крадёт  NO MATCH\n",
      "34                 30               1.00 Определённо, нет                         Определённо, нет\n",
      "35                 31               1.00 извращённые, современные, исповедующие с извращённые, современные, исповедующие с\n",
      "36                 32               1.00 овец                                     овец\n",
      "37                 33               1.00 детьми Божьими                           детьми Божьими\n",
      "38                 34               1.00 восклицая                                восклицая\n",
      "39                 35               1.00 учат такому ересям                       учат такому ересям\n",
      "40                 36               1.00 враждебными коррумпированными граждански враждебными коррумпированными граждански\n",
      "41                 37               1.00 различными коррумпированными гражданским различными коррумпированными гражданским\n",
      "42                 38               1.00 Конечно, нет                             Конечно, нет\n",
      "43                 39               1.00 «ужасом для дел злых                     «ужасом для дел злых\n",
      "44                 40               1.00 добрых, законопослушных людей            добрых, законопослушных людей\n",
      "45                 41               1.00 приводить в исполнение гнев на делающего приводить в исполнение гнев на делающего\n",
      "46                 42               0.94 Кто учит, что у них есть власть приводит Кто учит, что у них есть власть <<S_48>>\n",
      "--- article_span\n",
      "+++ wrapped_span\n",
      "@@ -1 +1 @@\n",
      "-Кто учит, что у них есть власть приводить в исполнение гнев на делающего добро, тот невежда и невежественный в своих заблуждениях\n",
      "+Кто учит, что у них есть власть <<S_48>>приводить в исполнение гнев на делающего добро<</S_48>>, тот невежда и невежественный в своих заблуждениях\n",
      "47                 -                0.00       приводить в исполнение гнев на делающего NO MATCH\n",
      "48                 43               1.00 злой гражданской власти                  злой гражданской власти\n",
      "49                 44               1.00 Господь Бог евреев                       Господь Бог евреев\n",
      "50                 45               1.00 самый кроткий человек на земле           самый кроткий человек на земле\n",
      "51                 46               1.00 с требованием подчиниться, иначе         с требованием подчиниться, иначе\n",
      "52                 47               1.00 Тиран                                    Тиран\n",
      "53                 48               1.00 мятежником Израиля                       мятежником Израиля\n",
      "54                 49               1.00 Господа                                  Господа\n",
      "55                 50               1.00 нарушение принципов подчинения           нарушение принципов подчинения\n",
      "56                 51               1.00 воскликнул громко                        воскликнул громко\n",
      "57                 52               1.00 тот, кто не поклонится и не упадёт, буде тот, кто не поклонится и не упадёт, буде\n",
      "58                 53               1.00 тирану                                   тирану\n",
      "59                 54               1.00 лишь коррумпированный царь               лишь коррумпированный царь\n",
      "60                 -                0.00       коррумпированных                         NO MATCH\n",
      "61                 55               0.86 коррумпированных священников и коррумпир <<S_62>>коррумпированных<</S_62>> священ\n",
      "--- article_span\n",
      "+++ wrapped_span\n",
      "@@ -1 +1 @@\n",
      "-коррумпированных священников и коррумпированных царей\n",
      "+<<S_62>>коррумпированных<</S_62>> священников и коррумпированных царей\n",
      "62                 56               1.00 все злодеяния                            все злодеяния\n",
      "63                 57               1.00 Несомненно, нет                          Несомненно, нет\n",
      "64                 58               1.00 Коррумпированные фарисеи                 Коррумпированные фарисеи\n",
      "65                 59               1.00 коррумпированное правительство           коррумпированное правительство\n",
      "66                 60               1.00 неучёные и простые люди                  неучёные и простые люди\n",
      "67                 61               1.00 тиранов                                  тиранов\n",
      "68                 62               1.00 злых, воюющих с Христом                  злых, воюющих с Христом\n",
      "69                 63               0.82 коррумпированными и злыми священниками   коррумпированными и <<S_72>>злыми<</S_72\n",
      "--- article_span\n",
      "+++ wrapped_span\n",
      "@@ -1 +1 @@\n",
      "-коррумпированными и злыми священниками\n",
      "+коррумпированными и <<S_72>>злыми<</S_72>> священниками\n",
      "70                 -                0.00       злыми                                    NO MATCH\n",
      "71                 64               1.00 коррумпированными и злыми в правительств коррумпированными и злыми в правительств\n",
      "72                 65               1.00 Нигде                                    Нигде\n",
      "73                 66               1.00 Кроме того, автор статьи, призывающей лю Кроме того, автор статьи, призывающей лю\n",
      "\n",
      "Wrapped spans not matched to any article span:\n",
      "Wrapped index 29: Подчиняться <<S_33>><<S_34>>тем<</S_34>> кто постоянно лжёт,\n"
     ]
    }
   ],
   "source": [
    "# --- OBSOLETE CODE ---\n",
    "# Original span comparison function\n",
    "# import difflib\n",
    "# \n",
    "# def compare_article_and_wrapped_span_texts(article_id, lang=\"en\", base_path=\"../data/processed/ru\", min_ratio=0.8):\n",
    "#     \"\"\"\n",
    "#     Compare the span texts from the article (offset-based) and wrapped (marker-based) versions,\n",
    "#     reporting the closest matches and their differences. Order is not assumed to be the same.\n",
    "#     \"\"\"\n",
    "#     # Get span texts from both sources\n",
    "#     article_spans = show_article_spans(article_id, lang=lang, base_path=base_path)\n",
    "#     wrapped_spans = show_wrapped_spans(article_id, lang=lang, base_path=base_path)\n",
    "# \n",
    "#     if not article_spans or not wrapped_spans:\n",
    "#         print(\"No spans found in one or both sources.\")\n",
    "#         return\n",
    "# \n",
    "#     # Extract just the span texts\n",
    "#     article_texts = [span[3].strip() for span in article_spans]\n",
    "#     wrapped_texts = [span[1].strip() for span in wrapped_spans]\n",
    "# \n",
    "#     # For each article span, find the best matching wrapped span (by similarity ratio)\n",
    "#     print(f\"\\n{'Article Span Index':<18} {'Best Match Index':<16} {'Similarity':<10} {'Article Span Text':<40} {'Wrapped Span Text'}\")\n",
    "#     print(\"-\" * 120)\n",
    "#     used_wrapped = set()\n",
    "#     for i, art_text in enumerate(article_texts):\n",
    "#         # Find the best match in wrapped_texts\n",
    "#         best_ratio = 0\n",
    "#         best_j = -1\n",
    "#         for j, wrap_text in enumerate(wrapped_texts):\n",
    "#             if j in used_wrapped:\n",
    "#                 continue\n",
    "#             ratio = difflib.SequenceMatcher(None, art_text, wrap_text).ratio()\n",
    "#             if ratio > best_ratio:\n",
    "#                 best_ratio = ratio\n",
    "#                 best_j = j\n",
    "#         # Optionally, only consider matches above a threshold\n",
    "#         if best_ratio >= min_ratio and best_j != -1:\n",
    "#             used_wrapped.add(best_j)\n",
    "#             print(f\"{i:<18} {best_j:<16} {best_ratio:.2f} {art_text[:40]:<40} {wrapped_texts[best_j][:40]}\")\n",
    "#             # Show diff if not identical\n",
    "#             if best_ratio < 1.0:\n",
    "#                 diff = difflib.unified_diff(\n",
    "#                     art_text.splitlines(), wrapped_texts[best_j].splitlines(),\n",
    "#                     fromfile='article_span', tofile='wrapped_span', lineterm=''\n",
    "#                 )\n",
    "#                 print('\\n'.join(diff))\n",
    "#         else:\n",
    "#             print(f\"{i:<18} {'-':<16} {'0.00':<10} {art_text[:40]:<40} {'NO MATCH'}\")\n",
    "# \n",
    "#     # Optionally, report wrapped spans that were not matched\n",
    "#     unmatched = [j for j in range(len(wrapped_texts)) if j not in used_wrapped]\n",
    "#     if unmatched:\n",
    "#         print(\"\\nWrapped spans not matched to any article span:\")\n",
    "#         for j in unmatched:\n",
    "#             print(f\"Wrapped index {j}: {wrapped_texts[j][:60]}\")\n",
    "# \n",
    "# # Example usage:\n",
    "# compare_article_and_wrapped_span_texts(sample_article_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
