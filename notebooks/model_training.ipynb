{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139483db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from pathlib import Path # Added import\n",
    "\n",
    "def read_article(path: str) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        raw = f.read()\n",
    "    return raw.decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "def load_span_labels(label_file: str\n",
    "    ) -> Dict[str, List[Tuple[str, int, int]]]:\n",
    "    \n",
    "    spans = defaultdict(list)\n",
    "    with open(label_file, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            # Handle potential empty lines or lines with incorrect format\n",
    "            parts = line.rstrip().split(\"\\t\")\n",
    "            if len(parts) == 4:\n",
    "                art_id, lab, s, e = parts\n",
    "                try:\n",
    "                    spans[art_id].append((lab, int(s), int(e)))\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Skipping malformed line in {label_file}: {line.rstrip()}\")\n",
    "            elif line.strip(): # Print warning for non-empty, but malformed lines\n",
    "                 print(f\"Warning: Skipping malformed line in {label_file}: {line.rstrip()}\")\n",
    "    return spans\n",
    "\n",
    "# Added function to extract base classes from the span file\n",
    "def get_base_classes_from_spans(label_file: str) -> Set[str]:\n",
    "    base_classes = set()\n",
    "    with open(label_file, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip().split(\"\\t\")\n",
    "            if len(parts) == 4:\n",
    "                _, lab, _, _ = parts\n",
    "                base_classes.add(lab)\n",
    "            elif line.strip():\n",
    "                 # Warnings handled in load_span_labels, no need to repeat here\n",
    "                 pass\n",
    "    return base_classes\n",
    "\n",
    "def build_label_maps(\n",
    "    base_classes: set[str] # Changed input to accept a set of base classes\n",
    ") -> tuple[list[str], dict[str, int], dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Input\n",
    "    -----\n",
    "    base_classes – A set containing all unique base label names (e.g., \"Appeal_to_Fear-Prejudice\")\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    bio_tags   – full list like [\"O\", \"B-Appeal_to_Fear-Prejudice\", \"I-Appeal_to_Fear-Prejudice\", …]\n",
    "    label2id   – {\"O\": 0, \"B-…\": 1, …}   – used to turn tags into numbers\n",
    "    id2label   – inverse of label2id      – needed by the model/Trainer\n",
    "    \"\"\"\n",
    "    # 1️⃣ Use the provided base classes\n",
    "    sorted_base_classes = sorted(list(base_classes))\n",
    "\n",
    "    # 2️⃣ build BIO strings\n",
    "    bio_tags = [\"O\"]                      # outside any span\n",
    "    for cls in sorted_base_classes:\n",
    "        bio_tags.extend([f\"B-{cls}\", f\"I-{cls}\"])   # beginning / inside\n",
    "\n",
    "    # 3️⃣ numeric maps\n",
    "    label2id = {tag: i for i, tag in enumerate(bio_tags)}\n",
    "    id2label = {i: tag for tag, i in label2id.items()}\n",
    "\n",
    "    return bio_tags, label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567f78d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/twoface/persuasion-detection/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 11853,    83,  3060,  7986,    47,    47,  1098, 20650,     5,\n",
      "             2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'offset_mapping': tensor([[[ 0,  0],\n",
      "         [ 0,  4],\n",
      "         [ 5,  7],\n",
      "         [ 8, 12],\n",
      "         [13, 17],\n",
      "         [18, 20],\n",
      "         [21, 23],\n",
      "         [23, 26],\n",
      "         [26, 29],\n",
      "         [29, 30],\n",
      "         [ 0,  0]]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizerFast\n",
    "\n",
    "# Load the pre-trained XLM-RoBERTa tokenizer\n",
    "# You can replace \"xlm-roberta-base\" with a specific model if needed\n",
    "tokenizer_name = \"xlm-roberta-large\"\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(tokenizer_name)\n",
    "\n",
    "def tokenize_text(text: str):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text using the pre-loaded XLMRobertaTokenizerFast.\n",
    "\n",
    "    Args:\n",
    "        text: The input string to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the tokenized 'input_ids', 'attention_mask', etc.\n",
    "    \"\"\"\n",
    "    # Tokenize the text, adding common options like truncation and max_length\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=512,  # You can adjust the max_length if needed\n",
    "        return_tensors=\"pt\", # Return PyTorch tensors, change to \"tf\" for TensorFlow if required\n",
    "        return_offsets_mapping=True, # Useful for aligning tokens with original text\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Example of how to use the function (optional)\n",
    "sample_text = \"Here is some text to tokenize.\"\n",
    "tokenized_result = tokenize_text(sample_text)\n",
    "print(tokenized_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad88d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['<s>', '▁Here', '▁is', '▁some', '▁text', '▁to', '▁to', 'ken', 'ize', '.', '</s>']\n",
      "Labels: ['O', 'O', 'O', 'B-LOC', 'O', 'O', 'B-VERB', 'I-VERB', 'I-VERB', 'O', 'O']\n",
      "<s>             O               [0, 0]\n",
      "▁Here           O               [0, 4]\n",
      "▁is             O               [5, 7]\n",
      "▁some           B-LOC           [8, 12]\n",
      "▁text           O               [13, 17]\n",
      "▁to             O               [18, 20]\n",
      "▁to             B-VERB          [21, 23]\n",
      "ken             I-VERB          [23, 26]\n",
      "ize             I-VERB          [26, 29]\n",
      ".               O               [29, 30]\n",
      "</s>            O               [0, 0]\n"
     ]
    }
   ],
   "source": [
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "def align_labels_with_tokens(\n",
    "    tokenized_inputs: BatchEncoding,\n",
    "    spans: List[Tuple[str, int, int]]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Aligns character-level span labels with token-level BIO labels.\n",
    "    Handles cases where token spans partially overlap with label spans.\n",
    "\n",
    "    Args:\n",
    "        tokenized_inputs: Output from the tokenizer (must include 'offset_mapping').\n",
    "        spans: A list of tuples, where each tuple is (label, start_char, end_char).\n",
    "\n",
    "    Returns:\n",
    "        A list of BIO labels (e.g., \"O\", \"B-Label\", \"I-Label\") corresponding to each token.\n",
    "    \"\"\"\n",
    "    # Ensure offset mapping is present\n",
    "    if 'offset_mapping' not in tokenized_inputs:\n",
    "        raise ValueError(\"Tokenizer output must include 'offset_mapping'.\")\n",
    "\n",
    "    # offset_mapping is typically shape (batch_size, sequence_length, 2)\n",
    "    # Assuming batch_size is 1 for this function\n",
    "    # Removed .tolist() as the input might already be a list\n",
    "    offsets = tokenized_inputs['offset_mapping'][0]\n",
    "    num_tokens = len(offsets)\n",
    "    labels = [\"O\"] * num_tokens # Initialize all labels as Outside\n",
    "\n",
    "    # Sort spans by start index to handle potential overlaps consistently (optional but good practice)\n",
    "    # spans.sort(key=lambda x: x[1]) # Uncomment if sorting is desired\n",
    "\n",
    "    for label, start_char, end_char in spans:\n",
    "        found_first_token = False\n",
    "        for token_idx, (token_start, token_end) in enumerate(offsets):\n",
    "            # Skip special tokens (like [CLS], [SEP]) which have (0, 0) offset\n",
    "            if token_start == token_end:\n",
    "                continue\n",
    "\n",
    "            # Check for overlap between token span and label span\n",
    "            # This condition is true if there is *any* overlap, including partial overlaps.\n",
    "            # max(start1, start2) < min(end1, end2)\n",
    "            if max(token_start, start_char) < min(token_end, end_char):\n",
    "                # Assign B- tag to the first token overlapping the span\n",
    "                if not found_first_token:\n",
    "                    labels[token_idx] = f\"B-{label}\" # Begin label\n",
    "                    found_first_token = True\n",
    "                # Assign I- tag to subsequent tokens overlapping the *same* span\n",
    "                else:\n",
    "                    labels[token_idx] = f\"I-{label}\" # Inside label\n",
    "            # Optimization: If the token starts after the span ends,\n",
    "            # we don't need to check further for this span.\n",
    "            # elif token_start >= end_char:\n",
    "            #     break # Uncomment if spans are sorted by start_char\n",
    "\n",
    "    return labels\n",
    "\n",
    "# Example Usage (using variables from previous cells if run in the same notebook)\n",
    "\n",
    "# Sample spans (replace with actual spans for your data)\n",
    "sample_spans = [\n",
    "    (\"LOC\", 8, 12),   # Corresponds to \"some\" in \"Here is some text to tokenize.\"\n",
    "    (\"VERB\", 21, 29) # Corresponds to \"tokenize\"\n",
    "]\n",
    "\n",
    "# Use the previously tokenized result\n",
    "aligned_labels = align_labels_with_tokens(tokenized_result, sample_spans)\n",
    "\n",
    "# Print tokens and their corresponding labels\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_result['input_ids'][0])\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Labels:\", aligned_labels)\n",
    "\n",
    "# Verify alignment (optional)\n",
    "for token, label, offset in zip(tokens, aligned_labels, tokenized_result['offset_mapping'][0].tolist()):\n",
    "    print(f\"{token:<15} {label:<15} {offset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82494ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spans and base classes from all sources...\n",
      "  Processing source: prefix='', file='/home/twoface/persuasion-detection/data/processed/ru/train-labels-subtask-3-spans.txt'\n",
      "    Found 190 articles and 23 base classes.\n",
      "  Processing source: prefix='en_', file='/home/twoface/persuasion-detection/data/processed/ru/train-labels-subtask-3-spans-en.txt'\n",
      "    Found 504 articles and 19 base classes.\n",
      "  Processing source: prefix='fr_', file='/home/twoface/persuasion-detection/data/processed/ru/train-labels-subtask-3-spans-fr.txt'\n",
      "    Found 209 articles and 23 base classes.\n",
      "  Processing source: prefix='ge_', file='/home/twoface/persuasion-detection/data/processed/ru/train-labels-subtask-3-spans-ge.txt'\n",
      "    Found 176 articles and 23 base classes.\n",
      "  Processing source: prefix='it_', file='/home/twoface/persuasion-detection/data/processed/ru/train-labels-subtask-3-spans-it.txt'\n",
      "    Found 302 articles and 23 base classes.\n",
      "\n",
      "Total unique articles found across all sources: 1381\n",
      "Total unique base classes found: 23\n",
      "Building label maps...\n",
      "Total BIO tags: 47\n",
      "\n",
      "Processing articles from: /home/twoface/persuasion-detection/data/processed/ru/unwrapped-articles\n",
      "  Processing article 100/1381 (ID: 2465)...\n",
      "  Processing article 100/1381 (ID: 2465)...\n",
      "  Processing article 200/1381 (ID: en_728169864)...\n",
      "  Processing article 200/1381 (ID: en_728169864)...\n",
      "  Processing article 300/1381 (ID: en_729651527)...\n",
      "  Processing article 300/1381 (ID: en_729651527)...\n",
      "  Processing article 400/1381 (ID: en_723823436)...\n",
      "  Processing article 400/1381 (ID: en_723823436)...\n",
      "  Processing article 500/1381 (ID: en_761334950)...\n",
      "  Processing article 500/1381 (ID: en_761334950)...\n",
      "  Processing article 600/1381 (ID: en_738781754)...\n",
      "  Processing article 600/1381 (ID: en_738781754)...\n",
      "  Processing article 700/1381 (ID: fr_2381)...\n",
      "  Processing article 700/1381 (ID: fr_2381)...\n",
      "  Processing article 800/1381 (ID: fr_23112)...\n",
      "  Processing article 800/1381 (ID: fr_23112)...\n",
      "  Processing article 900/1381 (ID: fr_23171)...\n",
      "  Processing article 900/1381 (ID: fr_23171)...\n",
      "  Processing article 1000/1381 (ID: ge_2281)...\n",
      "  Processing article 1000/1381 (ID: ge_2281)...\n",
      "  Processing article 1100/1381 (ID: it_2662)...\n",
      "  Processing article 1100/1381 (ID: it_2662)...\n",
      "  Processing article 1200/1381 (ID: it_26287)...\n",
      "  Processing article 1200/1381 (ID: it_26287)...\n",
      "  Processing article 1300/1381 (ID: it_26178)...\n",
      "  Processing article 1300/1381 (ID: it_26178)...\n",
      "\n",
      "Finished processing. Processed: 1381, Skipped (missing files): 0.\n",
      "\n",
      "--- Sample Article ID: 24151 ---\n",
      "Tokenized Input Keys: dict_keys(['input_ids', 'attention_mask'])\n",
      "Number of Tokens: 459\n",
      "Number of Labels: 459\n",
      "Label2ID mapping (sample): [('O', 0), ('B-Appeal_to_Authority', 1), ('I-Appeal_to_Authority', 2), ('B-Appeal_to_Fear-Prejudice', 3), ('I-Appeal_to_Fear-Prejudice', 4)]\n",
      "ID2Label mapping (sample): [(0, 'O'), (1, 'B-Appeal_to_Authority'), (2, 'I-Appeal_to_Authority'), (3, 'B-Appeal_to_Fear-Prejudice'), (4, 'I-Appeal_to_Fear-Prejudice')]\n",
      "\n",
      "Finished processing. Processed: 1381, Skipped (missing files): 0.\n",
      "\n",
      "--- Sample Article ID: 24151 ---\n",
      "Tokenized Input Keys: dict_keys(['input_ids', 'attention_mask'])\n",
      "Number of Tokens: 459\n",
      "Number of Labels: 459\n",
      "Label2ID mapping (sample): [('O', 0), ('B-Appeal_to_Authority', 1), ('I-Appeal_to_Authority', 2), ('B-Appeal_to_Fear-Prejudice', 3), ('I-Appeal_to_Fear-Prejudice', 4)]\n",
      "ID2Label mapping (sample): [(0, 'O'), (1, 'B-Appeal_to_Authority'), (2, 'I-Appeal_to_Authority'), (3, 'B-Appeal_to_Fear-Prejudice'), (4, 'I-Appeal_to_Fear-Prejudice')]\n"
     ]
    }
   ],
   "source": [
    "def create_token_label_mapping(\n",
    "    span_sources: List[Tuple[str, str]], # List of (prefix, label_file_path)\n",
    "    article_dir: str,\n",
    "    tokenizer: XLMRobertaTokenizerFast,\n",
    "    max_length: int = 512\n",
    ") -> Tuple[Dict[str, Dict[str, List[int]]], Dict[str, List[int]], Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Creates a mapping from article ID to tokenized inputs and numerical BIO labels\n",
    "    by processing multiple language sources.\n",
    "\n",
    "    Args:\n",
    "        span_sources: List of tuples, each containing (language_prefix, span_label_file_path).\n",
    "                      Example: [(\"\", \"path/ru.txt\"), (\"en_\", \"path/en.txt\"), (\"fr_\", \"path/fr.txt\")]\n",
    "        article_dir: Path to the single directory containing all article text files.\n",
    "                     Article filenames are expected to be like article{id}.txt or {prefix}article{id}.txt\n",
    "        tokenizer: An initialized Hugging Face tokenizer (e.g., XLMRobertaTokenizerFast).\n",
    "        max_length: Maximum sequence length for tokenization.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - tokenized_articles: Dict mapping unique_article_id to tokenized inputs (Dict).\n",
    "        - article_label_ids: Dict mapping unique_article_id to list of numerical label IDs.\n",
    "        - label2id: Dictionary mapping BIO label strings to integer IDs.\n",
    "        - id2label: Dictionary mapping integer IDs to BIO label strings.\n",
    "    \"\"\"\n",
    "    all_spans_by_article = {}\n",
    "    all_base_classes = set()\n",
    "\n",
    "    print(\"Loading spans and base classes from all sources...\")\n",
    "    for prefix, span_label_file in span_sources:\n",
    "        print(f\"  Processing source: prefix='{prefix}', file='{span_label_file}'\")\n",
    "        if not Path(span_label_file).exists():\n",
    "            print(f\"  Warning: Label file not found, skipping: {span_label_file}\")\n",
    "            continue\n",
    "        current_spans = load_span_labels(span_label_file)\n",
    "        current_base_classes = get_base_classes_from_spans(span_label_file)\n",
    "        all_base_classes.update(current_base_classes)\n",
    "        print(f\"    Found {len(current_spans)} articles and {len(current_base_classes)} base classes.\")\n",
    "        # Create unique IDs like \"en_12345\" or just \"12345\" if prefix is empty\n",
    "        for art_id, spans in current_spans.items():\n",
    "            unique_art_id = f\"{prefix}{art_id}\"\n",
    "            if unique_art_id in all_spans_by_article:\n",
    "                 print(f\"    Warning: Duplicate article ID found: {unique_art_id}. Overwriting spans.\")\n",
    "            all_spans_by_article[unique_art_id] = spans\n",
    "    \n",
    "    total_articles = len(all_spans_by_article)\n",
    "    print(f\"\\nTotal unique articles found across all sources: {total_articles}\")\n",
    "    print(f\"Total unique base classes found: {len(all_base_classes)}\")\n",
    "\n",
    "    if not all_spans_by_article:\n",
    "        print(\"Error: No spans loaded. Cannot proceed.\")\n",
    "        return {}, {}, {}, {}\n",
    "\n",
    "    print(\"Building label maps...\")\n",
    "    bio_tags, label2id, id2label = build_label_maps(all_base_classes)\n",
    "    print(f\"Total BIO tags: {len(bio_tags)}\")\n",
    "\n",
    "    tokenized_articles = {}\n",
    "    article_label_ids = {}\n",
    "    article_dir_path = Path(article_dir)\n",
    "\n",
    "    print(f\"\\nProcessing articles from: {article_dir}\")\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    for unique_art_id, spans in all_spans_by_article.items():\n",
    "        processed_count += 1\n",
    "        if processed_count % 100 == 0: # Print progress\n",
    "             print(f\"  Processing article {processed_count}/{total_articles} (ID: {unique_art_id})...\")\n",
    "\n",
    "        # Construct article filename based on the unique ID\n",
    "        # Check if unique_art_id contains a prefix (e.g., \"en_\")\n",
    "        if '_' in unique_art_id:\n",
    "            prefix, actual_id = unique_art_id.split('_', 1)\n",
    "            article_filename = f\"{prefix}_article{actual_id}.txt\"\n",
    "        else:\n",
    "            # No prefix, assume it's Russian or similar format\n",
    "            article_filename = f\"article{unique_art_id}.txt\"\n",
    "        \n",
    "        article_path = article_dir_path / article_filename\n",
    "\n",
    "        if not article_path.exists():\n",
    "            print(f\"  Warning: Article file not found, skipping: {article_path}\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        text = read_article(str(article_path))\n",
    "\n",
    "        # Tokenize\n",
    "        tokenized_inputs = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_offsets_mapping=True,\n",
    "            # No return_tensors=\"pt\" here, keep as lists for now\n",
    "        )\n",
    "\n",
    "        # Align character spans to token BIO labels\n",
    "        # Wrap tokenized_inputs for align_labels_with_tokens\n",
    "        temp_batch_encoding = BatchEncoding({\n",
    "            'input_ids': [tokenized_inputs['input_ids']],\n",
    "            'attention_mask': [tokenized_inputs['attention_mask']],\n",
    "            'offset_mapping': [tokenized_inputs['offset_mapping']]\n",
    "        })\n",
    "        bio_labels = align_labels_with_tokens(temp_batch_encoding, spans)\n",
    "\n",
    "        # Convert BIO labels to numerical IDs\n",
    "        label_ids = [label2id.get(label, label2id[\"O\"]) for label in bio_labels]\n",
    "\n",
    "        # Store results (remove offset mapping if not needed later)\n",
    "        tokenized_inputs.pop(\"offset_mapping\")\n",
    "        tokenized_articles[unique_art_id] = tokenized_inputs\n",
    "        article_label_ids[unique_art_id] = label_ids\n",
    "\n",
    "    print(f\"\\nFinished processing. Processed: {processed_count - skipped_count}, Skipped (missing files): {skipped_count}.\")\n",
    "    return tokenized_articles, article_label_ids, label2id, id2label\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# Define paths (adjust if necessary)\n",
    "base_data_path = Path(\"/home/twoface/persuasion-detection/data/processed\")\n",
    "articles_path = base_data_path / \"ru/unwrapped-articles\" # All articles are here\n",
    "\n",
    "span_sources_to_process = [\n",
    "    (\"\", base_data_path / \"ru/train-labels-subtask-3-spans.txt\"), # Russian (no prefix)\n",
    "    (\"en_\", base_data_path / \"ru/train-labels-subtask-3-spans-en.txt\"), # English\n",
    "    (\"fr_\", base_data_path / \"ru/train-labels-subtask-3-spans-fr.txt\"),  # French\n",
    "    (\"ge_\", base_data_path / \"ru/train-labels-subtask-3-spans-ge.txt\"), # German\n",
    "    (\"it_\", base_data_path / \"ru/train-labels-subtask-3-spans-it.txt\"), # Italian\n",
    "]\n",
    "\n",
    "# Convert Path objects to strings for the function\n",
    "span_sources_str = [(prefix, str(path)) for prefix, path in span_sources_to_process]\n",
    "articles_path_str = str(articles_path)\n",
    "\n",
    "# Ensure the tokenizer is loaded (from the second cell)\n",
    "if 'tokenizer' not in locals():\n",
    "    print(\"Tokenizer not found, please run the tokenizer loading cell first.\")\n",
    "else:\n",
    "    # Call the function with the list of sources\n",
    "    tokenized_data, label_data, l2i, i2l = create_token_label_mapping(\n",
    "        span_sources_str,\n",
    "        articles_path_str,\n",
    "        tokenizer\n",
    "    )\n",
    "\n",
    "    # Display results for a sample article (if data was processed)\n",
    "    if tokenized_data:\n",
    "        sample_id = list(tokenized_data.keys())[0]\n",
    "        print(f\"\\n--- Sample Article ID: {sample_id} ---\")\n",
    "        print(\"Tokenized Input Keys:\", tokenized_data[sample_id].keys())\n",
    "        print(\"Number of Tokens:\", len(tokenized_data[sample_id]['input_ids']))\n",
    "        print(\"Number of Labels:\", len(label_data[sample_id]))\n",
    "        print(\"Label2ID mapping (sample):\", list(l2i.items())[:5])\n",
    "        print(\"ID2Label mapping (sample):\", list(i2l.items())[:5])\n",
    "    else:\n",
    "        print(\"\\nNo articles were processed. Check paths and file existence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d13b48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Hugging Face dataset...\n",
      "Preparing dataset from 1381 articles...\n",
      "Finished preparing dataset. Total articles processed: 1381. Skipped: 0.\n",
      "\n",
      "Dataset created successfully!\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1381\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from typing import Dict, List\n",
    "from transformers.tokenization_utils_base import BatchEncoding # Already imported but good practice\n",
    "\n",
    "def create_hf_dataset(\n",
    "    tokenized_articles: Dict[str, Dict[str, List[int]]], # Adjusted type hint\n",
    "    article_label_ids: Dict[str, List[int]],\n",
    "    label2id: Dict[str, int] # Keep label2id for potential future use or validation\n",
    ") -> Dataset:\n",
    "    \"\"\"\n",
    "    Creates a Hugging Face Dataset object from tokenized articles and labels.\n",
    "\n",
    "    Args:\n",
    "        tokenized_articles: Dict mapping article_id to tokenized inputs (Dict with 'input_ids', 'attention_mask').\n",
    "        article_label_ids: Dict mapping article_id to list of numerical label IDs.\n",
    "        label2id: Dictionary mapping BIO label strings to integer IDs (optional, could be used for validation).\n",
    "\n",
    "    Returns:\n",
    "        A Hugging Face Dataset object with columns 'input_ids', 'attention_mask', 'labels'.\n",
    "    \"\"\"\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(f\"Preparing dataset from {len(tokenized_articles)} articles...\")\n",
    "    skipped_count = 0\n",
    "    # Sort article IDs for deterministic dataset creation (optional but good practice)\n",
    "    sorted_art_ids = sorted(tokenized_articles.keys())\n",
    "\n",
    "    for art_id in sorted_art_ids:\n",
    "        if art_id not in article_label_ids:\n",
    "            print(f\"Warning: Labels not found for article {art_id}. Skipping.\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        input_ids = tokenized_articles[art_id]['input_ids']\n",
    "        attention_mask = tokenized_articles[art_id]['attention_mask']\n",
    "        labels = article_label_ids[art_id]\n",
    "\n",
    "        # Sanity check: lengths must match\n",
    "        if not (len(input_ids) == len(attention_mask) == len(labels)):\n",
    "            print(f\"Warning: Length mismatch for article {art_id}. \",\n",
    "                  f\"input_ids: {len(input_ids)}, attention_mask: {len(attention_mask)}, labels: {len(labels)}. Skipping.\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_attention_masks.append(attention_mask)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    print(f\"Finished preparing dataset. Total articles processed: {len(all_input_ids)}. Skipped: {skipped_count}.\")\n",
    "\n",
    "    if not all_input_ids: # Handle case where no data was processed\n",
    "        print(\"Error: No valid data found to create dataset.\")\n",
    "        return None\n",
    "\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": all_input_ids,\n",
    "        \"attention_mask\": all_attention_masks,\n",
    "        \"labels\": all_labels\n",
    "    }\n",
    "\n",
    "    # Create the Dataset object\n",
    "    hf_dataset = Dataset.from_dict(dataset_dict)\n",
    "    return hf_dataset\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# Check if the required variables exist from the previous cell\n",
    "if 'tokenized_data' in locals() and 'label_data' in locals() and 'l2i' in locals():\n",
    "    print(\"\\nCreating Hugging Face dataset...\")\n",
    "    # Ensure the data is not empty before proceeding\n",
    "    if tokenized_data and label_data:\n",
    "        # Rename to combined_dataset or similar to reflect augmented data\n",
    "        combined_dataset = create_hf_dataset(tokenized_data, label_data, l2i)\n",
    "\n",
    "        if combined_dataset:\n",
    "            print(\"\\nDataset created successfully!\")\n",
    "            print(combined_dataset)\n",
    "            # You can inspect the first example:\n",
    "            # print(\"\\nFirst example:\", combined_dataset[0]) # Avoid printing large examples by default\n",
    "        else:\n",
    "            print(\"\\nDataset creation failed.\")\n",
    "    else:\n",
    "        print(\"\\nCannot create dataset: 'tokenized_data' or 'label_data' is empty.\")\n",
    "else:\n",
    "    print(\"\\nPlease run the previous cells to generate 'tokenized_data', 'label_data', and 'l2i'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad4e1d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inspecting the combined dataset ---\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1381\n",
      "})\n",
      "\n",
      "--- Inspecting the first example (combined_dataset[0]) ---\n",
      "Keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "Number of tokens: 512\n",
      "Number of labels: 512\n",
      "Number of attention mask values: 512\n"
     ]
    }
   ],
   "source": [
    "# Inspect the created dataset\n",
    "# Use the new variable name 'combined_dataset'\n",
    "if 'combined_dataset' in locals() and combined_dataset is not None and len(combined_dataset) > 0:\n",
    "    print(\"--- Inspecting the combined dataset ---\")\n",
    "    print(combined_dataset)\n",
    "    print(\"\\n--- Inspecting the first example (combined_dataset[0]) ---\")\n",
    "    first_example = combined_dataset[0]\n",
    "    # print(first_example) # Avoid printing potentially large example\n",
    "    print(\"Keys:\", first_example.keys())\n",
    "    print(\"Number of tokens:\", len(first_example['input_ids']))\n",
    "    print(\"Number of labels:\", len(first_example['labels']))\n",
    "    print(\"Number of attention mask values:\", len(first_example['attention_mask']))\n",
    "\n",
    "    # Slicing returns a dictionary where each key maps to a list of values for that slice\n",
    "    # first_three_examples = combined_dataset[:3]\n",
    "    # print(\"\\n--- Inspecting the first 3 examples (combined_dataset[:3]) ---\")\n",
    "    # print(\"Keys:\", first_three_examples.keys())\n",
    "    # print(\"Number of examples in slice:\", len(first_three_examples['input_ids'])) # Should be 3\n",
    "elif 'combined_dataset' in locals() and combined_dataset is not None:\n",
    "     print(\"The combined_dataset is empty.\")\n",
    "else:\n",
    "    print(\"Variable 'combined_dataset' not found or is None. Please run the previous cell to create it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a55794",
   "metadata": {},
   "source": [
    "## Model Training Setup\n",
    "\n",
    "Now we set up the components needed for training:\n",
    "1.  **Load Model**: Load `XLMRobertaForTokenClassification` with the correct number of labels.\n",
    "2.  **Training Arguments**: Configure hyperparameters like learning rate, batch size, epochs, and output directories.\n",
    "3.  **Metrics**: Define a function to compute evaluation metrics (precision, recall, F1) using `seqeval`.\n",
    "4.  **Data Collator**: Use `DataCollatorForTokenClassification` for dynamic padding.\n",
    "5.  **Trainer**: Initialize the `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c490037b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original combined dataset size: 1381\n",
      "Train split size: 1242\n",
      "Evaluation split size: 139\n",
      "\n",
      "Train split structure:\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1242\n",
      "})\n",
      "\n",
      "Evaluation split structure:\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 139\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split the combined dataset (e.g., 90% train, 10% evaluation)\n",
    "# Use shuffle=True (default) and a seed for reproducibility\n",
    "split_dataset = combined_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Assign the splits to new variables\n",
    "train_split = split_dataset['train']\n",
    "eval_split = split_dataset['test'] # The 'test' key holds the evaluation split\n",
    "\n",
    "print(f\"Original combined dataset size: {len(combined_dataset)}\")\n",
    "print(f\"Train split size: {len(train_split)}\")\n",
    "print(f\"Evaluation split size: {len(eval_split)}\")\n",
    "\n",
    "# Display the structure of the splits\n",
    "print(\"\\nTrain split structure:\")\n",
    "print(train_split)\n",
    "print(\"\\nEvaluation split structure:\")\n",
    "print(eval_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b118ae",
   "metadata": {},
   "source": [
    "## Addressing Zero F1 Score (Model Predicting Only \"O\") - Attempt 2: Class Weights\n",
    "\n",
    "Increasing epochs and adjusting the learning rate did not resolve the zero F1 score, indicating the model still struggles to identify minority classes (B- and I- tags) due to the high prevalence of the 'O' tag (data imbalance).\n",
    "\n",
    "**Next Adjustment: Class Weighting**\n",
    "\n",
    "We will introduce class weights into the loss function. This assigns a higher penalty when the model misclassifies less frequent classes, forcing it to pay more attention to them.\n",
    "\n",
    "Steps:\n",
    "1.  **Calculate Weights:** Compute weights inversely proportional to class frequencies in the training set.\n",
    "2.  **Custom Trainer:** Create a `WeightedLossTrainer` by subclassing `Trainer`.\n",
    "3.  **Override Loss:** Modify the `compute_loss` method in the custom trainer to use `torch.nn.CrossEntropyLoss` with the calculated weights.\n",
    "4.  **Train:** Use this custom trainer for the next training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4b7ed44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating class weights...\n",
      "Total non-padding tokens in training set: 597276\n",
      "Number of classes (unique BIO tags): 47\n",
      "Calculated class weights tensor (first 5): tensor([2.9324e-02, 3.8509e+01, 1.8067e+00, 2.0464e+01, 1.1635e+00])\n",
      "Weight for 'O' (ID 0): 0.0293\n",
      "Weight for 'B-Appeal_to_Authority' (ID 1): 38.5091\n",
      "Weight for 'I-Appeal_to_Authority' (ID 2): 1.8067\n",
      "Total non-padding tokens in training set: 597276\n",
      "Number of classes (unique BIO tags): 47\n",
      "Calculated class weights tensor (first 5): tensor([2.9324e-02, 3.8509e+01, 1.8067e+00, 2.0464e+01, 1.1635e+00])\n",
      "Weight for 'O' (ID 0): 0.0293\n",
      "Weight for 'B-Appeal_to_Authority' (ID 1): 38.5091\n",
      "Weight for 'I-Appeal_to_Authority' (ID 2): 1.8067\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Calculate class weights based on the training split\n",
    "print(\"Calculating class weights...\")\n",
    "\n",
    "# Flatten all labels in the training set, ignoring padding (-100)\n",
    "# This list contains the numerical IDs for every token's label (O, B-Tag1, I-Tag1, B-Tag2, etc.)\n",
    "all_labels_flat = [label for sublist in train_split['labels'] for label in sublist if label != -100]\n",
    "\n",
    "if not all_labels_flat:\n",
    "    print(\"Warning: No valid labels found in train_split to calculate weights. Using uniform weights.\")\n",
    "    num_classes = len(l2i) # l2i maps BIO tags to IDs\n",
    "    class_weights_tensor = torch.ones(num_classes)\n",
    "else:\n",
    "    total_tokens = len(all_labels_flat)\n",
    "    num_classes = len(l2i) # Total number of unique BIO tags (O + B-Class1 + I-Class1 + ...)\n",
    "    # Count the occurrences of each specific BIO tag ID\n",
    "    label_counts = Counter(all_labels_flat)\n",
    "\n",
    "    print(f\"Total non-padding tokens in training set: {total_tokens}\")\n",
    "    print(f\"Number of classes (unique BIO tags): {num_classes}\")\n",
    "    # print(f\"Label counts (sample): {dict(list(label_counts.items())[:10])}\") # Optional: view counts\n",
    "\n",
    "    # Calculate weights inversely proportional to the frequency of each specific BIO tag.\n",
    "    # Formula: total_tokens / (num_classes * count_of_specific_tag)\n",
    "    # This inherently handles:\n",
    "    #   1. O vs B/I imbalance: 'O' (ID 0) is usually most frequent, getting the lowest weight.\n",
    "    #   2. Base class imbalance: A frequent class like 'Loaded_Language' will result in\n",
    "    #      higher counts for its B/I tags compared to a rare class, thus B/I tags\n",
    "    #      for 'Loaded_Language' will get lower weights than B/I tags for the rare class.\n",
    "    # Using label_counts.get(i, 1) handles classes potentially not present in the training split (assigns high weight).\n",
    "    weights = [total_tokens / (num_classes * label_counts.get(i, 1)) for i in range(num_classes)]\n",
    "\n",
    "    # Convert to tensor\n",
    "    class_weights_tensor = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "    print(f\"Calculated class weights tensor (first 5): {class_weights_tensor[:5]}\")\n",
    "    print(f\"Weight for 'O' (ID {l2i.get('O', -1)}): {class_weights_tensor[l2i.get('O', 0)]:.4f}\") # Use l2i to find O's ID\n",
    "    # Example: Print weight for the first B-tag if available\n",
    "    first_b_tag = next((tag for tag in l2i if tag.startswith('B-')), None)\n",
    "    if first_b_tag:\n",
    "        first_b_id = l2i[first_b_tag]\n",
    "        print(f\"Weight for '{first_b_tag}' (ID {first_b_id}): {class_weights_tensor[first_b_id]:.4f}\")\n",
    "    # Example: Print weight for the first I-tag if available\n",
    "    first_i_tag = next((tag for tag in l2i if tag.startswith('I-')), None)\n",
    "    if first_i_tag:\n",
    "        first_i_id = l2i[first_i_tag]\n",
    "        print(f\"Weight for '{first_i_tag}' (ID {first_i_id}): {class_weights_tensor[first_i_id]:.4f}\")\n",
    "\n",
    "# Ensure the tensor is available\n",
    "if 'class_weights_tensor' not in locals():\n",
    "   raise RuntimeError(\"class_weights_tensor was not calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d8461bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WeightedLossTrainer class defined.\n",
      "Setting up training components with weighted loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model re-loaded: xlm-roberta-large with 47 labels.\n",
      "TrainingArguments defined for weighted loss run (v2).\n",
      "TrainingArguments defined for weighted loss run (v2).\n",
      "compute_metrics function defined.\n",
      "DataCollatorForTokenClassification initialized.\n",
      "compute_metrics function defined.\n",
      "DataCollatorForTokenClassification initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97221/3336317287.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WeightedLossTrainer initialized.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import numpy as np\n",
    "import evaluate # Use evaluate instead of load_metric\n",
    "import torch # Ensure torch is imported\n",
    "\n",
    "# Define the Custom Trainer with Weighted Loss\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    # Modified signature to accept **kwargs\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Define loss function with weights\n",
    "        # Ensure weights are on the same device as the model\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(model.device))\n",
    "        # Compute loss\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "print(\"WeightedLossTrainer class defined.\")\n",
    "\n",
    "print(\"Setting up training components with weighted loss...\")\n",
    "model = XLMRobertaForTokenClassification.from_pretrained(\n",
    "    tokenizer_name,\n",
    "    id2label=i2l,\n",
    "    label2id=l2i\n",
    ")\n",
    "print(f\"Model re-loaded: {tokenizer_name} with {model.config.num_labels} labels.\")\n",
    "# Or reuse the existing model variable if desired\n",
    "# print(f\"Using existing model: {tokenizer_name} with {model.config.num_labels} labels.\") # Commented out reuse message\n",
    "\n",
    "# 2. Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_combined_weighted_v2\", # Changed output dir again\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5, # Increased learning rate slightly\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10, # Increased epochs\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_combined_weighted_v2', # Changed logging dir again\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "print(\"TrainingArguments defined for weighted loss run (v2).\")\n",
    "\n",
    "# 3. Metrics Calculation\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "label_list = list(i2l.values()) # Use i2l from the previous cell\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (-100) and convert indices to labels\n",
    "    true_predictions_filtered = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels_filtered = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # --- Debugging Print ---\n",
    "    # Print the first few examples to see what's being passed to seqeval\n",
    "    # print(\"\\n--- Debugging compute_metrics ---\") # Keep commented out unless debugging\n",
    "    # num_examples_to_print = 1 # Reduced for brevity\n",
    "    # for i in range(min(num_examples_to_print, len(true_labels_filtered))):\n",
    "    #     print(f\"Example {i+1} (first 50 labels/preds shown):\")\n",
    "    #     # Ensure lists are not empty before slicing/printing\n",
    "    #     if true_labels_filtered[i]:\n",
    "    #          print(f\"  Labels     : {true_labels_filtered[i][:50]\")\n",
    "    #     else:\n",
    "    #          print(\"  Labels     : [] (all filtered out)\")\n",
    "    #     if true_predictions_filtered[i]:\n",
    "    #          print(f\"  Predictions: {true_predictions_filtered[i][:50]\")\n",
    "    #     else:\n",
    "    #          print(\"  Predictions: [] (all filtered out)\")\n",
    "    # print(\"-----------------------------\")\n",
    "    # --- End Debugging Print ---\n",
    "\n",
    "    # Check if filtering resulted in empty lists (e.g., all tokens were padding)\n",
    "    # Filter out examples where either true labels or predictions became empty after filtering -100\n",
    "    valid_indices = [i for i, (lbls, preds) in enumerate(zip(true_labels_filtered, true_predictions_filtered)) if lbls and preds]\n",
    "    if not valid_indices:\n",
    "        print(\"Warning: No valid examples found after filtering -100. Returning zero metrics.\")\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"accuracy\": 0.0}\n",
    "        \n",
    "    # Use only the valid examples for metric computation\n",
    "    final_true_predictions = [true_predictions_filtered[i] for i in valid_indices]\n",
    "    final_true_labels = [true_labels_filtered[i] for i in valid_indices]\n",
    "\n",
    "    # Compute all metrics using seqeval\n",
    "    results = metric.compute(predictions=final_true_predictions, references=final_true_labels)\n",
    "\n",
    "    # Check if overall F1 is zero and print a suggestion\n",
    "    if results.get(\"overall_f1\", 0.0) == 0.0:\n",
    "        print(\"\\nNote: Overall F1 score is 0.0. The model might not be predicting any entities correctly yet.\")\n",
    "        print(\"Consider training for more epochs, adjusting hyperparameters, or checking label alignment.\")\n",
    "\n",
    "    # Return the main overall metrics required by the Trainer\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "print(\"compute_metrics function defined.\")\n",
    "\n",
    "# 4. Data Collator (remains the same)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "print(\"DataCollatorForTokenClassification initialized.\")\n",
    "\n",
    "# 5. Trainer - Use the custom WeightedLossTrainer\n",
    "# Ensure class_weights_tensor is defined from the previous cell\n",
    "if 'class_weights_tensor' not in locals():\n",
    "   raise NameError(\"Variable 'class_weights_tensor' not defined. Please run the previous cell.\")\n",
    "\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_split, # Use train_split\n",
    "    eval_dataset=eval_split,   # Use eval_split\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights_tensor # Pass the calculated weights\n",
    ")\n",
    "print(\"WeightedLossTrainer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b909c91",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "Execute the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d33791e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training on the combined dataset with weighted loss (v2)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='1560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/1560 00:56 < 12:12:16, 0.04 it/s, Epoch 0.02/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mtrainer\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(trainer, WeightedLossTrainer):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting training on the combined dataset with weighted loss (v2)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining finished.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Save the final model, tokenizer, and training arguments\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persuasion-detection/.venv/lib/python3.12/site-packages/transformers/trainer.py:3736\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3740\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3741\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3742\u001b[39m ):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mWeightedLossTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, **kwargs)\u001b[39m\n\u001b[32m     18\u001b[39m logits = outputs.get(\u001b[33m\"\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Define loss function with weights\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Ensure weights are on the same device as the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m loss_fct = torch.nn.CrossEntropyLoss(weight=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[32m     23\u001b[39m loss = loss_fct(logits.view(-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.model.config.num_labels), labels.view(-\u001b[32m1\u001b[39m))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Start the training with weighted loss\n",
    "if 'trainer' in locals() and isinstance(trainer, WeightedLossTrainer):\n",
    "    print(\"\\nStarting training on the combined dataset with weighted loss (v2)...\")\n",
    "    train_result = trainer.train()\n",
    "    print(\"\\nTraining finished.\")\n",
    "\n",
    "    # Save the final model, tokenizer, and training arguments\n",
    "    final_model_path = training_args.output_dir # Use path from args\n",
    "    trainer.save_model() # Saves to output_dir defined in training_args\n",
    "    print(f\"Model saved to {final_model_path}\")\n",
    "\n",
    "    # Log metrics\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    print(\"Training metrics and state saved.\")\n",
    "    \n",
    "    # Evaluate the best model on the evaluation set\n",
    "    print(\"\\nEvaluating the best model on the evaluation set...\")\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    trainer.log_metrics(\"eval\", eval_metrics)\n",
    "    trainer.save_metrics(\"eval\", eval_metrics)\n",
    "    print(\"Evaluation metrics saved.\")\n",
    "    print(eval_metrics) # Print final eval metrics\n",
    "else:\n",
    "    print(\"\\nWeightedLossTrainer was not initialized correctly. Cannot start training. Please check the setup cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc153469",
   "metadata": {},
   "source": [
    "## Detailed Error Analysis\n",
    "\n",
    "Let's examine the performance on the evaluation set in more detail, looking at the precision, recall, and F1-score for each entity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521a7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing detailed evaluation and error analysis...\n",
      "\n",
      "Classification Report:\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "             Appeal_to_Authority     0.0008    0.0952    0.0016        21\n",
      "        Appeal_to_Fear-Prejudice     0.0014    0.0377    0.0026        53\n",
      "             Appeal_to_Hypocrisy     0.0000    0.0000    0.0000        29\n",
      "            Appeal_to_Popularity     0.0000    0.0000    0.0000        10\n",
      "                  Appeal_to_Time     0.0030    0.1667    0.0060         6\n",
      "                Appeal_to_Values     0.0014    0.0400    0.0027        25\n",
      "       Causal_Oversimplification     0.0000    0.0000    0.0000        20\n",
      "Consequential_Oversimplification     0.0000    0.0000    0.0000        16\n",
      "             Conversation_Killer     0.0018    0.0588    0.0034        34\n",
      "                           Doubt     0.0000    0.0000    0.0000       239\n",
      "       Exaggeration-Minimisation     0.0020    0.0351    0.0038        57\n",
      "         False_Dilemma-No_Choice     0.0000    0.0000    0.0000        25\n",
      "                     Flag_Waving     0.0006    0.0455    0.0012        22\n",
      "            Guilt_by_Association     0.0000    0.0000    0.0000        20\n",
      "                 Loaded_Language     0.0138    0.0924    0.0240       303\n",
      "           Name_Calling-Labeling     0.0161    0.2150    0.0300       200\n",
      " Obfuscation-Vagueness-Confusion     0.0000    0.0000    0.0000        18\n",
      "      Questioning_the_Reputation     0.0000    0.0000    0.0000       103\n",
      "                     Red_Herring     0.0000    0.0000    0.0000         7\n",
      "                      Repetition     0.0000    0.0000    0.0000        42\n",
      "                         Slogans     0.0036    0.0938    0.0069        32\n",
      "                       Straw_Man     0.0000    0.0000    0.0000        16\n",
      "                    Whataboutism     0.0000    0.0000    0.0000        11\n",
      "\n",
      "                       micro avg     0.0034    0.0649    0.0064      1309\n",
      "                       macro avg     0.0019    0.0383    0.0036      1309\n",
      "                    weighted avg     0.0060    0.0649    0.0108      1309\n",
      "\n",
      "\n",
      "Classification report saved to: results_combined_weighted_v2/classification_report.txt\n",
      "\n",
      "Classification Report:\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "             Appeal_to_Authority     0.0008    0.0952    0.0016        21\n",
      "        Appeal_to_Fear-Prejudice     0.0014    0.0377    0.0026        53\n",
      "             Appeal_to_Hypocrisy     0.0000    0.0000    0.0000        29\n",
      "            Appeal_to_Popularity     0.0000    0.0000    0.0000        10\n",
      "                  Appeal_to_Time     0.0030    0.1667    0.0060         6\n",
      "                Appeal_to_Values     0.0014    0.0400    0.0027        25\n",
      "       Causal_Oversimplification     0.0000    0.0000    0.0000        20\n",
      "Consequential_Oversimplification     0.0000    0.0000    0.0000        16\n",
      "             Conversation_Killer     0.0018    0.0588    0.0034        34\n",
      "                           Doubt     0.0000    0.0000    0.0000       239\n",
      "       Exaggeration-Minimisation     0.0020    0.0351    0.0038        57\n",
      "         False_Dilemma-No_Choice     0.0000    0.0000    0.0000        25\n",
      "                     Flag_Waving     0.0006    0.0455    0.0012        22\n",
      "            Guilt_by_Association     0.0000    0.0000    0.0000        20\n",
      "                 Loaded_Language     0.0138    0.0924    0.0240       303\n",
      "           Name_Calling-Labeling     0.0161    0.2150    0.0300       200\n",
      " Obfuscation-Vagueness-Confusion     0.0000    0.0000    0.0000        18\n",
      "      Questioning_the_Reputation     0.0000    0.0000    0.0000       103\n",
      "                     Red_Herring     0.0000    0.0000    0.0000         7\n",
      "                      Repetition     0.0000    0.0000    0.0000        42\n",
      "                         Slogans     0.0036    0.0938    0.0069        32\n",
      "                       Straw_Man     0.0000    0.0000    0.0000        16\n",
      "                    Whataboutism     0.0000    0.0000    0.0000        11\n",
      "\n",
      "                       micro avg     0.0034    0.0649    0.0064      1309\n",
      "                       macro avg     0.0019    0.0383    0.0036      1309\n",
      "                    weighted avg     0.0060    0.0649    0.0108      1309\n",
      "\n",
      "\n",
      "Classification report saved to: results_combined_weighted_v2/classification_report.txt\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "print(\"Performing detailed evaluation and error analysis...\")\n",
    "\n",
    "# Ensure trainer and eval_split exist\n",
    "if 'trainer' in locals() and 'eval_split' in locals():\n",
    "    # Get predictions\n",
    "    predictions_output = trainer.predict(eval_split)\n",
    "    predictions = np.argmax(predictions_output.predictions, axis=2)\n",
    "    true_labels = predictions_output.label_ids\n",
    "\n",
    "    # Convert indices to labels, removing -100\n",
    "    true_predictions_list = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, true_labels)\n",
    "    ]\n",
    "    true_labels_list = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, true_labels)\n",
    "    ]\n",
    "    \n",
    "    # Filter out empty examples if any occurred after removing -100\n",
    "    valid_indices = [i for i, (lbls, preds) in enumerate(zip(true_labels_list, true_predictions_list)) if lbls and preds]\n",
    "    final_true_predictions = [true_predictions_list[i] for i in valid_indices]\n",
    "    final_true_labels = [true_labels_list[i] for i in valid_indices]\n",
    "\n",
    "    if final_true_labels and final_true_predictions:\n",
    "        # Generate and print the classification report\n",
    "        report = classification_report(final_true_labels, final_true_predictions, digits=4)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(report)\n",
    "        \n",
    "        # Save the report to a file\n",
    "        report_path = Path(training_args.output_dir) / \"classification_report.txt\"\n",
    "        try:\n",
    "            with open(report_path, \"w\") as f:\n",
    "                f.write(report)\n",
    "            print(f\"\\nClassification report saved to: {report_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saving classification report: {e}\")\n",
    "            \n",
    "    else:\n",
    "         print(\"\\nCould not generate classification report: No valid labels/predictions found after filtering.\")\n",
    "else:\n",
    "    print(\"\\nCould not perform error analysis: 'trainer' or 'eval_split' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6864ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting experiment results...\n",
      "Experiment summary saved successfully to: ../experiment_results/experiment_summary_20250501_210146.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from transformers import TrainingArguments # Assuming TrainingArguments is needed\n",
    "# Assuming seqeval report format is a string, adjust if it's a dict\n",
    "from typing import Dict, Any, Union \n",
    "\n",
    "def export_experiment_results(\n",
    "    training_args: TrainingArguments,\n",
    "    tokenizer_name: str,\n",
    "    eval_metrics: Dict[str, float],\n",
    "    report: Union[str, Dict[str, Any]], # Allow string or dict for report\n",
    "    output_dir_override: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Exports experiment results (config, metrics, report) to a timestamped JSON file.\n",
    "\n",
    "    Args:\n",
    "        training_args: The TrainingArguments object used for the run.\n",
    "        tokenizer_name: The name of the tokenizer/model used.\n",
    "        eval_metrics: A dictionary containing evaluation metrics (e.g., from trainer.evaluate()).\n",
    "        report: The classification report (string or dictionary).\n",
    "        output_dir_override: Optional path to specify a different output directory. \n",
    "                             If None, uses training_args.output_dir.\n",
    "    \"\"\"\n",
    "    print(\"Exporting experiment results...\")\n",
    "\n",
    "    # Create a timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Prepare data for export\n",
    "    export_data = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"model_name\": tokenizer_name,\n",
    "        \"training_arguments\": training_args.to_dict(), # Convert TrainingArguments to dict\n",
    "        \"evaluation_metrics\": eval_metrics,\n",
    "        \"classification_report\": report\n",
    "    }\n",
    "    \n",
    "    # Define output path\n",
    "    if output_dir_override:\n",
    "        output_dir = Path(output_dir_override)\n",
    "    else:\n",
    "        output_dir = Path(training_args.output_dir)\n",
    "        \n",
    "    output_dir.mkdir(parents=True, exist_ok=True) # Ensure directory exists\n",
    "    export_filename = output_dir / f\"experiment_summary_{timestamp}.json\"\n",
    "    \n",
    "    # Save data to JSON file\n",
    "    try:\n",
    "        with open(export_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(export_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Experiment summary saved successfully to: {export_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving experiment summary: {e}\")\n",
    "\n",
    "export_experiment_results(\n",
    "    training_args,\n",
    "    tokenizer_name,\n",
    "    eval_metrics,\n",
    "    report,\n",
    "    output_dir_override='../experiment_results' # Example override path (adjust as needed)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc0edea",
   "metadata": {},
   "source": [
    "## Clear Model from Memory\n",
    "\n",
    "Delete the model and trainer objects and clear the GPU cache to free up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c24ec74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing model and trainer from memory...\n",
      "  Deleted 'model' variable.\n",
      "  Deleted 'trainer' variable.\n",
      "  Ran garbage collector.\n",
      "  Cleared PyTorch CUDA cache.\n",
      "Memory clearing process finished.\n",
      "  Cleared PyTorch CUDA cache.\n",
      "Memory clearing process finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"Clearing model and trainer from memory...\")\n",
    "\n",
    "# Check if variables exist before deleting\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "    print(\"  Deleted 'model' variable.\")\n",
    "if 'trainer' in locals():\n",
    "    del trainer\n",
    "    print(\"  Deleted 'trainer' variable.\")\n",
    "# Add any other large variables related to the model if needed\n",
    "# e.g., del optimizer, del scheduler\n",
    "\n",
    "# Run Python's garbage collector\n",
    "gc.collect()\n",
    "print(\"  Ran garbage collector.\")\n",
    "\n",
    "# Clear PyTorch's CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"  Cleared PyTorch CUDA cache.\")\n",
    "else:\n",
    "    print(\"  CUDA not available, skipping cache clearing.\")\n",
    "\n",
    "print(\"Memory clearing process finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
